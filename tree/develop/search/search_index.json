{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Autoware (Architecture Proposal) # AutowareArchitectureProposal is a repository to explore and establish the architecture design of Autoware, an autonomous driving software managed by Autoware Foundation. There already exists Autoware.Auto repository in Autoware Foundation GitLab. The architecture investigation, however, was done as a separate repository rather than a fork to explore architecture without prejudice from the existing source code. The established architecture will be presented to Autoware.Auto which will then be reviewed by the community members and be used to improve Autoware.Auto. AutowareArchitectureProposal also contains new functions that do not yet exist in Autoware.Auto to verify that the architecture is feasible for various use cases. These functions are also planned to be refactored and merged into Autoware.Auto. The details are explained in the Future plans section. Note for non-Tier IV members # All source code relating to this meta-repository is intended solely to demonstrate a potential new architecture for Autoware Please do not use any source code related to this repository to control actual vehicles While the documentation is published on GitHub Pages, the repository is private. We don't accept any contributions from outside of Tier IV Target # AutowareArchitectureProposal aims to realize autonomous driving in various environments. Autonomous driving on public roads is an extremely challenging project, and it cannot be achieved in a short period of time. Therefore we are trying to accumulate technology by applying the current AutowareArchitectureProposal to more restricted use cases such as in-factory transportation. At the same time, demonstration experiments in public roads are also in progress. Future plans # Again, autonomous driving is an extremely challenging project and this cannot be achieved in a short time. It cannot be achieved by only one company. People and companies all over the world need to work together to make it possible. We build the system and society through collaboration. So Tier IV established the Autoware Foundation (AWF) in 2018 to initiate, grow, and fund Autoware-related projects worldwide. Autoware.Auto is the current flagship AWF project and is a complete architectural redesign of Autoware.AI that employs best-in-class software engineering practices. As part of Tier IV's commitment to collaboration with the AWF and its members, we plan to merge the additional functionality of AutowareArchitectureProposal to Autoware.Auto. Note that since Autoware.Auto has its own scope and ODD (Operational Design Domain, prerequisite environmental conditions for an automatic driving system to operate) that needs to be achieved, not all the features in AutowareArchitectureProposal will be required. We keep using AutowareArchitectureProposal for some time, but remember that the core of our products will shift to Autoware.Auto.","title":"Autoware (Architecture Proposal)"},{"location":"#autoware-architecture-proposal","text":"AutowareArchitectureProposal is a repository to explore and establish the architecture design of Autoware, an autonomous driving software managed by Autoware Foundation. There already exists Autoware.Auto repository in Autoware Foundation GitLab. The architecture investigation, however, was done as a separate repository rather than a fork to explore architecture without prejudice from the existing source code. The established architecture will be presented to Autoware.Auto which will then be reviewed by the community members and be used to improve Autoware.Auto. AutowareArchitectureProposal also contains new functions that do not yet exist in Autoware.Auto to verify that the architecture is feasible for various use cases. These functions are also planned to be refactored and merged into Autoware.Auto. The details are explained in the Future plans section.","title":"Autoware (Architecture Proposal)"},{"location":"#note-for-non-tier-iv-members","text":"All source code relating to this meta-repository is intended solely to demonstrate a potential new architecture for Autoware Please do not use any source code related to this repository to control actual vehicles While the documentation is published on GitHub Pages, the repository is private. We don't accept any contributions from outside of Tier IV","title":"Note for non-Tier IV members"},{"location":"#target","text":"AutowareArchitectureProposal aims to realize autonomous driving in various environments. Autonomous driving on public roads is an extremely challenging project, and it cannot be achieved in a short period of time. Therefore we are trying to accumulate technology by applying the current AutowareArchitectureProposal to more restricted use cases such as in-factory transportation. At the same time, demonstration experiments in public roads are also in progress.","title":"Target"},{"location":"#future-plans","text":"Again, autonomous driving is an extremely challenging project and this cannot be achieved in a short time. It cannot be achieved by only one company. People and companies all over the world need to work together to make it possible. We build the system and society through collaboration. So Tier IV established the Autoware Foundation (AWF) in 2018 to initiate, grow, and fund Autoware-related projects worldwide. Autoware.Auto is the current flagship AWF project and is a complete architectural redesign of Autoware.AI that employs best-in-class software engineering practices. As part of Tier IV's commitment to collaboration with the AWF and its members, we plan to merge the additional functionality of AutowareArchitectureProposal to Autoware.Auto. Note that since Autoware.Auto has its own scope and ODD (Operational Design Domain, prerequisite environmental conditions for an automatic driving system to operate) that needs to be achieved, not all the features in AutowareArchitectureProposal will be required. We keep using AutowareArchitectureProposal for some time, but remember that the core of our products will shift to Autoware.Auto.","title":"Future plans"},{"location":"Credits/","text":"Certain AutowareArchitectureProposal packages rely on pre-trained CNN models provided by other open source repositories. tensorrt_yolo3 The pre-trained models originate from TRTForYolov3 . Weights for the trained model (416 folder) are automatically downloaded during the build process. traffic_light_fine_detector The trained model in this package is based on the pjreddie's YOLO .weights file , with additional fine-tuning by Tier IV using Darknet . After fine-tuning, the new weights for the trained model are converted into an ONNX file using Python . lidar_apollo_instance_segmentation This package makes use of three pre-trained models provided by Apollo Auto . The following files are automatically downloaded during the build process: VLP-16 HDL-64 VLS-128","title":"Credits"},{"location":"License/","text":"License # See GitHub repository","title":"License"},{"location":"License/#license","text":"See GitHub repository","title":"License"},{"location":"design/apis/ja/concept/","text":"Concept of Autoware API # \u6ce8\u610f\u4e8b\u9805 # \u672c\u30da\u30fc\u30b8\u3067\u306f Autoware \u3067\u4f7f\u7528\u3055\u308c\u308b API \u306b\u3064\u3044\u3066\u8a18\u8f09\u3059\u308b\u3002Autoware \u3068\u3044\u3046\u5358\u8a9e\u304c\u3069\u3053\u307e\u3067\u306e\u7bc4\u56f2\u3092\u6307\u3057\u793a\u3057\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u306f\u8b70\u8ad6\u304c\u5fc5\u8981\u3060\u304c\u3001\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u304a\u3044\u3066\u306f\u8eca\u4e21\u5236\u5fa1\u3084\u5468\u8fba\u88c5\u7f6e\u306a\u3069\u3092\u542b\u3081\u3001\u81ea\u52d5\u904b\u8ee2\u3092\u884c\u3046\u30b7\u30b9\u30c6\u30e0\u304c\u4f7f\u7528\u3059\u308b API \u3092\u5168\u90e8\u307e\u3068\u3081\u3066 Autoware API \u3068\u547c\u79f0\u3059\u308b\u3053\u3068\u306b\u3059\u308b\u3002 \u8a2d\u8a08\u65b9\u91dd # High-level API \u306f\u3001FMS \u3084\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306a\u3069\u30e6\u30fc\u30b6\u30fc\u8996\u70b9\u3067\u5fc5\u8981\u3068\u306a\u308b\u64cd\u4f5c\u3092\u5b89\u5b9a\u3057\u3066\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6a19\u306b\u8a2d\u8a08\u3059\u308b\u3002\u5177\u4f53\u7684\u306a\u5b9f\u88c5\u306f\u8eca\u4e21\u3084\u74b0\u5883\u306b\u3088\u308a\u7570\u306a\u308b\u305f\u3081\u3001\u5404\u5c64\u3067\u5b9a\u7fa9\u3057\u305f API \u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u67d4\u8edf\u306b\u52d5\u4f5c\u5909\u66f4\u3059\u308b\u4ed5\u7d44\u307f\u3092\u5185\u90e8\u3067\u7528\u610f\u3057\u3066\u304a\u304d\u3001High-level API \u306e\u4ed5\u69d8\u3092\u7dad\u6301\u3057\u305f\u307e\u307e\u69d8\u3005\u306a\u8981\u671b\u3078\u306e\u5bfe\u5fdc\u3092\u53ef\u80fd\u3068\u3059\u308b\u3002 \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc # API \u306e\u5206\u985e # High-level API \u8eca\u4e21\u306e\u4e57\u5ba2\u3084\u6574\u5099\u54e1\u3001FMS\u306a\u3069\u81ea\u52d5\u904b\u8ee2\u30b7\u30b9\u30c6\u30e0(Autoware)\u306eUser\u306b\u63d0\u4f9b\u3059\u308bAPI\u3002\u5168\u3066\u306e\u8eca\u4e21\u3067\u4ed5\u69d8\u304c\u5171\u901a\u3067\u3042\u308a\u9577\u671f\u9593\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u6c42\u3081\u3089\u308c\u308b\u3002 Low-level API \u81ea\u52d5\u904b\u8ee2\u30b7\u30b9\u30c6\u30e0(Autoware)\u5185\u90e8\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u63d0\u4f9b\u3059\u308bAPI\u3002\u4e3b\u306bFOA\u306a\u3069\u306e\u5185\u90e8\u30c4\u30fc\u30eb\u3084\u3001\u8eca\u4e21\u56fa\u6709\u306e\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u3092\u5236\u5fa1\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u3002 \u5171\u901a\u8a2d\u5b9a\u9805\u76ee # Behavior API\u306e\u52d5\u4f5c\u304cTopic/Service/Action\u306e\u3069\u308c\u306a\u306e\u304b\u3092\u6307\u5b9a\u3059\u308b\u3002 Category API\u306e\u5b9f\u88c5\u304c\u5fc5\u9808\u306a\u306e\u304b\u4efb\u610f\u306a\u306e\u304b\u3092\u6307\u5b9a\u3059\u308b\u3002 Data Type API\u3067\u4f7f\u7528\u3055\u308c\u308b\u30c7\u30fc\u30bf\u306e\u5f62\u3092\u6307\u5b9a\u3059\u308b\u3002 QoS API\u306e\u30b5\u30fc\u30d3\u30b9\u30ec\u30d9\u30eb\u3092\u6307\u5b9a\u3059\u308b\u3002 Topic Rate API\u306e\u52d5\u4f5c\u304cTopic\u5834\u5408\u306b\u9001\u53d7\u4fe1\u306e\u5468\u671f\u3092\u6307\u5b9a\u3059\u308b\u3002 Timeout API\u306e\u52d5\u4f5c\u304cService\u5834\u5408\u306b\u5fdc\u7b54\u6642\u9593\u306e\u5236\u9650\u3092\u6307\u5b9a\u3059\u308b\u3002","title":"Concept"},{"location":"design/apis/ja/concept/#concept-of-autoware-api","text":"","title":"Concept of Autoware API"},{"location":"design/apis/ja/concept/#_1","text":"\u672c\u30da\u30fc\u30b8\u3067\u306f Autoware \u3067\u4f7f\u7528\u3055\u308c\u308b API \u306b\u3064\u3044\u3066\u8a18\u8f09\u3059\u308b\u3002Autoware \u3068\u3044\u3046\u5358\u8a9e\u304c\u3069\u3053\u307e\u3067\u306e\u7bc4\u56f2\u3092\u6307\u3057\u793a\u3057\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u306f\u8b70\u8ad6\u304c\u5fc5\u8981\u3060\u304c\u3001\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u304a\u3044\u3066\u306f\u8eca\u4e21\u5236\u5fa1\u3084\u5468\u8fba\u88c5\u7f6e\u306a\u3069\u3092\u542b\u3081\u3001\u81ea\u52d5\u904b\u8ee2\u3092\u884c\u3046\u30b7\u30b9\u30c6\u30e0\u304c\u4f7f\u7528\u3059\u308b API \u3092\u5168\u90e8\u307e\u3068\u3081\u3066 Autoware API \u3068\u547c\u79f0\u3059\u308b\u3053\u3068\u306b\u3059\u308b\u3002","title":"\u6ce8\u610f\u4e8b\u9805"},{"location":"design/apis/ja/concept/#_2","text":"High-level API \u306f\u3001FMS \u3084\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306a\u3069\u30e6\u30fc\u30b6\u30fc\u8996\u70b9\u3067\u5fc5\u8981\u3068\u306a\u308b\u64cd\u4f5c\u3092\u5b89\u5b9a\u3057\u3066\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6a19\u306b\u8a2d\u8a08\u3059\u308b\u3002\u5177\u4f53\u7684\u306a\u5b9f\u88c5\u306f\u8eca\u4e21\u3084\u74b0\u5883\u306b\u3088\u308a\u7570\u306a\u308b\u305f\u3081\u3001\u5404\u5c64\u3067\u5b9a\u7fa9\u3057\u305f API \u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u67d4\u8edf\u306b\u52d5\u4f5c\u5909\u66f4\u3059\u308b\u4ed5\u7d44\u307f\u3092\u5185\u90e8\u3067\u7528\u610f\u3057\u3066\u304a\u304d\u3001High-level API \u306e\u4ed5\u69d8\u3092\u7dad\u6301\u3057\u305f\u307e\u307e\u69d8\u3005\u306a\u8981\u671b\u3078\u306e\u5bfe\u5fdc\u3092\u53ef\u80fd\u3068\u3059\u308b\u3002","title":"\u8a2d\u8a08\u65b9\u91dd"},{"location":"design/apis/ja/concept/#_3","text":"","title":"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc"},{"location":"design/apis/ja/concept/#api","text":"High-level API \u8eca\u4e21\u306e\u4e57\u5ba2\u3084\u6574\u5099\u54e1\u3001FMS\u306a\u3069\u81ea\u52d5\u904b\u8ee2\u30b7\u30b9\u30c6\u30e0(Autoware)\u306eUser\u306b\u63d0\u4f9b\u3059\u308bAPI\u3002\u5168\u3066\u306e\u8eca\u4e21\u3067\u4ed5\u69d8\u304c\u5171\u901a\u3067\u3042\u308a\u9577\u671f\u9593\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u6c42\u3081\u3089\u308c\u308b\u3002 Low-level API \u81ea\u52d5\u904b\u8ee2\u30b7\u30b9\u30c6\u30e0(Autoware)\u5185\u90e8\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u63d0\u4f9b\u3059\u308bAPI\u3002\u4e3b\u306bFOA\u306a\u3069\u306e\u5185\u90e8\u30c4\u30fc\u30eb\u3084\u3001\u8eca\u4e21\u56fa\u6709\u306e\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u3092\u5236\u5fa1\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u3002","title":"API \u306e\u5206\u985e"},{"location":"design/apis/ja/concept/#_4","text":"Behavior API\u306e\u52d5\u4f5c\u304cTopic/Service/Action\u306e\u3069\u308c\u306a\u306e\u304b\u3092\u6307\u5b9a\u3059\u308b\u3002 Category API\u306e\u5b9f\u88c5\u304c\u5fc5\u9808\u306a\u306e\u304b\u4efb\u610f\u306a\u306e\u304b\u3092\u6307\u5b9a\u3059\u308b\u3002 Data Type API\u3067\u4f7f\u7528\u3055\u308c\u308b\u30c7\u30fc\u30bf\u306e\u5f62\u3092\u6307\u5b9a\u3059\u308b\u3002 QoS API\u306e\u30b5\u30fc\u30d3\u30b9\u30ec\u30d9\u30eb\u3092\u6307\u5b9a\u3059\u308b\u3002 Topic Rate API\u306e\u52d5\u4f5c\u304cTopic\u5834\u5408\u306b\u9001\u53d7\u4fe1\u306e\u5468\u671f\u3092\u6307\u5b9a\u3059\u308b\u3002 Timeout API\u306e\u52d5\u4f5c\u304cService\u5834\u5408\u306b\u5fdc\u7b54\u6642\u9593\u306e\u5236\u9650\u3092\u6307\u5b9a\u3059\u308b\u3002","title":"\u5171\u901a\u8a2d\u5b9a\u9805\u76ee"},{"location":"design/apis/ja/list/","text":"List of Autoware API # External API # Behavior Name Data Type service /api/external/get/version autoware_external_api_msgs/srv/GetVersion service /api/external/set/service autoware_external_api_msgs/srv/SetService topic /api/external/get/service autoware_external_api_msgs/msg/Service topic /api/external/get/diagnostics autoware_external_api_msgs/msg/ClassifiedDiagnostics service /api/external/set/engage autoware_external_api_msgs/srv/Engage topic /api/external/get/engage autoware_external_api_msgs/msg/EngageStatus service /api/external/set/emergency autoware_external_api_msgs/srv/SetEmergency service /api/external/set/door autoware_external_api_msgs/srv/SetDoor service /api/external/set/initialize_pose autoware_external_api_msgs/srv/InitializePose service /api/external/set/initialize_pose_auto autoware_external_api_msgs/srv/InitializePoseAuto service /api/external/set/route autoware_external_api_msgs/srv/SetRoute topic /api/external/get/route autoware_external_api_msgs/msg/Route service /api/external/set/clear_route autoware_external_api_msgs/srv/ClearRoute service /api/external/set/operator autoware_external_api_msgs/srv/SetOperator topic /api/external/get/operator autoware_external_api_msgs/msg/Operator service /api/external/set/observer autoware_external_api_msgs/srv/SetObserver topic /api/external/get/observer autoware_external_api_msgs/msg/Observer topic /api/external/get/map/info/hash autoware_external_api_msgs/msg/MapHash service /api/external/set/pause_driving autoware_external_api_msgs/srv/PauseDriving service /api/external/set/velocity_limit autoware_external_api_msgs/srv/SetVelocityLimit topic /api/external/set/command/control autoware_external_api_msgs/msg/ControlCommandStamped topic /api/external/set/command/gear_shift autoware_external_api_msgs/msg/GearShiftStamped topic /api/external/set/command/turn_signal autoware_external_api_msgs/msg/TurnSignalStamped topic /api/external/set/command/heartbeat autoware_external_api_msgs/msg/Heartbeat Internal API # Behavior Name Data Type service /api/internal/set/engage autoware_external_api_msgs/srv/Engage topic /api/internal/get/engage autoware_vehicle_msgs/msg/Engage service /api/internal/set/emergency autoware_external_api_msgs/srv/SetEmergency service /api/internal/set/initialize_pose autoware_external_api_msgs/srv/InitializePose service /api/internal/set/initialize_pose_auto autoware_external_api_msgs/srv/InitializePoseAuto service /api/internal/set/route autoware_external_api_msgs/srv/SetRoute topic /api/internal/get/route autoware_external_api_msgs/msg/Route service /api/internal/set/clear_route autoware_external_api_msgs/srv/ClearRoute service /api/internal/set/goal autoware_external_api_msgs/srv/SetPose service /api/internal/set/checkpoint autoware_external_api_msgs/srv/SetPose service /api/internal/set/operator autoware_external_api_msgs/srv/SetOperator topic /api/internal/get/operator autoware_external_api_msgs/msg/Operator service /api/internal/set/observer autoware_external_api_msgs/srv/SetObserver topic /api/internal/get/observer autoware_external_api_msgs/msg/Observer topic /api/internal/get/map/info/hash autoware_external_api_msgs/msg/MapHash service /api/internal/set/pause_driving autoware_external_api_msgs/srv/PauseDriving service /api/internal/set/velocity_limit autoware_external_api_msgs/srv/SetVelocityLimit topic /api/internal/get/velocity_limit autoware_planning_msgs/msg/VelocityLimit topic /api/internal/set/traffic_light_states autoware_perception_msgs/msg/TrafficLightStateArray topic /api/internal/set/crosswalk_states autoware_api_msgs/msg/CrosswalkStatus topic /api/internal/set/intersection_states autoware_api_msgs/msg/IntersectionStatus","title":"List"},{"location":"design/apis/ja/list/#list-of-autoware-api","text":"","title":"List of Autoware API"},{"location":"design/apis/ja/list/#external-api","text":"Behavior Name Data Type service /api/external/get/version autoware_external_api_msgs/srv/GetVersion service /api/external/set/service autoware_external_api_msgs/srv/SetService topic /api/external/get/service autoware_external_api_msgs/msg/Service topic /api/external/get/diagnostics autoware_external_api_msgs/msg/ClassifiedDiagnostics service /api/external/set/engage autoware_external_api_msgs/srv/Engage topic /api/external/get/engage autoware_external_api_msgs/msg/EngageStatus service /api/external/set/emergency autoware_external_api_msgs/srv/SetEmergency service /api/external/set/door autoware_external_api_msgs/srv/SetDoor service /api/external/set/initialize_pose autoware_external_api_msgs/srv/InitializePose service /api/external/set/initialize_pose_auto autoware_external_api_msgs/srv/InitializePoseAuto service /api/external/set/route autoware_external_api_msgs/srv/SetRoute topic /api/external/get/route autoware_external_api_msgs/msg/Route service /api/external/set/clear_route autoware_external_api_msgs/srv/ClearRoute service /api/external/set/operator autoware_external_api_msgs/srv/SetOperator topic /api/external/get/operator autoware_external_api_msgs/msg/Operator service /api/external/set/observer autoware_external_api_msgs/srv/SetObserver topic /api/external/get/observer autoware_external_api_msgs/msg/Observer topic /api/external/get/map/info/hash autoware_external_api_msgs/msg/MapHash service /api/external/set/pause_driving autoware_external_api_msgs/srv/PauseDriving service /api/external/set/velocity_limit autoware_external_api_msgs/srv/SetVelocityLimit topic /api/external/set/command/control autoware_external_api_msgs/msg/ControlCommandStamped topic /api/external/set/command/gear_shift autoware_external_api_msgs/msg/GearShiftStamped topic /api/external/set/command/turn_signal autoware_external_api_msgs/msg/TurnSignalStamped topic /api/external/set/command/heartbeat autoware_external_api_msgs/msg/Heartbeat","title":"External API"},{"location":"design/apis/ja/list/#internal-api","text":"Behavior Name Data Type service /api/internal/set/engage autoware_external_api_msgs/srv/Engage topic /api/internal/get/engage autoware_vehicle_msgs/msg/Engage service /api/internal/set/emergency autoware_external_api_msgs/srv/SetEmergency service /api/internal/set/initialize_pose autoware_external_api_msgs/srv/InitializePose service /api/internal/set/initialize_pose_auto autoware_external_api_msgs/srv/InitializePoseAuto service /api/internal/set/route autoware_external_api_msgs/srv/SetRoute topic /api/internal/get/route autoware_external_api_msgs/msg/Route service /api/internal/set/clear_route autoware_external_api_msgs/srv/ClearRoute service /api/internal/set/goal autoware_external_api_msgs/srv/SetPose service /api/internal/set/checkpoint autoware_external_api_msgs/srv/SetPose service /api/internal/set/operator autoware_external_api_msgs/srv/SetOperator topic /api/internal/get/operator autoware_external_api_msgs/msg/Operator service /api/internal/set/observer autoware_external_api_msgs/srv/SetObserver topic /api/internal/get/observer autoware_external_api_msgs/msg/Observer topic /api/internal/get/map/info/hash autoware_external_api_msgs/msg/MapHash service /api/internal/set/pause_driving autoware_external_api_msgs/srv/PauseDriving service /api/internal/set/velocity_limit autoware_external_api_msgs/srv/SetVelocityLimit topic /api/internal/get/velocity_limit autoware_planning_msgs/msg/VelocityLimit topic /api/internal/set/traffic_light_states autoware_perception_msgs/msg/TrafficLightStateArray topic /api/internal/set/crosswalk_states autoware_api_msgs/msg/CrosswalkStatus topic /api/internal/set/intersection_states autoware_api_msgs/msg/IntersectionStatus","title":"Internal API"},{"location":"design/apis/ja/state/","text":"State Machine of Autoware API (Proposal) # Power State # \u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u3068\u7d42\u4e86\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u4e3b\u306b\u96fb\u6e90\u6295\u5165\u5f8c\u306b\u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u304c\u5b8c\u4e86\u3057\u305f\u304b\u306e\u5224\u5b9a\u3084\u3001\u96fb\u6e90\u5207\u65ad\u304c\u53ef\u80fd\u304b\u3092\u7ba1\u7406\u3059\u308b\u3002 State Description Startup \u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 PowerOn \u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u304c\u5b8c\u4e86\u3057\u3001\u6a5f\u80fd\u3092\u63d0\u4f9b\u3067\u304d\u308b\u72b6\u614b\u3002 Shutdown \u30b7\u30b9\u30c6\u30e0\u306e\u7d42\u4e86\u51e6\u7406\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 PowerOff \u30b7\u30b9\u30c6\u30e0\u306e\u7d42\u4e86\u51e6\u7406\u304c\u5b8c\u4e86\u3057\u3001\u96fb\u6e90\u3092\u5207\u3063\u3066\u826f\u3044\u72b6\u614b\u3002 Operator State # \u8eca\u4e21\u306e\u64cd\u4f5c\u8005\u3092\u7ba1\u7406\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u8eca\u4e21\u3092\u76f4\u63a5\u624b\u52d5\u3067\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3068\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u4ecb\u3057\u305f\u9593\u63a5\u624b\u52d5\u64cd\u4f5c\u3001Autoware \u306b\u3088\u308b\u81ea\u5f8b\u5236\u5fa1\u306b\u5206\u304b\u308c\u308b\u3002 State Description Driver \u8eca\u4e21\u304c\u7528\u610f\u3057\u305f\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u4f7f\u7528\u3057\u3066\u624b\u52d5\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Observer \u8fd1\u8ddd\u96e2\u307e\u305f\u306f\u9060\u8ddd\u96e2\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306b\u3088\u308a\u624b\u52d5\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Takeover Operating \u81ea\u7acb\u5236\u5fa1\u3078\u306e\u79fb\u884c\u304c\u53ef\u80fd\u304b\u3092\u5224\u65ad\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Driving \u81ea\u5f8b\u5236\u5fa1\u306b\u3088\u308a\u8d70\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Takeover Requesting ODD \u9038\u8131\u306a\u3069\u306b\u3088\u308a\u624b\u52d5\u904b\u8ee2\u3078\u306e\u79fb\u884c\u3092\u8981\u6c42\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Observer State # \u8eca\u4e21\u306e\u76e3\u8996\u8005\u3092\u7ba1\u7406\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u8eca\u4e21\u306b\u554f\u984c\u304c\u8d77\u304d\u305f\u3068\u304d\u306b\u64cd\u4f5c\u3092\u5f15\u304d\u7d99\u3050\u5bfe\u8c61\u3092\u6307\u5b9a\u3059\u308b\u3002 State Description None \u8eca\u4e21\u306e\u904b\u8ee2\u624b\u304c\u76e3\u8996\u8005\u3092\u517c\u306d\u3066\u3044\u308b\u72b6\u614b\u3002 Local \u8eca\u4e21\u304c\u8fd1\u63a5\u76e3\u8996\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 Remote \u8eca\u4e21\u304c\u9060\u9694\u76e3\u8996\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 Driving State # \u81ea\u5f8b\u8d70\u884c\u3092\u884c\u3046\u305f\u3081\u306e\u521d\u671f\u5316\u3068\u767a\u8eca\u5224\u65ad\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002 State Description Initializing \u30ce\u30fc\u30c9\u306e\u8d77\u52d5\u4e2d\u3084\u81ea\u5df1\u4f4d\u7f6e\u63a8\u5b9a\u672a\u5b8c\u4e86\u306a\u3069\u306e\u521d\u671f\u5316\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u308b\u72b6\u614b\u3002 Preparing \u30eb\u30fc\u30c8\u306e\u914d\u4fe1\u5f85\u3061\u3084\u30c9\u30a2\u306e\u958b\u9589\u4e2d\u306a\u3069\u767a\u8eca\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u304c\u6574\u3063\u3066\u3044\u306a\u3044\u72b6\u614b\u3002 Ready \u767a\u8eca\u306e\u6e96\u5099\u304c\u3067\u304d\u3066\u304a\u308a\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u64cd\u4f5c\u3092\u5f85\u3063\u3066\u3044\u308b\u72b6\u614b\u3002 Driving \u76ee\u7684\u5730\u306b\u5411\u304b\u3063\u3066\u8d70\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Route State # \u30eb\u30fc\u30c8\u306e\u8a08\u753b\u3068\u8a2d\u5b9a\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002 State Description Accepting \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u53d7\u3051\u4ed8\u3051\u3066\u3044\u308b\u72b6\u614b\u3002 Planning \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u8a08\u753b\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Planned \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u306e\u8a08\u753b\u304c\u5b8c\u4e86\u3057\u3066\u3044\u3066\u53cd\u6620\u3067\u304d\u308b\u72b6\u614b\u3002 Updating \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u3068\u3057\u3066\u53cd\u6620\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Failed \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u306b\u53cd\u6620\u5931\u6557\u3057\u305f\u72b6\u614b\u3002 Succeeded \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u306b\u53cd\u6620\u6210\u529f\u3057\u305f\u72b6\u614b\u3002 Waiting \u30eb\u30fc\u30c8\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u72b6\u614b\u3002 Driving \u30eb\u30fc\u30c8\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 Changing \u30eb\u30fc\u30c8\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u72b6\u614b\u3002 Fail Safe State # \u30b7\u30b9\u30c6\u30e0\u7570\u5e38\u6642\u306e\u8eca\u4e21\u5236\u5fa1\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\uff08\u624b\u52d5\u64cd\u4f5c\u30fb\u81ea\u5f8b\u5236\u5fa1\u306a\u3069\uff09\u306b\u3088\u308a\u7570\u5e38\u3068\u5224\u5b9a\u3055\u308c\u308b\u6761\u4ef6\u306f\u7570\u306a\u308a\u3001\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u8eca\u4e21\u3092\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3055\u305b\u308b\u3002 State Description Normal \u7570\u5e38\u304c\u306a\u304f\u8eca\u4e21\u304c\u554f\u984c\u306a\u304f\u5236\u5fa1\u3067\u304d\u308b\u72b6\u614b\u3002 OverrideRequesting \u7570\u5e38\u304c\u3042\u308b\u304c\u307e\u3060 MRM \u3092\u52d5\u4f5c\u3055\u305b\u305a\u904b\u8ee2\u624b\u306b\u5f15\u7d99\u8981\u6c42\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 MrmOperating \u7570\u5e38\u304c\u3042\u308a MRM \u304c\u8eca\u4e21\u3092\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 MrmSucceeded \u7570\u5e38\u304c\u3042\u308a MRM \u306b\u3088\u308a\u8eca\u4e21\u304c\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3057\u305f\u72b6\u614b\u3002 MrmFailed \u7570\u5e38\u304c\u3042\u308a MRM \u306b\u3088\u308a\u8eca\u4e21\u304c\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3067\u304d\u306a\u304b\u3063\u305f\u72b6\u614b\u3002","title":"State"},{"location":"design/apis/ja/state/#state-machine-of-autoware-api-proposal","text":"","title":"State Machine of Autoware API (Proposal)"},{"location":"design/apis/ja/state/#power-state","text":"\u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u3068\u7d42\u4e86\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u4e3b\u306b\u96fb\u6e90\u6295\u5165\u5f8c\u306b\u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u304c\u5b8c\u4e86\u3057\u305f\u304b\u306e\u5224\u5b9a\u3084\u3001\u96fb\u6e90\u5207\u65ad\u304c\u53ef\u80fd\u304b\u3092\u7ba1\u7406\u3059\u308b\u3002 State Description Startup \u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 PowerOn \u30b7\u30b9\u30c6\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u304c\u5b8c\u4e86\u3057\u3001\u6a5f\u80fd\u3092\u63d0\u4f9b\u3067\u304d\u308b\u72b6\u614b\u3002 Shutdown \u30b7\u30b9\u30c6\u30e0\u306e\u7d42\u4e86\u51e6\u7406\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 PowerOff \u30b7\u30b9\u30c6\u30e0\u306e\u7d42\u4e86\u51e6\u7406\u304c\u5b8c\u4e86\u3057\u3001\u96fb\u6e90\u3092\u5207\u3063\u3066\u826f\u3044\u72b6\u614b\u3002","title":"Power State"},{"location":"design/apis/ja/state/#operator-state","text":"\u8eca\u4e21\u306e\u64cd\u4f5c\u8005\u3092\u7ba1\u7406\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u8eca\u4e21\u3092\u76f4\u63a5\u624b\u52d5\u3067\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3068\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u4ecb\u3057\u305f\u9593\u63a5\u624b\u52d5\u64cd\u4f5c\u3001Autoware \u306b\u3088\u308b\u81ea\u5f8b\u5236\u5fa1\u306b\u5206\u304b\u308c\u308b\u3002 State Description Driver \u8eca\u4e21\u304c\u7528\u610f\u3057\u305f\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u4f7f\u7528\u3057\u3066\u624b\u52d5\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Observer \u8fd1\u8ddd\u96e2\u307e\u305f\u306f\u9060\u8ddd\u96e2\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306b\u3088\u308a\u624b\u52d5\u64cd\u4f5c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Takeover Operating \u81ea\u7acb\u5236\u5fa1\u3078\u306e\u79fb\u884c\u304c\u53ef\u80fd\u304b\u3092\u5224\u65ad\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Driving \u81ea\u5f8b\u5236\u5fa1\u306b\u3088\u308a\u8d70\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Takeover Requesting ODD \u9038\u8131\u306a\u3069\u306b\u3088\u308a\u624b\u52d5\u904b\u8ee2\u3078\u306e\u79fb\u884c\u3092\u8981\u6c42\u3057\u3066\u3044\u308b\u72b6\u614b\u3002","title":"Operator State"},{"location":"design/apis/ja/state/#observer-state","text":"\u8eca\u4e21\u306e\u76e3\u8996\u8005\u3092\u7ba1\u7406\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u8eca\u4e21\u306b\u554f\u984c\u304c\u8d77\u304d\u305f\u3068\u304d\u306b\u64cd\u4f5c\u3092\u5f15\u304d\u7d99\u3050\u5bfe\u8c61\u3092\u6307\u5b9a\u3059\u308b\u3002 State Description None \u8eca\u4e21\u306e\u904b\u8ee2\u624b\u304c\u76e3\u8996\u8005\u3092\u517c\u306d\u3066\u3044\u308b\u72b6\u614b\u3002 Local \u8eca\u4e21\u304c\u8fd1\u63a5\u76e3\u8996\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 Remote \u8eca\u4e21\u304c\u9060\u9694\u76e3\u8996\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002","title":"Observer State"},{"location":"design/apis/ja/state/#driving-state","text":"\u81ea\u5f8b\u8d70\u884c\u3092\u884c\u3046\u305f\u3081\u306e\u521d\u671f\u5316\u3068\u767a\u8eca\u5224\u65ad\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002 State Description Initializing \u30ce\u30fc\u30c9\u306e\u8d77\u52d5\u4e2d\u3084\u81ea\u5df1\u4f4d\u7f6e\u63a8\u5b9a\u672a\u5b8c\u4e86\u306a\u3069\u306e\u521d\u671f\u5316\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u308b\u72b6\u614b\u3002 Preparing \u30eb\u30fc\u30c8\u306e\u914d\u4fe1\u5f85\u3061\u3084\u30c9\u30a2\u306e\u958b\u9589\u4e2d\u306a\u3069\u767a\u8eca\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u304c\u6574\u3063\u3066\u3044\u306a\u3044\u72b6\u614b\u3002 Ready \u767a\u8eca\u306e\u6e96\u5099\u304c\u3067\u304d\u3066\u304a\u308a\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u64cd\u4f5c\u3092\u5f85\u3063\u3066\u3044\u308b\u72b6\u614b\u3002 Driving \u76ee\u7684\u5730\u306b\u5411\u304b\u3063\u3066\u8d70\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002","title":"Driving State"},{"location":"design/apis/ja/state/#route-state","text":"\u30eb\u30fc\u30c8\u306e\u8a08\u753b\u3068\u8a2d\u5b9a\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002 State Description Accepting \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u53d7\u3051\u4ed8\u3051\u3066\u3044\u308b\u72b6\u614b\u3002 Planning \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u8a08\u753b\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Planned \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u306e\u8a08\u753b\u304c\u5b8c\u4e86\u3057\u3066\u3044\u3066\u53cd\u6620\u3067\u304d\u308b\u72b6\u614b\u3002 Updating \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u3068\u3057\u3066\u53cd\u6620\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 Failed \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u306b\u53cd\u6620\u5931\u6557\u3057\u305f\u72b6\u614b\u3002 Succeeded \u65b0\u3057\u3044\u30eb\u30fc\u30c8\u3092\u73fe\u5728\u306e\u30eb\u30fc\u30c8\u306b\u53cd\u6620\u6210\u529f\u3057\u305f\u72b6\u614b\u3002 Waiting \u30eb\u30fc\u30c8\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u72b6\u614b\u3002 Driving \u30eb\u30fc\u30c8\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u72b6\u614b\u3002 Changing \u30eb\u30fc\u30c8\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u72b6\u614b\u3002","title":"Route State"},{"location":"design/apis/ja/state/#fail-safe-state","text":"\u30b7\u30b9\u30c6\u30e0\u7570\u5e38\u6642\u306e\u8eca\u4e21\u5236\u5fa1\u306b\u95a2\u3059\u308b\u72b6\u614b\u9077\u79fb\u3002\u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\uff08\u624b\u52d5\u64cd\u4f5c\u30fb\u81ea\u5f8b\u5236\u5fa1\u306a\u3069\uff09\u306b\u3088\u308a\u7570\u5e38\u3068\u5224\u5b9a\u3055\u308c\u308b\u6761\u4ef6\u306f\u7570\u306a\u308a\u3001\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u8eca\u4e21\u3092\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3055\u305b\u308b\u3002 State Description Normal \u7570\u5e38\u304c\u306a\u304f\u8eca\u4e21\u304c\u554f\u984c\u306a\u304f\u5236\u5fa1\u3067\u304d\u308b\u72b6\u614b\u3002 OverrideRequesting \u7570\u5e38\u304c\u3042\u308b\u304c\u307e\u3060 MRM \u3092\u52d5\u4f5c\u3055\u305b\u305a\u904b\u8ee2\u624b\u306b\u5f15\u7d99\u8981\u6c42\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 MrmOperating \u7570\u5e38\u304c\u3042\u308a MRM \u304c\u8eca\u4e21\u3092\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3057\u3066\u3044\u308b\u72b6\u614b\u3002 MrmSucceeded \u7570\u5e38\u304c\u3042\u308a MRM \u306b\u3088\u308a\u8eca\u4e21\u304c\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3057\u305f\u72b6\u614b\u3002 MrmFailed \u7570\u5e38\u304c\u3042\u308a MRM \u306b\u3088\u308a\u8eca\u4e21\u304c\u5b89\u5168\u306a\u72b6\u614b\u306b\u79fb\u884c\u3067\u304d\u306a\u304b\u3063\u305f\u72b6\u614b\u3002","title":"Fail Safe State"},{"location":"design/apis/ja/api/external/get/diagnostics/","text":"/api/external/get/diagnostics # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/ClassifiedDiagnostics Description # \u8eca\u4e21\u306e\u5404\u7a2e\u8a3a\u65ad\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u306b\u5fdc\u3058\u3066\u5206\u985e\u3057\u305f\u5404\u7a2e\u8a3a\u65ad\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3002","title":"/api/external/get/diagnostics"},{"location":"design/apis/ja/api/external/get/diagnostics/#apiexternalgetdiagnostics","text":"","title":"/api/external/get/diagnostics"},{"location":"design/apis/ja/api/external/get/diagnostics/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/ClassifiedDiagnostics","title":"Classification"},{"location":"design/apis/ja/api/external/get/diagnostics/#description","text":"\u8eca\u4e21\u306e\u5404\u7a2e\u8a3a\u65ad\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/diagnostics/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u306b\u5fdc\u3058\u3066\u5206\u985e\u3057\u305f\u5404\u7a2e\u8a3a\u65ad\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/engage/","text":"/api/external/get/engage # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/EngageStatus Description # \u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u304c\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/engage"},{"location":"design/apis/ja/api/external/get/engage/#apiexternalgetengage","text":"","title":"/api/external/get/engage"},{"location":"design/apis/ja/api/external/get/engage/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/EngageStatus","title":"Classification"},{"location":"design/apis/ja/api/external/get/engage/#description","text":"\u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/engage/#requirement","text":"\u73fe\u5728\u306e\u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u304c\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/observer/","text":"/api/external/get/observer # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Observer Description # \u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3046\u5bfe\u8c61\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u73fe\u5728\u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3063\u3066\u3044\u308b\u5bfe\u8c61\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/observer"},{"location":"design/apis/ja/api/external/get/observer/#apiexternalgetobserver","text":"","title":"/api/external/get/observer"},{"location":"design/apis/ja/api/external/get/observer/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Observer","title":"Classification"},{"location":"design/apis/ja/api/external/get/observer/#description","text":"\u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3046\u5bfe\u8c61\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/observer/#requirement","text":"\u73fe\u5728\u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3063\u3066\u3044\u308b\u5bfe\u8c61\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/operator/","text":"/api/external/get/operator # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Operator Description # \u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3046\u5bfe\u8c61\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u73fe\u5728\u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3063\u3066\u3044\u308b\u5bfe\u8c61\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/operator"},{"location":"design/apis/ja/api/external/get/operator/#apiexternalgetoperator","text":"","title":"/api/external/get/operator"},{"location":"design/apis/ja/api/external/get/operator/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Operator","title":"Classification"},{"location":"design/apis/ja/api/external/get/operator/#description","text":"\u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3046\u5bfe\u8c61\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/operator/#requirement","text":"\u73fe\u5728\u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3063\u3066\u3044\u308b\u5bfe\u8c61\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/route/","text":"/api/external/get/route # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Route Description # \u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # [[/api/external/set/route]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/route"},{"location":"design/apis/ja/api/external/get/route/#apiexternalgetroute","text":"","title":"/api/external/get/route"},{"location":"design/apis/ja/api/external/get/route/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Route","title":"Classification"},{"location":"design/apis/ja/api/external/get/route/#description","text":"\u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/route/#requirement","text":"[[/api/external/set/route]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/service/","text":"/api/external/get/service # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Service Description # \u8eca\u4e21\u306e\u30b5\u30fc\u30d3\u30b9\u63d0\u4f9b\u72b6\u614b\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # [[/api/external/set/engage]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u30b5\u30fc\u30d3\u30b9\u72b6\u614b\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/service"},{"location":"design/apis/ja/api/external/get/service/#apiexternalgetservice","text":"","title":"/api/external/get/service"},{"location":"design/apis/ja/api/external/get/service/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/Service","title":"Classification"},{"location":"design/apis/ja/api/external/get/service/#description","text":"\u8eca\u4e21\u306e\u30b5\u30fc\u30d3\u30b9\u63d0\u4f9b\u72b6\u614b\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/service/#requirement","text":"[[/api/external/set/engage]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u30b5\u30fc\u30d3\u30b9\u72b6\u614b\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/version/","text":"/api/external/get/version # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/GetVersion Description # Autoware External API \u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u4ee5\u4e0b\u306e\u898f\u5247\u306b\u5f93\u3063\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u6587\u5b57\u5217 major.minor.patch \u304c\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002 major: \u5168\u4f53\u306b\u95a2\u308f\u308b\u5927\u898f\u6a21\u306a\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002 minor: \u4e92\u63db\u6027\u306e\u5931\u308f\u308c\u308b\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002 patch: \u4e92\u63db\u6027\u306e\u7dad\u6301\u3055\u308c\u308b\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002","title":"/api/external/get/version"},{"location":"design/apis/ja/api/external/get/version/#apiexternalgetversion","text":"","title":"/api/external/get/version"},{"location":"design/apis/ja/api/external/get/version/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/GetVersion","title":"Classification"},{"location":"design/apis/ja/api/external/get/version/#description","text":"Autoware External API \u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/version/#requirement","text":"\u4ee5\u4e0b\u306e\u898f\u5247\u306b\u5f93\u3063\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u6587\u5b57\u5217 major.minor.patch \u304c\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002 major: \u5168\u4f53\u306b\u95a2\u308f\u308b\u5927\u898f\u6a21\u306a\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002 minor: \u4e92\u63db\u6027\u306e\u5931\u308f\u308c\u308b\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002 patch: \u4e92\u63db\u6027\u306e\u7dad\u6301\u3055\u308c\u308b\u5909\u66f4\u304c\u3042\u3063\u305f\u3068\u304d\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/get/map/info/hash/","text":"/api/external/get/map/info/hash # Classification # Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/MapHash Description # \u73fe\u5728\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5730\u56f3\u306e\u30cf\u30c3\u30b7\u30e5\u3092\u53d6\u5f97\u3059\u308b\u3002 Requirement # \u5730\u56f3\u30c7\u30fc\u30bf\u3092\u4fdd\u6301\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u306e\u30cf\u30c3\u30b7\u30e5\u5024(SHA-256)\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"/api/external/get/map/info/hash"},{"location":"design/apis/ja/api/external/get/map/info/hash/#apiexternalgetmapinfohash","text":"","title":"/api/external/get/map/info/hash"},{"location":"design/apis/ja/api/external/get/map/info/hash/#classification","text":"Category: Mandatory Behavior: Topic DataType: autoware_external_api_msgs/msg/MapHash","title":"Classification"},{"location":"design/apis/ja/api/external/get/map/info/hash/#description","text":"\u73fe\u5728\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5730\u56f3\u306e\u30cf\u30c3\u30b7\u30e5\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/get/map/info/hash/#requirement","text":"\u5730\u56f3\u30c7\u30fc\u30bf\u3092\u4fdd\u6301\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u306e\u30cf\u30c3\u30b7\u30e5\u5024(SHA-256)\u3092\u53d6\u5f97\u3067\u304d\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/clear_route/","text":"/api/external/set/clear_route # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/ClearRoute Description # \u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u308a\u6d88\u3059\u3002 Requirement # [[/api/external/set/route]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u308a\u6d88\u3059\u3053\u3068\u3002","title":"/api/external/set/clear_route"},{"location":"design/apis/ja/api/external/set/clear_route/#apiexternalsetclear_route","text":"","title":"/api/external/set/clear_route"},{"location":"design/apis/ja/api/external/set/clear_route/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/ClearRoute","title":"Classification"},{"location":"design/apis/ja/api/external/set/clear_route/#description","text":"\u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u308a\u6d88\u3059\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/clear_route/#requirement","text":"[[/api/external/set/route]]\u306b\u3088\u308a\u8a2d\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u53d6\u308a\u6d88\u3059\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/door/","text":"/api/external/set/door # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetDoor Description # \u8eca\u4e21\u306e\u30c9\u30a2\u958b\u9589\u3092\u884c\u3046\u3002 Requirement # \u8eca\u4e21\u306e\u30c9\u30a2\u64cd\u4f5c\u3092\u884c\u3046\u3053\u3068\u3002\u8eca\u4e21\u306b\u30c9\u30a2\u304c\u306a\u3044\u5834\u5408\u306f\u5e38\u306b\u5931\u6557\u3068\u3057\u3066\u5fdc\u7b54\u3059\u308b\u3053\u3068\u3002","title":"/api/external/set/door"},{"location":"design/apis/ja/api/external/set/door/#apiexternalsetdoor","text":"","title":"/api/external/set/door"},{"location":"design/apis/ja/api/external/set/door/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetDoor","title":"Classification"},{"location":"design/apis/ja/api/external/set/door/#description","text":"\u8eca\u4e21\u306e\u30c9\u30a2\u958b\u9589\u3092\u884c\u3046\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/door/#requirement","text":"\u8eca\u4e21\u306e\u30c9\u30a2\u64cd\u4f5c\u3092\u884c\u3046\u3053\u3068\u3002\u8eca\u4e21\u306b\u30c9\u30a2\u304c\u306a\u3044\u5834\u5408\u306f\u5e38\u306b\u5931\u6557\u3068\u3057\u3066\u5fdc\u7b54\u3059\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/emergency/","text":"/api/external/set/emergency # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetEmergency Description # \u8eca\u4e21\u3092\u7dca\u6025\u505c\u6b62\u72b6\u614b\u306b\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u305f\u9069\u5207\u306a\u7dca\u6025\u505c\u6b62\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/emergency"},{"location":"design/apis/ja/api/external/set/emergency/#apiexternalsetemergency","text":"","title":"/api/external/set/emergency"},{"location":"design/apis/ja/api/external/set/emergency/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetEmergency","title":"Classification"},{"location":"design/apis/ja/api/external/set/emergency/#description","text":"\u8eca\u4e21\u3092\u7dca\u6025\u505c\u6b62\u72b6\u614b\u306b\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/emergency/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u305f\u9069\u5207\u306a\u7dca\u6025\u505c\u6b62\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/engage/","text":"/api/external/set/engage # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/Engage Description # \u8eca\u4e21\u306e\u505c\u6b62\u4fdd\u6301\u72b6\u614b\u3092\u8a2d\u5b9a\u30fb\u89e3\u9664\u3059\u308b\u3002\u81ea\u5f8b\u5236\u5fa1\u958b\u59cb\u6642\u3001\u307e\u305f\u306f\u3001\u505c\u7559\u6240\u3084\u8377\u7269\u306e\u7a4d\u307f\u4e0b\u308d\u3057\u4e2d\u306a\u3069\u3001\u4f55\u3089\u304b\u306e\u30e6\u30fc\u30b6\u30fc\u64cd\u4f5c\u304c\u884c\u308f\u308c\u308b\u307e\u3067\u8eca\u4e21\u306e\u505c\u6b62\u3092\u7d99\u7d9a\u3055\u305b\u308b\u3002 Requirement # \u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u304c true \u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u8eca\u4e21\u304c\u505c\u6b62\u4fdd\u6301\u3059\u308b\u3088\u3046\u306b\u5236\u5fa1\u4fe1\u53f7\u3092\u51fa\u529b\u3059\u308b\u3053\u3068\u3002","title":"/api/external/set/engage"},{"location":"design/apis/ja/api/external/set/engage/#apiexternalsetengage","text":"","title":"/api/external/set/engage"},{"location":"design/apis/ja/api/external/set/engage/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/Engage","title":"Classification"},{"location":"design/apis/ja/api/external/set/engage/#description","text":"\u8eca\u4e21\u306e\u505c\u6b62\u4fdd\u6301\u72b6\u614b\u3092\u8a2d\u5b9a\u30fb\u89e3\u9664\u3059\u308b\u3002\u81ea\u5f8b\u5236\u5fa1\u958b\u59cb\u6642\u3001\u307e\u305f\u306f\u3001\u505c\u7559\u6240\u3084\u8377\u7269\u306e\u7a4d\u307f\u4e0b\u308d\u3057\u4e2d\u306a\u3069\u3001\u4f55\u3089\u304b\u306e\u30e6\u30fc\u30b6\u30fc\u64cd\u4f5c\u304c\u884c\u308f\u308c\u308b\u307e\u3067\u8eca\u4e21\u306e\u505c\u6b62\u3092\u7d99\u7d9a\u3055\u305b\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/engage/#requirement","text":"\u30a8\u30f3\u30b2\u30fc\u30b8\u72b6\u614b\u304c true \u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u8eca\u4e21\u304c\u505c\u6b62\u4fdd\u6301\u3059\u308b\u3088\u3046\u306b\u5236\u5fa1\u4fe1\u53f7\u3092\u51fa\u529b\u3059\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/initialize_pose/","text":"/api/external/set/initialize_pose # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/InitializePose Description # \u6307\u5b9a\u3057\u305f\u4f4d\u7f6e\u59ff\u52e2\u3092\u3082\u3068\u306b\u3001\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u4f4d\u7f6e\u59ff\u52e2\u306b\u95a2\u3059\u308b\u4e8b\u524d\u72b6\u614b\u3092\u8003\u616e\u305b\u305a\u3001\u6307\u5b9a\u3057\u305f\u4f4d\u7f6e\u59ff\u52e2\u306e\u307f\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"/api/external/set/initialize_pose"},{"location":"design/apis/ja/api/external/set/initialize_pose/#apiexternalsetinitialize_pose","text":"","title":"/api/external/set/initialize_pose"},{"location":"design/apis/ja/api/external/set/initialize_pose/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/InitializePose","title":"Classification"},{"location":"design/apis/ja/api/external/set/initialize_pose/#description","text":"\u6307\u5b9a\u3057\u305f\u4f4d\u7f6e\u59ff\u52e2\u3092\u3082\u3068\u306b\u3001\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/initialize_pose/#requirement","text":"\u4f4d\u7f6e\u59ff\u52e2\u306b\u95a2\u3059\u308b\u4e8b\u524d\u72b6\u614b\u3092\u8003\u616e\u305b\u305a\u3001\u6307\u5b9a\u3057\u305f\u4f4d\u7f6e\u59ff\u52e2\u306e\u307f\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/initialize_pose_auto/","text":"/api/external/set/initialize_pose_auto # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/InitializePoseAuto Description # GNSS \u306b\u3088\u308b\u4f4d\u7f6e\u59ff\u52e2\u3092\u3082\u3068\u306b\u3001\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u4f4d\u7f6e\u59ff\u52e2\u306b\u95a2\u3059\u308b\u4e8b\u524d\u72b6\u614b\u3092\u8003\u616e\u305b\u305a\u3001GNSS \u306b\u3088\u308b\u4f4d\u7f6e\u59ff\u52e2\u306e\u307f\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"/api/external/set/initialize_pose_auto"},{"location":"design/apis/ja/api/external/set/initialize_pose_auto/#apiexternalsetinitialize_pose_auto","text":"","title":"/api/external/set/initialize_pose_auto"},{"location":"design/apis/ja/api/external/set/initialize_pose_auto/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/InitializePoseAuto","title":"Classification"},{"location":"design/apis/ja/api/external/set/initialize_pose_auto/#description","text":"GNSS \u306b\u3088\u308b\u4f4d\u7f6e\u59ff\u52e2\u3092\u3082\u3068\u306b\u3001\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/initialize_pose_auto/#requirement","text":"\u4f4d\u7f6e\u59ff\u52e2\u306b\u95a2\u3059\u308b\u4e8b\u524d\u72b6\u614b\u3092\u8003\u616e\u305b\u305a\u3001GNSS \u306b\u3088\u308b\u4f4d\u7f6e\u59ff\u52e2\u306e\u307f\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u4f4d\u7f6e\u59ff\u52e2\u3092\u521d\u671f\u5316\u30fb\u518d\u8a2d\u5b9a\u3059\u308b\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/observer/","text":"/api/external/set/observer # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetObserver Description # \u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3046\u5bfe\u8c61\u3092\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u6307\u5b9a\u3055\u308c\u305f\u5bfe\u8c61\u304c\u8eca\u4e21\u3092\u76e3\u8996\u3067\u304d\u308b\u72b6\u614b\u304b\u78ba\u8a8d\u3057\u3066\u5207\u308a\u66ff\u3048\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/observer"},{"location":"design/apis/ja/api/external/set/observer/#apiexternalsetobserver","text":"","title":"/api/external/set/observer"},{"location":"design/apis/ja/api/external/set/observer/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetObserver","title":"Classification"},{"location":"design/apis/ja/api/external/set/observer/#description","text":"\u8eca\u4e21\u306e\u76e3\u8996\u3092\u884c\u3046\u5bfe\u8c61\u3092\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/observer/#requirement","text":"\u6307\u5b9a\u3055\u308c\u305f\u5bfe\u8c61\u304c\u8eca\u4e21\u3092\u76e3\u8996\u3067\u304d\u308b\u72b6\u614b\u304b\u78ba\u8a8d\u3057\u3066\u5207\u308a\u66ff\u3048\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/operator/","text":"/api/external/set/operator # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetOperator Description # \u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3046\u5bfe\u8c61\u3092\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u6307\u5b9a\u3055\u308c\u305f\u5bfe\u8c61\u304c\u8eca\u4e21\u3092\u64cd\u4f5c\u3067\u304d\u308b\u72b6\u614b\u304b\u78ba\u8a8d\u3057\u3066\u5207\u308a\u66ff\u3048\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/operator"},{"location":"design/apis/ja/api/external/set/operator/#apiexternalsetoperator","text":"","title":"/api/external/set/operator"},{"location":"design/apis/ja/api/external/set/operator/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetOperator","title":"Classification"},{"location":"design/apis/ja/api/external/set/operator/#description","text":"\u8eca\u4e21\u306e\u64cd\u4f5c\u3092\u884c\u3046\u5bfe\u8c61\u3092\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/operator/#requirement","text":"\u6307\u5b9a\u3055\u308c\u305f\u5bfe\u8c61\u304c\u8eca\u4e21\u3092\u64cd\u4f5c\u3067\u304d\u308b\u72b6\u614b\u304b\u78ba\u8a8d\u3057\u3066\u5207\u308a\u66ff\u3048\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/pause_driving/","text":"/api/external/set/pause_driving # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/PauseDriving Description # \u8eca\u4e21\u3092\u505c\u6b62\u72b6\u614b\u306b\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u305f\u9069\u5207\u306a\u505c\u6b62\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/pause_driving"},{"location":"design/apis/ja/api/external/set/pause_driving/#apiexternalsetpause_driving","text":"","title":"/api/external/set/pause_driving"},{"location":"design/apis/ja/api/external/set/pause_driving/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/PauseDriving","title":"Classification"},{"location":"design/apis/ja/api/external/set/pause_driving/#description","text":"\u8eca\u4e21\u3092\u505c\u6b62\u72b6\u614b\u306b\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/pause_driving/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u305f\u9069\u5207\u306a\u505c\u6b62\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/route/","text":"/api/external/set/route # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetRoute Description # \u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u6307\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u8d70\u884c\u7d4c\u8def\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3002","title":"/api/external/set/route"},{"location":"design/apis/ja/api/external/set/route/#apiexternalsetroute","text":"","title":"/api/external/set/route"},{"location":"design/apis/ja/api/external/set/route/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetRoute","title":"Classification"},{"location":"design/apis/ja/api/external/set/route/#description","text":"\u8eca\u4e21\u306e\u904b\u884c\u30eb\u30fc\u30c8\u3092\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/route/#requirement","text":"\u6307\u5b9a\u3055\u308c\u305f\u904b\u884c\u30eb\u30fc\u30c8\u3092\u7528\u3044\u3066\u8eca\u4e21\u306e\u8d70\u884c\u7d4c\u8def\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/service/","text":"/api/external/set/service # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetService Description # \u8eca\u4e21\u306e\u30b5\u30fc\u30d3\u30b9\u63d0\u4f9b\u72b6\u614b\u3092\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u6307\u5b9a\u3055\u308c\u305f\u30b5\u30fc\u30d3\u30b9\u72b6\u614b\u3092\u4fdd\u6301\u3059\u308b\u3053\u3068\u3002","title":"/api/external/set/service"},{"location":"design/apis/ja/api/external/set/service/#apiexternalsetservice","text":"","title":"/api/external/set/service"},{"location":"design/apis/ja/api/external/set/service/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetService","title":"Classification"},{"location":"design/apis/ja/api/external/set/service/#description","text":"\u8eca\u4e21\u306e\u30b5\u30fc\u30d3\u30b9\u63d0\u4f9b\u72b6\u614b\u3092\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/service/#requirement","text":"\u6307\u5b9a\u3055\u308c\u305f\u30b5\u30fc\u30d3\u30b9\u72b6\u614b\u3092\u4fdd\u6301\u3059\u308b\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/velocity_limit/","text":"/api/external/set/velocity_limit # Classification # Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetVelocityLimit Description # \u8eca\u4e21\u306e\u5236\u9650\u901f\u5ea6\u3092\u8a2d\u5b9a\u3059\u308b\u3002 Requirement # \u6307\u5b9a\u3055\u308c\u305f\u5236\u9650\u901f\u5ea6\u4ee5\u4e0b\u306b\u306a\u308b\u3088\u3046\u306b\u8eca\u4e21\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002\u73fe\u5728\u306e\u901f\u5ea6\u304c\u5236\u9650\u901f\u5ea6\u3092\u8d85\u3048\u3066\u3044\u308b\u5834\u5408\u3001\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u3066\u9069\u5207\u306b\u5236\u9650\u901f\u5ea6\u4ee5\u4e0b\u306b\u306a\u308b\u3088\u3046\u306a\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/velocity_limit"},{"location":"design/apis/ja/api/external/set/velocity_limit/#apiexternalsetvelocity_limit","text":"","title":"/api/external/set/velocity_limit"},{"location":"design/apis/ja/api/external/set/velocity_limit/#classification","text":"Category: Mandatory Behavior: Service DataType: autoware_external_api_msgs/srv/SetVelocityLimit","title":"Classification"},{"location":"design/apis/ja/api/external/set/velocity_limit/#description","text":"\u8eca\u4e21\u306e\u5236\u9650\u901f\u5ea6\u3092\u8a2d\u5b9a\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/velocity_limit/#requirement","text":"\u6307\u5b9a\u3055\u308c\u305f\u5236\u9650\u901f\u5ea6\u4ee5\u4e0b\u306b\u306a\u308b\u3088\u3046\u306b\u8eca\u4e21\u306e\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002\u73fe\u5728\u306e\u901f\u5ea6\u304c\u5236\u9650\u901f\u5ea6\u3092\u8d85\u3048\u3066\u3044\u308b\u5834\u5408\u3001\u8eca\u4e21\u306e\u72b6\u614b\u3092\u8003\u616e\u3057\u3066\u9069\u5207\u306b\u5236\u9650\u901f\u5ea6\u4ee5\u4e0b\u306b\u306a\u308b\u3088\u3046\u306a\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/command/control/","text":"/api/external/set/command/control # Classification # Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/ControlCommandStamped Description # \u8eca\u4e21\u306e\u30a2\u30af\u30bb\u30eb\u30fb\u30d6\u30ec\u30fc\u30ad\u30fb\u30b9\u30c6\u30a2\u30ea\u30f3\u30b0\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/command/control"},{"location":"design/apis/ja/api/external/set/command/control/#apiexternalsetcommandcontrol","text":"","title":"/api/external/set/command/control"},{"location":"design/apis/ja/api/external/set/command/control/#classification","text":"Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/ControlCommandStamped","title":"Classification"},{"location":"design/apis/ja/api/external/set/command/control/#description","text":"\u8eca\u4e21\u306e\u30a2\u30af\u30bb\u30eb\u30fb\u30d6\u30ec\u30fc\u30ad\u30fb\u30b9\u30c6\u30a2\u30ea\u30f3\u30b0\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/command/control/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/command/gear_shift/","text":"/api/external/set/command/gear_shift # Classification # Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/GearShiftStamped Description # \u8eca\u4e21\u306e\u5909\u901f\u6a5f\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/command/gear_shift"},{"location":"design/apis/ja/api/external/set/command/gear_shift/#apiexternalsetcommandgear_shift","text":"","title":"/api/external/set/command/gear_shift"},{"location":"design/apis/ja/api/external/set/command/gear_shift/#classification","text":"Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/GearShiftStamped","title":"Classification"},{"location":"design/apis/ja/api/external/set/command/gear_shift/#description","text":"\u8eca\u4e21\u306e\u5909\u901f\u6a5f\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/command/gear_shift/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/command/heartbeat/","text":"/api/external/set/command/heartbeat # Classification # Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/Heartbeat Description # \u8eca\u4e21\u3068\u306e\u901a\u4fe1\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306e\u4fe1\u53f7\u3092\u9001\u4fe1\u3059\u308b\u3002 Requirement # \u3053\u306e\u4fe1\u53f7\u304c\u9014\u5207\u308c\u305f\u5834\u5408\u3001\u8eca\u4e21\u306f\u76e3\u8996\u3055\u308c\u3066\u3044\u306a\u3044\u72b6\u614b\u306b\u306a\u3063\u305f\u3068\u3057\u3066\u9069\u5207\u306a\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/command/heartbeat"},{"location":"design/apis/ja/api/external/set/command/heartbeat/#apiexternalsetcommandheartbeat","text":"","title":"/api/external/set/command/heartbeat"},{"location":"design/apis/ja/api/external/set/command/heartbeat/#classification","text":"Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/Heartbeat","title":"Classification"},{"location":"design/apis/ja/api/external/set/command/heartbeat/#description","text":"\u8eca\u4e21\u3068\u306e\u901a\u4fe1\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306e\u4fe1\u53f7\u3092\u9001\u4fe1\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/command/heartbeat/#requirement","text":"\u3053\u306e\u4fe1\u53f7\u304c\u9014\u5207\u308c\u305f\u5834\u5408\u3001\u8eca\u4e21\u306f\u76e3\u8996\u3055\u308c\u3066\u3044\u306a\u3044\u72b6\u614b\u306b\u306a\u3063\u305f\u3068\u3057\u3066\u9069\u5207\u306a\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/apis/ja/api/external/set/command/turn_signal/","text":"/api/external/set/command/turn_signal # Classification # Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/TurnSignalStamped Description # \u8eca\u4e21\u306e\u65b9\u5411\u6307\u793a\u5668\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002 Requirement # \u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"/api/external/set/command/turn_signal"},{"location":"design/apis/ja/api/external/set/command/turn_signal/#apiexternalsetcommandturn_signal","text":"","title":"/api/external/set/command/turn_signal"},{"location":"design/apis/ja/api/external/set/command/turn_signal/#classification","text":"Category: Optional Behavior: Topic DataType: autoware_external_api_msgs/msg/TurnSignalStamped","title":"Classification"},{"location":"design/apis/ja/api/external/set/command/turn_signal/#description","text":"\u8eca\u4e21\u306e\u65b9\u5411\u6307\u793a\u5668\u3092\u5236\u5fa1\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u9001\u4fe1\u3059\u308b\u3002","title":"Description"},{"location":"design/apis/ja/api/external/set/command/turn_signal/#requirement","text":"\u73fe\u5728\u306e\u8eca\u4e21\u72b6\u614b\u3092\u8003\u616e\u3057\u3001\u6307\u5b9a\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u3092\u53ef\u80fd\u306a\u9650\u308a\u53cd\u6620\u3057\u305f\u5236\u5fa1\u3092\u884c\u3046\u3053\u3068\u3002","title":"Requirement"},{"location":"design/release/Release/","text":"Release # Repository Structure # Release Mechanism # Release Flow # Branch Strategy # Reference and Product Branch Strategy #","title":"Release"},{"location":"design/release/Release/#release","text":"","title":"Release"},{"location":"design/release/Release/#repository-structure","text":"","title":"Repository Structure"},{"location":"design/release/Release/#release-mechanism","text":"","title":"Release Mechanism"},{"location":"design/release/Release/#release-flow","text":"","title":"Release Flow"},{"location":"design/release/Release/#branch-strategy","text":"","title":"Branch Strategy"},{"location":"design/release/Release/#reference-and-product-branch-strategy","text":"","title":"Reference and Product Branch Strategy"},{"location":"design/repository/Repository/","text":"TODO # Repository Overview # What is autoware.proj # autoware.proj is a meta-repository for AutowareArchitectureProposal. Since AutowareArchitectureProposal is made up of code stored in multiple, version-specific GitHub repositories, creating a new build environment would involve individually importing each repository which is both time-consuming and prone to error. To avoid both of these problems, autoware.proj was created as a meta-repository to streamline the management of all necessary repositories through the use of vcstool . autoware.proj is the top directory of the AutowareArchitectureProposal project. Therefore, this repository contains high-level information of AutowareArchitectureProposal such as the architecture design .","title":"Repository"},{"location":"design/repository/Repository/#todo","text":"","title":"TODO"},{"location":"design/repository/Repository/#repository-overview","text":"","title":"Repository Overview"},{"location":"design/repository/Repository/#what-is-autowareproj","text":"autoware.proj is a meta-repository for AutowareArchitectureProposal. Since AutowareArchitectureProposal is made up of code stored in multiple, version-specific GitHub repositories, creating a new build environment would involve individually importing each repository which is both time-consuming and prone to error. To avoid both of these problems, autoware.proj was created as a meta-repository to streamline the management of all necessary repositories through the use of vcstool . autoware.proj is the top directory of the AutowareArchitectureProposal project. Therefore, this repository contains high-level information of AutowareArchitectureProposal such as the architecture design .","title":"What is autoware.proj"},{"location":"design/software_architecture/ForDevelopers/","text":"How to setup for the specific hardware configuration # In order to test Autoware in a real vehicle, it is necessary to setup Autoware for each specific combination of vehicle, drive-by-wire system and sensors as follows: 1. Sensor TF # The sensor TF describes the positional relationship of each sensor to the vehicle's base link (defined as the center of the vehicle's rear axle) and has to be created for each configuration of sensors. Please setup following the TF design document . 2. Vehicle interface # The vehicle interface is the Autoware module that communicates with the vehicle's DBW (drive-by-wire) system, and must be created for each specific combination of vehicle and DBW. Please create an appropriate vehicle interface following the \"How to design a new vehicle interface\" section of the Vehicle stack design document . Sample vehicle interface file (for the Lexus RX 450H vehicle using AutonomouStuff's PacMod system ) 3. Vehicle info # The vehicle_info YAML configuration file contains global parameters for the vehicle's physical configuration (e.g. wheel radius) that are read by Autoware in rosparam format and published to the ROS Parameter Server. The required parameters are as follows: /vehicle_info/wheel_radius # wheel radius /vehicle_info/wheel_width # wheel width /vehicle_info/wheel_base # between front wheel center and rear wheel center /vehicle_info/wheel_tread # between left wheel center and right wheel center /vehicle_info/front_overhang # between front wheel center and vehicle front /vehicle_info/rear_overhang # between rear wheel center and vehicle rear /vehicle_info/vehicle_height # from the ground point to the highest point Sample vehicle info file (for the Lexus RX 450H) 4. Sensor launch file # The sensor.launch file defines which sensor driver nodes are launched when running Autoware, and is dependent on the specific sensors (type, OEM and model) that are to be used. Sample sensor.launch file","title":"For developers"},{"location":"design/software_architecture/ForDevelopers/#how-to-setup-for-the-specific-hardware-configuration","text":"In order to test Autoware in a real vehicle, it is necessary to setup Autoware for each specific combination of vehicle, drive-by-wire system and sensors as follows:","title":"How to setup for the specific hardware configuration"},{"location":"design/software_architecture/ForDevelopers/#1-sensor-tf","text":"The sensor TF describes the positional relationship of each sensor to the vehicle's base link (defined as the center of the vehicle's rear axle) and has to be created for each configuration of sensors. Please setup following the TF design document .","title":"1. Sensor TF"},{"location":"design/software_architecture/ForDevelopers/#2-vehicle-interface","text":"The vehicle interface is the Autoware module that communicates with the vehicle's DBW (drive-by-wire) system, and must be created for each specific combination of vehicle and DBW. Please create an appropriate vehicle interface following the \"How to design a new vehicle interface\" section of the Vehicle stack design document . Sample vehicle interface file (for the Lexus RX 450H vehicle using AutonomouStuff's PacMod system )","title":"2. Vehicle interface"},{"location":"design/software_architecture/ForDevelopers/#3-vehicle-info","text":"The vehicle_info YAML configuration file contains global parameters for the vehicle's physical configuration (e.g. wheel radius) that are read by Autoware in rosparam format and published to the ROS Parameter Server. The required parameters are as follows: /vehicle_info/wheel_radius # wheel radius /vehicle_info/wheel_width # wheel width /vehicle_info/wheel_base # between front wheel center and rear wheel center /vehicle_info/wheel_tread # between left wheel center and right wheel center /vehicle_info/front_overhang # between front wheel center and vehicle front /vehicle_info/rear_overhang # between rear wheel center and vehicle rear /vehicle_info/vehicle_height # from the ground point to the highest point Sample vehicle info file (for the Lexus RX 450H)","title":"3. Vehicle info"},{"location":"design/software_architecture/ForDevelopers/#4-sensor-launch-file","text":"The sensor.launch file defines which sensor driver nodes are launched when running Autoware, and is dependent on the specific sensors (type, OEM and model) that are to be used. Sample sensor.launch file","title":"4. Sensor launch file"},{"location":"design/software_architecture/Messages/","text":"Messages # Overview # This page describes the eight categories of message in the new architecture, along with definitions for each message. Autoware control messages Autoware lanelet2 messages Autoware perception messages Autoware planning messages Autoware system messages Autoware traffic light messages Autoware vector map messages Autoware vehicle messages Autoware control messages # ControlCommand.msg # float64 steering_angle float64 steering_angle_velocity float64 velocity float64 acceleration ControlCommandStamped.msg # Header header autoware_control_msgs/ControlCommand control Autoware lanelet2 messages # MapBin.msg # Header header string format_version string map_version int8[] data Autoware perception messages # DynamicObject.msg # uuid_msgs/UniqueID id Semantic semantic State state Shape shape DynamicObjectArray.msg # std_msgs/Header header DynamicObject[] objects DynamicObjectWithFeature.msg # DynamicObject object Feature feature DynamicObjectWithFeatureArray.msg # std_msgs/Header header DynamicObjectWithFeature[] feature_objects Feature.msg # sensor_msgs/PointCloud2 cluster sensor_msgs/RegionOfInterest roi PredictedPath.msg # geometry_msgs/PoseWithCovarianceStamped[] path float64 confidence Semantic.msg # uint8 UNKNOWN=0 uint8 CAR=1 uint8 TRUCK=2 uint8 BUS=3 uint8 BICYCLE=4 uint8 MOTORBIKE=5 uint8 PEDESTRIAN=6 uint8 ANIMAL=7 uint32 type float64 confidence Shape.msg # uint8 BOUNDING_BOX=0 uint8 CYLINDER=1 uint8 POLYGON=2 uint8 type geometry_msgs/Vector3 dimensions geometry_msgs/Polygon footprint State.msg # geometry_msgs/PoseWithCovariance pose_covariance bool orientation_reliable geometry_msgs/TwistWithCovariance twist_covariance bool twist_reliable geometry_msgs/AccelWithCovariance acceleration_covariance bool acceleration_reliable PredictedPath[] predicted_paths Autoware planning messages # LaneSequence.msg # int64[] lane_ids Path.msg # std_msgs/Header header autoware_planning_msgs/PathPoint[] points nav_msgs/OccupancyGrid drivable_area PathPoint.msg # uint8 REFERENCE=0 uint8 FIXED=1 geometry_msgs/Pose pose geometry_msgs/Twist twist uint8 type PathPointWithLaneId.msg # autoware_planning_msgs/PathPoint point int64[] lane_ids PathWithLaneId.msg # std_msgs/Header header autoware_planning_msgs/PathPointWithLaneId[] points nav_msgs/OccupancyGrid drivable_area Route.msg # std_msgs/Header header geometry_msgs/Pose goal_pose autoware_planning_msgs/RouteSection[] route_sections RouteSection.msg # int64[] lane_ids int64 preferred_lane_id int64[] continued_lane_ids Scenario.msg # string Empty=Empty string LaneDriving=LaneDriving string Parking=Parking string current_scenario string[] activating_scenarios Trajectory.msg # std_msgs/Header header autoware_planning_msgs/TrajectoryPoint[] points TrajectoryPoint.msg # geometry_msgs/Pose pose geometry_msgs/Twist twist geometry_msgs/Accel accel Autoware system messages # AutowareState.msg # string Error=Error string InitializingVehicle=InitializingVehicle string WaitingForRoute=WaitingForRoute string Planning=Planning string WaitingForEngage=WaitingForEngage string Driving=Driving string ArrivedGoal=ArrivedGoal string FailedToArriveGoal=FailedToArriveGoal string state string msg Autoware traffic light messages # LampState.msg # uint8 UNKNOWN=0 uint8 RED=1 uint8 GREEN=2 uint8 YELLOW=3 uint8 LEFT=4 uint8 RIGHT=5 uint8 UP=6 uint8 DOWN=7 uint32 type float32 confidence TrafficLightRoi.msg # sensor_msgs/RegionOfInterest roi int32 id TrafficLightRoiArray.msg # std_msgs/Header header autoware_traffic_light_msgs/TrafficLightRoi[] rois TrafficLightState.msg # autoware_traffic_light_msgs/LampState[] lamp_states int32 id TrafficLightStateArray.msg # std_msgs/Header header autoware_traffic_light_msgs/TrafficLightState[] states Autoware vector map messages # BinaryGpkgMap.msg # std_msgs/Header header string format_version string map_version int8[] data Autoware vehicle messages # ControlMode.msg # std_msgs/Header header uint8 MANUAL = 0 uint8 AUTO = 1 uint8 AUTO_STEER_ONLY = 2 uint8 AUTO_PEDAL_ONLY = 3 int32 data Pedal.msg # std_msgs/Header header float64 throttle float64 brake Shift.msg # uint8 NONE=0 uint8 PARKING=1 uint8 REVERSE=2 uint8 NEUTRAL=3 uint8 DRIVE=4 uint8 LOW=5 int32 data ShiftStamped.msg # std_msgs/Header header autoware_vehicle_msgs/Shift shift Steering.msg # std_msgs/Header header float32 data TurnSignal.msg # std_msgs/Header header uint8 NONE = 0 uint8 LEFT = 1 uint8 RIGHT = 2 uint8 HAZARD = 3 int32 data VehicleCommand.msg # std_msgs/Header header autoware_control_msgs/ControlCommand control autoware_vehicle_msgs/Shift shift int32 emergency","title":"Messages"},{"location":"design/software_architecture/Messages/#messages","text":"","title":"Messages"},{"location":"design/software_architecture/Messages/#overview","text":"This page describes the eight categories of message in the new architecture, along with definitions for each message. Autoware control messages Autoware lanelet2 messages Autoware perception messages Autoware planning messages Autoware system messages Autoware traffic light messages Autoware vector map messages Autoware vehicle messages","title":"Overview"},{"location":"design/software_architecture/Messages/#autoware-control-messages","text":"","title":"Autoware control messages"},{"location":"design/software_architecture/Messages/#controlcommandmsg","text":"float64 steering_angle float64 steering_angle_velocity float64 velocity float64 acceleration","title":"ControlCommand.msg"},{"location":"design/software_architecture/Messages/#controlcommandstampedmsg","text":"Header header autoware_control_msgs/ControlCommand control","title":"ControlCommandStamped.msg"},{"location":"design/software_architecture/Messages/#autoware-lanelet2-messages","text":"","title":"Autoware lanelet2 messages"},{"location":"design/software_architecture/Messages/#mapbinmsg","text":"Header header string format_version string map_version int8[] data","title":"MapBin.msg"},{"location":"design/software_architecture/Messages/#autoware-perception-messages","text":"","title":"Autoware perception messages"},{"location":"design/software_architecture/Messages/#dynamicobjectmsg","text":"uuid_msgs/UniqueID id Semantic semantic State state Shape shape","title":"DynamicObject.msg"},{"location":"design/software_architecture/Messages/#dynamicobjectarraymsg","text":"std_msgs/Header header DynamicObject[] objects","title":"DynamicObjectArray.msg"},{"location":"design/software_architecture/Messages/#dynamicobjectwithfeaturemsg","text":"DynamicObject object Feature feature","title":"DynamicObjectWithFeature.msg"},{"location":"design/software_architecture/Messages/#dynamicobjectwithfeaturearraymsg","text":"std_msgs/Header header DynamicObjectWithFeature[] feature_objects","title":"DynamicObjectWithFeatureArray.msg"},{"location":"design/software_architecture/Messages/#featuremsg","text":"sensor_msgs/PointCloud2 cluster sensor_msgs/RegionOfInterest roi","title":"Feature.msg"},{"location":"design/software_architecture/Messages/#predictedpathmsg","text":"geometry_msgs/PoseWithCovarianceStamped[] path float64 confidence","title":"PredictedPath.msg"},{"location":"design/software_architecture/Messages/#semanticmsg","text":"uint8 UNKNOWN=0 uint8 CAR=1 uint8 TRUCK=2 uint8 BUS=3 uint8 BICYCLE=4 uint8 MOTORBIKE=5 uint8 PEDESTRIAN=6 uint8 ANIMAL=7 uint32 type float64 confidence","title":"Semantic.msg"},{"location":"design/software_architecture/Messages/#shapemsg","text":"uint8 BOUNDING_BOX=0 uint8 CYLINDER=1 uint8 POLYGON=2 uint8 type geometry_msgs/Vector3 dimensions geometry_msgs/Polygon footprint","title":"Shape.msg"},{"location":"design/software_architecture/Messages/#statemsg","text":"geometry_msgs/PoseWithCovariance pose_covariance bool orientation_reliable geometry_msgs/TwistWithCovariance twist_covariance bool twist_reliable geometry_msgs/AccelWithCovariance acceleration_covariance bool acceleration_reliable PredictedPath[] predicted_paths","title":"State.msg"},{"location":"design/software_architecture/Messages/#autoware-planning-messages","text":"","title":"Autoware planning messages"},{"location":"design/software_architecture/Messages/#lanesequencemsg","text":"int64[] lane_ids","title":"LaneSequence.msg"},{"location":"design/software_architecture/Messages/#pathmsg","text":"std_msgs/Header header autoware_planning_msgs/PathPoint[] points nav_msgs/OccupancyGrid drivable_area","title":"Path.msg"},{"location":"design/software_architecture/Messages/#pathpointmsg","text":"uint8 REFERENCE=0 uint8 FIXED=1 geometry_msgs/Pose pose geometry_msgs/Twist twist uint8 type","title":"PathPoint.msg"},{"location":"design/software_architecture/Messages/#pathpointwithlaneidmsg","text":"autoware_planning_msgs/PathPoint point int64[] lane_ids","title":"PathPointWithLaneId.msg"},{"location":"design/software_architecture/Messages/#pathwithlaneidmsg","text":"std_msgs/Header header autoware_planning_msgs/PathPointWithLaneId[] points nav_msgs/OccupancyGrid drivable_area","title":"PathWithLaneId.msg"},{"location":"design/software_architecture/Messages/#routemsg","text":"std_msgs/Header header geometry_msgs/Pose goal_pose autoware_planning_msgs/RouteSection[] route_sections","title":"Route.msg"},{"location":"design/software_architecture/Messages/#routesectionmsg","text":"int64[] lane_ids int64 preferred_lane_id int64[] continued_lane_ids","title":"RouteSection.msg"},{"location":"design/software_architecture/Messages/#scenariomsg","text":"string Empty=Empty string LaneDriving=LaneDriving string Parking=Parking string current_scenario string[] activating_scenarios","title":"Scenario.msg"},{"location":"design/software_architecture/Messages/#trajectorymsg","text":"std_msgs/Header header autoware_planning_msgs/TrajectoryPoint[] points","title":"Trajectory.msg"},{"location":"design/software_architecture/Messages/#trajectorypointmsg","text":"geometry_msgs/Pose pose geometry_msgs/Twist twist geometry_msgs/Accel accel","title":"TrajectoryPoint.msg"},{"location":"design/software_architecture/Messages/#autoware-system-messages","text":"","title":"Autoware system messages"},{"location":"design/software_architecture/Messages/#autowarestatemsg","text":"string Error=Error string InitializingVehicle=InitializingVehicle string WaitingForRoute=WaitingForRoute string Planning=Planning string WaitingForEngage=WaitingForEngage string Driving=Driving string ArrivedGoal=ArrivedGoal string FailedToArriveGoal=FailedToArriveGoal string state string msg","title":"AutowareState.msg"},{"location":"design/software_architecture/Messages/#autoware-traffic-light-messages","text":"","title":"Autoware traffic light messages"},{"location":"design/software_architecture/Messages/#lampstatemsg","text":"uint8 UNKNOWN=0 uint8 RED=1 uint8 GREEN=2 uint8 YELLOW=3 uint8 LEFT=4 uint8 RIGHT=5 uint8 UP=6 uint8 DOWN=7 uint32 type float32 confidence","title":"LampState.msg"},{"location":"design/software_architecture/Messages/#trafficlightroimsg","text":"sensor_msgs/RegionOfInterest roi int32 id","title":"TrafficLightRoi.msg"},{"location":"design/software_architecture/Messages/#trafficlightroiarraymsg","text":"std_msgs/Header header autoware_traffic_light_msgs/TrafficLightRoi[] rois","title":"TrafficLightRoiArray.msg"},{"location":"design/software_architecture/Messages/#trafficlightstatemsg","text":"autoware_traffic_light_msgs/LampState[] lamp_states int32 id","title":"TrafficLightState.msg"},{"location":"design/software_architecture/Messages/#trafficlightstatearraymsg","text":"std_msgs/Header header autoware_traffic_light_msgs/TrafficLightState[] states","title":"TrafficLightStateArray.msg"},{"location":"design/software_architecture/Messages/#autoware-vector-map-messages","text":"","title":"Autoware vector map messages"},{"location":"design/software_architecture/Messages/#binarygpkgmapmsg","text":"std_msgs/Header header string format_version string map_version int8[] data","title":"BinaryGpkgMap.msg"},{"location":"design/software_architecture/Messages/#autoware-vehicle-messages","text":"","title":"Autoware vehicle messages"},{"location":"design/software_architecture/Messages/#controlmodemsg","text":"std_msgs/Header header uint8 MANUAL = 0 uint8 AUTO = 1 uint8 AUTO_STEER_ONLY = 2 uint8 AUTO_PEDAL_ONLY = 3 int32 data","title":"ControlMode.msg"},{"location":"design/software_architecture/Messages/#pedalmsg","text":"std_msgs/Header header float64 throttle float64 brake","title":"Pedal.msg"},{"location":"design/software_architecture/Messages/#shiftmsg","text":"uint8 NONE=0 uint8 PARKING=1 uint8 REVERSE=2 uint8 NEUTRAL=3 uint8 DRIVE=4 uint8 LOW=5 int32 data","title":"Shift.msg"},{"location":"design/software_architecture/Messages/#shiftstampedmsg","text":"std_msgs/Header header autoware_vehicle_msgs/Shift shift","title":"ShiftStamped.msg"},{"location":"design/software_architecture/Messages/#steeringmsg","text":"std_msgs/Header header float32 data","title":"Steering.msg"},{"location":"design/software_architecture/Messages/#turnsignalmsg","text":"std_msgs/Header header uint8 NONE = 0 uint8 LEFT = 1 uint8 RIGHT = 2 uint8 HAZARD = 3 int32 data","title":"TurnSignal.msg"},{"location":"design/software_architecture/Messages/#vehiclecommandmsg","text":"std_msgs/Header header autoware_control_msgs/ControlCommand control autoware_vehicle_msgs/Shift shift int32 emergency","title":"VehicleCommand.msg"},{"location":"design/software_architecture/NamingConvention/","text":"Naming Conventions # Package Names # Although Autoware does not have its own explicit naming convention, it does adhere to the guidance given in REP-144 . Thus an Autoware package name must: only consist of lowercase alphanumerics and _ separators, and start with an alphabetic character not use multiple _ separators consecutively be at least two characters long Topic Names # Default topic names # In Autoware, all topics should be named according to the guidelines in the ROS wiki . Additionally, it is strongly recommended that the default topic names specified in source code should follow these conventions: All topics must be set under private namespaces. Any global topics must have a documented explanation. All topics must be specified under one of the following namespaces within the node's private namespace. Doing so allows users to easily understand which topics are inputs and which are outputs when they look at remapping in launch files for example. input : subscribed topics output : published topics debug : published topics that are meant for debugging (e.g. for visualization) Consider, for example, a node that subscribes to pointcloud data, applies a voxel grid filter and then publishes the filtered data. In this case, the topics should be named as follows: ~input/points_original ~output/points_filtered Remapped Topics # The default topics of each node can be remapped to other topic names using a launch file. For encapsulation purposes and ease of understanding, remapped topics should be published under the namespaces of the appropriate modules as per Autoware's layered architecture. Doing so allows both developers and users to see at a glance where the topic is used in the architecture. Some key topics are listed below: /control/vehicle_cmd /perception/object_recognition/detection/objects /perception/object_recognition/objects /perception/object_recognition/tracking/objects /perception/traffic_light_recognition/traffic_light_states /planning/mission_planning/route /planning/scenario_planning/lane_driving/behavior_planning/path /planning/scenario_planning/lane_driving/trajectory /planning/scenario_planning/parking/trajectory /planning/scenario_planning/scenario /planning/scenario_planning/scenario_selector/trajectory /planning/scenario_planning/trajectory","title":"Naming convention"},{"location":"design/software_architecture/NamingConvention/#naming-conventions","text":"","title":"Naming Conventions"},{"location":"design/software_architecture/NamingConvention/#package-names","text":"Although Autoware does not have its own explicit naming convention, it does adhere to the guidance given in REP-144 . Thus an Autoware package name must: only consist of lowercase alphanumerics and _ separators, and start with an alphabetic character not use multiple _ separators consecutively be at least two characters long","title":"Package Names"},{"location":"design/software_architecture/NamingConvention/#topic-names","text":"","title":"Topic Names"},{"location":"design/software_architecture/NamingConvention/#default-topic-names","text":"In Autoware, all topics should be named according to the guidelines in the ROS wiki . Additionally, it is strongly recommended that the default topic names specified in source code should follow these conventions: All topics must be set under private namespaces. Any global topics must have a documented explanation. All topics must be specified under one of the following namespaces within the node's private namespace. Doing so allows users to easily understand which topics are inputs and which are outputs when they look at remapping in launch files for example. input : subscribed topics output : published topics debug : published topics that are meant for debugging (e.g. for visualization) Consider, for example, a node that subscribes to pointcloud data, applies a voxel grid filter and then publishes the filtered data. In this case, the topics should be named as follows: ~input/points_original ~output/points_filtered","title":"Default topic names"},{"location":"design/software_architecture/NamingConvention/#remapped-topics","text":"The default topics of each node can be remapped to other topic names using a launch file. For encapsulation purposes and ease of understanding, remapped topics should be published under the namespaces of the appropriate modules as per Autoware's layered architecture. Doing so allows both developers and users to see at a glance where the topic is used in the architecture. Some key topics are listed below: /control/vehicle_cmd /perception/object_recognition/detection/objects /perception/object_recognition/objects /perception/object_recognition/tracking/objects /perception/traffic_light_recognition/traffic_light_states /planning/mission_planning/route /planning/scenario_planning/lane_driving/behavior_planning/path /planning/scenario_planning/lane_driving/trajectory /planning/scenario_planning/parking/trajectory /planning/scenario_planning/scenario /planning/scenario_planning/scenario_selector/trajectory /planning/scenario_planning/trajectory","title":"Remapped Topics"},{"location":"design/software_architecture/NodeDiagram/","text":"Node diagram # View in fullscreen Please check the detailed documentation here .","title":"Node diagram"},{"location":"design/software_architecture/NodeDiagram/#node-diagram","text":"View in fullscreen Please check the detailed documentation here .","title":"Node diagram"},{"location":"design/software_architecture/Overview/","text":"Architecture Overview # Introduction # Currently it is difficult to improve Autoware.AI's capabilities due to a lack of concrete architecture design and a lot of technical debt, such as the tight coupling between modules as well as unclear responsibilities for each module. At Tier IV, we thought that a new architecture was needed to help accelerate the development of Autoware. The purpose of this proposal is to define a layered architecture that clarifies each module's role and simplifies the interface between them. By doing so: Autoware's internal processing becomes more transparent Collaborative development is made easier because of the reduced interdependency between modules Users can easily replace an existing module (e.g. localization) with their own software component by simply wrapping their software to fit in with Autoware's interface Note that the initial focus of this architecture design was solely on driving capability, and so the following features were left as future work: Fail safe HMI Real-time processing Redundant system State monitoring system Use Cases # When designing the architecture, the use case of last-mile travel was chosen. For example: Description: Travelling to/from a grocery store in the same city Actors: User, Vehicle with Autoware installed (hence referred to as \"Autoware\") Assumptions: Environment is an urban or suburban area less than 1 km^2. Weather conditions are fine Accurate HD map of the environment is available Basic Flow: User: Starts a browser on their phone and accesses the Autoware web app. Presses \"Summon\", and the app sends the user\u2019s GPS location to Autoware Autoware: Plans a route to the user\u2019s location, and shows it on the user\u2019s phone User: Confirms the route and presses \u201cEngage\u201d Autoware: Starts driving autonomously to the requested location and pulls over to the side of the road on arrival User: Gets in the vehicle and presses \"Go Home\" Autoware: Plans the route to the user\u2019s location User: Confirms the route and presses \u201cEngage\u201d Autoware: Drives autonomously to the user's home Requirements # To achieve this last-mile use case, the following functional requirements for Autoware were set: Autoware can plan a route to the specified goal within the type of environment described above. Autoware can drive along the planned route without violating any traffic rules. (Nice to have) Autoware provides a comfortable ride with smooth acceleration and limited jerk. Since Autoware is open source and is meant to be used/developed by people around the world, we also set some non-functional requirements for the architecture: Architecture can be extended for use with new algorithms without having to change the interface Architecture can be extended to follow traffic rules for different countries The role and interface of a module must be clearly defined High-level Architecture Design # This new architecture consists of the following six stacks. Each of these design pages contains a more detailed set of requirements and use cases specific to that stack: Sensing Localization Perception Planning Control Vehicle Map References # New architecture presentation given to the AWF Technical Steering Committee, March 2020","title":"Overview"},{"location":"design/software_architecture/Overview/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"design/software_architecture/Overview/#introduction","text":"Currently it is difficult to improve Autoware.AI's capabilities due to a lack of concrete architecture design and a lot of technical debt, such as the tight coupling between modules as well as unclear responsibilities for each module. At Tier IV, we thought that a new architecture was needed to help accelerate the development of Autoware. The purpose of this proposal is to define a layered architecture that clarifies each module's role and simplifies the interface between them. By doing so: Autoware's internal processing becomes more transparent Collaborative development is made easier because of the reduced interdependency between modules Users can easily replace an existing module (e.g. localization) with their own software component by simply wrapping their software to fit in with Autoware's interface Note that the initial focus of this architecture design was solely on driving capability, and so the following features were left as future work: Fail safe HMI Real-time processing Redundant system State monitoring system","title":"Introduction"},{"location":"design/software_architecture/Overview/#use-cases","text":"When designing the architecture, the use case of last-mile travel was chosen. For example: Description: Travelling to/from a grocery store in the same city Actors: User, Vehicle with Autoware installed (hence referred to as \"Autoware\") Assumptions: Environment is an urban or suburban area less than 1 km^2. Weather conditions are fine Accurate HD map of the environment is available Basic Flow: User: Starts a browser on their phone and accesses the Autoware web app. Presses \"Summon\", and the app sends the user\u2019s GPS location to Autoware Autoware: Plans a route to the user\u2019s location, and shows it on the user\u2019s phone User: Confirms the route and presses \u201cEngage\u201d Autoware: Starts driving autonomously to the requested location and pulls over to the side of the road on arrival User: Gets in the vehicle and presses \"Go Home\" Autoware: Plans the route to the user\u2019s location User: Confirms the route and presses \u201cEngage\u201d Autoware: Drives autonomously to the user's home","title":"Use Cases"},{"location":"design/software_architecture/Overview/#requirements","text":"To achieve this last-mile use case, the following functional requirements for Autoware were set: Autoware can plan a route to the specified goal within the type of environment described above. Autoware can drive along the planned route without violating any traffic rules. (Nice to have) Autoware provides a comfortable ride with smooth acceleration and limited jerk. Since Autoware is open source and is meant to be used/developed by people around the world, we also set some non-functional requirements for the architecture: Architecture can be extended for use with new algorithms without having to change the interface Architecture can be extended to follow traffic rules for different countries The role and interface of a module must be clearly defined","title":"Requirements"},{"location":"design/software_architecture/Overview/#high-level-architecture-design","text":"This new architecture consists of the following six stacks. Each of these design pages contains a more detailed set of requirements and use cases specific to that stack: Sensing Localization Perception Planning Control Vehicle Map","title":"High-level Architecture Design"},{"location":"design/software_architecture/Overview/#references","text":"New architecture presentation given to the AWF Technical Steering Committee, March 2020","title":"References"},{"location":"design/software_architecture/TF/","text":"TF tree in Autoware # Autoware uses the ROS TF library for transforming coordinates. The Autoware TF tree can be accessed from any module within Autoware, and is illustrated below. Frames # earth: the origin of the ECEF coordinate system (i.e. the center of the Earth). Although this frame is not currently used by any modules (and so is not shown in the diagram above), it was added to support the use of larger maps and multiple vehicles in the future. map: Local ENU coordinate. This keeps the xy-plane relatively parallel to the ground surface (thus the z-axis points upwards). All map information should be provided in this frame, or provided in a frame that can be statically transformed into this frame, since most planning calculations are done in this frame. base_link: A frame that is rigidly attached to the vehicle. Currently, this is defined as the midpoint of the rear wheels projected to the ground. sensor_frame(s): One or more frames that represent the position of individual sensors. The actual name of each frame should be a combination of the name of the sensor and its position relative to the vehicle, such as camera_front , gnss_back and lidar_top . Note that a camera should provide both camera frame and camera optical frame as suggested in REP-103 (eg: 'camera_front' and 'camera_front_optical'). Transforms # TF Type Providing Module Description earth->map static Map Map modules will provide this TF according to an origin information parameter file. map->base_link dynamic Localization The Localization module calculates the vehicle's position relative to maps specified in the map frame. base_link->sensor static Vehicle The Vehicle module provide sensor positions relative to the base_link using URDF. There may be multiple static transforms between the base_link and a sensor frame. For example, if a camera is calibrated against a LiDAR, then the camera's TF can be expressed by base_link->lidar->camera. Remarks: A static frame does not mean it does not change. It may be remapped at times, but no interpolation will be done between the change (i.e. only the newest information is used). base_link->sensor is assumed to be static. In reality, this is not true due to vehicle suspension, but we assume that the displacement is small enough that it doesn't affect control of the vehicle. There is a discussion about a new nav_base frame that resolves this issue, and this new frame may be integrated at some later point. The specification above is not meant to restrict the addition of other frames. Developers may add any additional frames as required, but the meaning of existing frames as described above must not be changed. Regarding REP105 # For TF, ROS follows the naming conventions and semantic meanings in REP-105 . The explanation given above also follows REP-105, but with the significant change of removing the odom frame. What is the odom frame? # The odom frame is defined as follows in REP-105: The coordinate frame called odom is a world-fixed frame. The pose of a mobile platform in the odom frame can drift over time, without any bounds. This drift makes the odom frame useless as a long-term global reference. However, the pose of a robot in the odom frame is guaranteed to be continuous, meaning that the pose of a mobile platform in the odom frame always evolves in a smooth way, without discrete jumps. In a typical setup the odom frame is computed based on an odometry source, such as wheel odometry, visual odometry or an inertial measurement unit. The odom frame is useful as an accurate, short-term local reference, but drift makes it a poor frame for long-term reference. There have been some discussions about the purpose of the odom frame, and the main reasons for using it are as follows: odom->base_link is high-frequency and therefore suitable for control odom->base_link is continuous and keeps control from \"jerks\" odom->base_link is independent of localization, and so it is still safe to use in the event of localization failure so long as control is done in the odom frame. Why we removed the odom frame # Tier IV's view is that control is affected by localization, even if trajectory following is done only in the odom frame. For example, if a trajectory is calculated from the shape of the lane specified in an HD map, the localization result will be indirectly used when projecting the trajectory into the odom frame, and thus the trajectory calculation will be thrown off if localization fails. Also, any map-based calculations that are done before trajectory calculation, such as shape optimization using the map's drivable areas or velocity optimization using the predicted trajectory of other vehicles (derived from lane shape information) will also be affected by localization failure. In order to ensure that control is unaffected by localization failure, we require that all preceding calculations do not use the map->odom transform. However, since trajectory following comes after planning in Autoware, it is almost impossible to prevent map->odom from being involved in trajectory calculations. Although this might be possible if Autoware planned like a human (who only uses route graph information from the map and can obtain geometry information from perception), it is very unlikely that an autonomous driving stack is capable of ensuring safety without using geometry information. Therefore, regardless of the frame in which it is done, control will still be affected by localization. To control a vehicle without jerking or sudden steering in the event of localization failure, we set the following requirements for the Localization module: Localization results must be continuous Localization failures must be detected and the vehicle's pose should not be updated with any failed localization results This new Localization architecture assumes that twist and pose will be integrated with a sequential Bayesian Filter such as EKF or a particle filter . Additionally, the new architecture is able to integrate odometry information directly from sensor data (currently IMU and vehicle speed sensors only, but GNSS and doppler sensor data may be added in future) and is able to update TF smoothly and continuously at high frequency. As a result, all of the merits of odom->base_link stated in REP-105 can be satisfied by map->base_link in this new architecture and so there is no need to set the odom frame. To conclude, we removed the odom frame from this architecture proposal for the following reasons: It is almost impossible to avoid using map information in the Planning module, and thus the trajectory will have a dependency on map->odom (or map->base_link) Therefore, to maintain safe control even after a localization failure, the following conditions must be satisfied by the Localization module: It must be able to detect localization failures When a failure is detected, map->odom should not be updatedD:wq If the Localization module can satisfy the above conditions, then there is no benefit in using odom->base_link, and all modules should use map->base_link instead whenever they need a world-fixed frame. Possible Concerns # The argument above focuses on replacing map->odom->base_link with map->base_link, but doesn't prove that map->base_link is better. If we set odom->base_link, wouldn't we have more frame options available? Once we split map->base_link into map->odom and odom->base_link, we lose velocity information and uncertainty (covariance) information between them. We can expect more robustness if we integrate all information (odometry and output of multiple localization) at once. We have to wait for both transforms in order to obtain the map->base_link transform. However, it is easier to estimate delay time if map->base_link is combined into one TF. Creating the odom frame within this new architecture should be possible, but first one has to consider if there are any components that actually need to use odom->base_link. For example, we anticipate that odom->base_link might be needed when designing a safety architecture (out-of-scope for this proposal). However, it should only be added after a comprehensive safety analysis has been completed since we already know that using the odom frame in the Control module does not ensure safety . Most ROS localization nodes assume odom->base_link to calculate map->odom. Wouldn't it be impossible to utilize such nodes without the odom frame? It is very unlikely that a single algorithm supports all use cases in all environments. We need a module that integrates output from different localization algorithms, but most third-party localization nodes are not made with consideration for integration modules. Therefore, we still need to make changes when using third-party software anyway. Technically, REP-105 is integrating the odometry and localization results by sequentially connecting TFs, but this is not realistic when we add more algorithms. We should be thinking of a way to integrate localization methods in parallel, and this new architecture was made to support such a use case. Reference # REP105: https://www.ros.org/reps/rep-0105.html REP103: https://www.ros.org/reps/rep-0103.html TF discussion on ROS Discourse: https://discourse.ros.org/t/localization-architecture/8602/28 TF discussion in Autoware.Auto (GitLab): https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/issues/292","title":"TF"},{"location":"design/software_architecture/TF/#tf-tree-in-autoware","text":"Autoware uses the ROS TF library for transforming coordinates. The Autoware TF tree can be accessed from any module within Autoware, and is illustrated below.","title":"TF tree in Autoware"},{"location":"design/software_architecture/TF/#frames","text":"earth: the origin of the ECEF coordinate system (i.e. the center of the Earth). Although this frame is not currently used by any modules (and so is not shown in the diagram above), it was added to support the use of larger maps and multiple vehicles in the future. map: Local ENU coordinate. This keeps the xy-plane relatively parallel to the ground surface (thus the z-axis points upwards). All map information should be provided in this frame, or provided in a frame that can be statically transformed into this frame, since most planning calculations are done in this frame. base_link: A frame that is rigidly attached to the vehicle. Currently, this is defined as the midpoint of the rear wheels projected to the ground. sensor_frame(s): One or more frames that represent the position of individual sensors. The actual name of each frame should be a combination of the name of the sensor and its position relative to the vehicle, such as camera_front , gnss_back and lidar_top . Note that a camera should provide both camera frame and camera optical frame as suggested in REP-103 (eg: 'camera_front' and 'camera_front_optical').","title":"Frames"},{"location":"design/software_architecture/TF/#transforms","text":"TF Type Providing Module Description earth->map static Map Map modules will provide this TF according to an origin information parameter file. map->base_link dynamic Localization The Localization module calculates the vehicle's position relative to maps specified in the map frame. base_link->sensor static Vehicle The Vehicle module provide sensor positions relative to the base_link using URDF. There may be multiple static transforms between the base_link and a sensor frame. For example, if a camera is calibrated against a LiDAR, then the camera's TF can be expressed by base_link->lidar->camera. Remarks: A static frame does not mean it does not change. It may be remapped at times, but no interpolation will be done between the change (i.e. only the newest information is used). base_link->sensor is assumed to be static. In reality, this is not true due to vehicle suspension, but we assume that the displacement is small enough that it doesn't affect control of the vehicle. There is a discussion about a new nav_base frame that resolves this issue, and this new frame may be integrated at some later point. The specification above is not meant to restrict the addition of other frames. Developers may add any additional frames as required, but the meaning of existing frames as described above must not be changed.","title":"Transforms"},{"location":"design/software_architecture/TF/#regarding-rep105","text":"For TF, ROS follows the naming conventions and semantic meanings in REP-105 . The explanation given above also follows REP-105, but with the significant change of removing the odom frame.","title":"Regarding REP105"},{"location":"design/software_architecture/TF/#what-is-the-odom-frame","text":"The odom frame is defined as follows in REP-105: The coordinate frame called odom is a world-fixed frame. The pose of a mobile platform in the odom frame can drift over time, without any bounds. This drift makes the odom frame useless as a long-term global reference. However, the pose of a robot in the odom frame is guaranteed to be continuous, meaning that the pose of a mobile platform in the odom frame always evolves in a smooth way, without discrete jumps. In a typical setup the odom frame is computed based on an odometry source, such as wheel odometry, visual odometry or an inertial measurement unit. The odom frame is useful as an accurate, short-term local reference, but drift makes it a poor frame for long-term reference. There have been some discussions about the purpose of the odom frame, and the main reasons for using it are as follows: odom->base_link is high-frequency and therefore suitable for control odom->base_link is continuous and keeps control from \"jerks\" odom->base_link is independent of localization, and so it is still safe to use in the event of localization failure so long as control is done in the odom frame.","title":"What is the odom frame?"},{"location":"design/software_architecture/TF/#why-we-removed-the-odom-frame","text":"Tier IV's view is that control is affected by localization, even if trajectory following is done only in the odom frame. For example, if a trajectory is calculated from the shape of the lane specified in an HD map, the localization result will be indirectly used when projecting the trajectory into the odom frame, and thus the trajectory calculation will be thrown off if localization fails. Also, any map-based calculations that are done before trajectory calculation, such as shape optimization using the map's drivable areas or velocity optimization using the predicted trajectory of other vehicles (derived from lane shape information) will also be affected by localization failure. In order to ensure that control is unaffected by localization failure, we require that all preceding calculations do not use the map->odom transform. However, since trajectory following comes after planning in Autoware, it is almost impossible to prevent map->odom from being involved in trajectory calculations. Although this might be possible if Autoware planned like a human (who only uses route graph information from the map and can obtain geometry information from perception), it is very unlikely that an autonomous driving stack is capable of ensuring safety without using geometry information. Therefore, regardless of the frame in which it is done, control will still be affected by localization. To control a vehicle without jerking or sudden steering in the event of localization failure, we set the following requirements for the Localization module: Localization results must be continuous Localization failures must be detected and the vehicle's pose should not be updated with any failed localization results This new Localization architecture assumes that twist and pose will be integrated with a sequential Bayesian Filter such as EKF or a particle filter . Additionally, the new architecture is able to integrate odometry information directly from sensor data (currently IMU and vehicle speed sensors only, but GNSS and doppler sensor data may be added in future) and is able to update TF smoothly and continuously at high frequency. As a result, all of the merits of odom->base_link stated in REP-105 can be satisfied by map->base_link in this new architecture and so there is no need to set the odom frame. To conclude, we removed the odom frame from this architecture proposal for the following reasons: It is almost impossible to avoid using map information in the Planning module, and thus the trajectory will have a dependency on map->odom (or map->base_link) Therefore, to maintain safe control even after a localization failure, the following conditions must be satisfied by the Localization module: It must be able to detect localization failures When a failure is detected, map->odom should not be updatedD:wq If the Localization module can satisfy the above conditions, then there is no benefit in using odom->base_link, and all modules should use map->base_link instead whenever they need a world-fixed frame.","title":"Why we removed the odom frame"},{"location":"design/software_architecture/TF/#possible-concerns","text":"The argument above focuses on replacing map->odom->base_link with map->base_link, but doesn't prove that map->base_link is better. If we set odom->base_link, wouldn't we have more frame options available? Once we split map->base_link into map->odom and odom->base_link, we lose velocity information and uncertainty (covariance) information between them. We can expect more robustness if we integrate all information (odometry and output of multiple localization) at once. We have to wait for both transforms in order to obtain the map->base_link transform. However, it is easier to estimate delay time if map->base_link is combined into one TF. Creating the odom frame within this new architecture should be possible, but first one has to consider if there are any components that actually need to use odom->base_link. For example, we anticipate that odom->base_link might be needed when designing a safety architecture (out-of-scope for this proposal). However, it should only be added after a comprehensive safety analysis has been completed since we already know that using the odom frame in the Control module does not ensure safety . Most ROS localization nodes assume odom->base_link to calculate map->odom. Wouldn't it be impossible to utilize such nodes without the odom frame? It is very unlikely that a single algorithm supports all use cases in all environments. We need a module that integrates output from different localization algorithms, but most third-party localization nodes are not made with consideration for integration modules. Therefore, we still need to make changes when using third-party software anyway. Technically, REP-105 is integrating the odometry and localization results by sequentially connecting TFs, but this is not realistic when we add more algorithms. We should be thinking of a way to integrate localization methods in parallel, and this new architecture was made to support such a use case.","title":"Possible Concerns"},{"location":"design/software_architecture/TF/#reference","text":"REP105: https://www.ros.org/reps/rep-0105.html REP103: https://www.ros.org/reps/rep-0103.html TF discussion on ROS Discourse: https://discourse.ros.org/t/localization-architecture/8602/28 TF discussion in Autoware.Auto (GitLab): https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/issues/292","title":"Reference"},{"location":"design/software_architecture/Control/Control/","text":"Control # Overview # Control stack generates control signals to drive a vehicle following trajectories considering vehicle dynamics. This layer ensures that the vehicle follows the trajectory planned by planning. The output of Control stack includes velocity, acceleration, and steering. Role # There are two main roles of Control Stack: Generation of control command for following target trajectory Post-processing of vehicle control command Use Case # Control stack supports the following use cases. Driving without excessive speed Driving at slope Smooth stop by normal obstacles / Sudden stop by obstacle's running-out Forward/Reverse parking Requirement # To achieve the above use case, Control stack requires the following conditions. The input trajectory includes speed limit at each point (Use case 1). The input pose includes gradient information (=vehicle orientation) (Use case 2). The output vehicle command includes acceleration but also velocity (Use case 2, 3). The output vehicle command includes the command to shift drive/reverse gear(Use case 4.). Input # The input to Control stack: Input Topic (Data Type) Explanation Trajectory /planning/scenario_planning/trajectory ( autoware_planning_msgs::Trajectory ) Target trajectory to follow Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist /vehicle/status/twist ( geometry_msgs::TwistStamped ) Current twist of the vehicle Steer /vehicle/status/steering ( autoware_vehicle_msgs::Steering ) Current steer of the vehicle Engage Command /autoware/engage ( std_msgs::Bool ) Whether to send commands to the vehicle Remote Command - Control command from remote As the above requirements, the data type of target trajectory, autoware_planning_msgs::Trajectory , includes the speed at each point. Output # The table below summarizes the output from Control stack: Output Topic(Data Type) Explanation Vehicle Command /control/vehicle_cmd ( autoware_vehicle_msgs/VehicleCommand ) Table Below The main outputs included in Vehicle Command are as follows. Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Gear shifting command std_msgs/Int32 Emergency command std_msgs/Int32 As above requirements, the control stack outputs gear shifting command and acceleration command as Vehicle command Design # Trajectory follower # Role # Generate control command for following given trajectory smoothly. Input # Target trajectory Target trajectory includes target position, target twist, target acceleration Current pose Current velocity Current steering Output # Steering angle command Steering angular velocity command Velocity command Acceleration command Vehicle command gate # Role # Systematic post-processing of vehicle control command, independent of trajectory following process Reshape the vehicle control command Select the command values (Trajectory follow command, Remote manual command) Observe the maximum speed limit, maximum lateral/longitudinal jerk Stop urgently when an emergency command is received Input # Control commands from Trajectory Follower module Remote Control commands Engage Commands Output # Control signal for vehicles References # TBU","title":"Overall"},{"location":"design/software_architecture/Control/Control/#control","text":"","title":"Control"},{"location":"design/software_architecture/Control/Control/#overview","text":"Control stack generates control signals to drive a vehicle following trajectories considering vehicle dynamics. This layer ensures that the vehicle follows the trajectory planned by planning. The output of Control stack includes velocity, acceleration, and steering.","title":"Overview"},{"location":"design/software_architecture/Control/Control/#role","text":"There are two main roles of Control Stack: Generation of control command for following target trajectory Post-processing of vehicle control command","title":"Role"},{"location":"design/software_architecture/Control/Control/#use-case","text":"Control stack supports the following use cases. Driving without excessive speed Driving at slope Smooth stop by normal obstacles / Sudden stop by obstacle's running-out Forward/Reverse parking","title":"Use Case"},{"location":"design/software_architecture/Control/Control/#requirement","text":"To achieve the above use case, Control stack requires the following conditions. The input trajectory includes speed limit at each point (Use case 1). The input pose includes gradient information (=vehicle orientation) (Use case 2). The output vehicle command includes acceleration but also velocity (Use case 2, 3). The output vehicle command includes the command to shift drive/reverse gear(Use case 4.).","title":"Requirement"},{"location":"design/software_architecture/Control/Control/#input","text":"The input to Control stack: Input Topic (Data Type) Explanation Trajectory /planning/scenario_planning/trajectory ( autoware_planning_msgs::Trajectory ) Target trajectory to follow Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist /vehicle/status/twist ( geometry_msgs::TwistStamped ) Current twist of the vehicle Steer /vehicle/status/steering ( autoware_vehicle_msgs::Steering ) Current steer of the vehicle Engage Command /autoware/engage ( std_msgs::Bool ) Whether to send commands to the vehicle Remote Command - Control command from remote As the above requirements, the data type of target trajectory, autoware_planning_msgs::Trajectory , includes the speed at each point.","title":"Input"},{"location":"design/software_architecture/Control/Control/#output","text":"The table below summarizes the output from Control stack: Output Topic(Data Type) Explanation Vehicle Command /control/vehicle_cmd ( autoware_vehicle_msgs/VehicleCommand ) Table Below The main outputs included in Vehicle Command are as follows. Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Gear shifting command std_msgs/Int32 Emergency command std_msgs/Int32 As above requirements, the control stack outputs gear shifting command and acceleration command as Vehicle command","title":"Output"},{"location":"design/software_architecture/Control/Control/#design","text":"","title":"Design"},{"location":"design/software_architecture/Control/Control/#trajectory-follower","text":"","title":"Trajectory follower"},{"location":"design/software_architecture/Control/Control/#role_1","text":"Generate control command for following given trajectory smoothly.","title":"Role"},{"location":"design/software_architecture/Control/Control/#input_1","text":"Target trajectory Target trajectory includes target position, target twist, target acceleration Current pose Current velocity Current steering","title":"Input"},{"location":"design/software_architecture/Control/Control/#output_1","text":"Steering angle command Steering angular velocity command Velocity command Acceleration command","title":"Output"},{"location":"design/software_architecture/Control/Control/#vehicle-command-gate","text":"","title":"Vehicle command gate"},{"location":"design/software_architecture/Control/Control/#role_2","text":"Systematic post-processing of vehicle control command, independent of trajectory following process Reshape the vehicle control command Select the command values (Trajectory follow command, Remote manual command) Observe the maximum speed limit, maximum lateral/longitudinal jerk Stop urgently when an emergency command is received","title":"Role"},{"location":"design/software_architecture/Control/Control/#input_2","text":"Control commands from Trajectory Follower module Remote Control commands Engage Commands","title":"Input"},{"location":"design/software_architecture/Control/Control/#output_2","text":"Control signal for vehicles","title":"Output"},{"location":"design/software_architecture/Control/Control/#references","text":"TBU","title":"References"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/","text":"Latlon Coupler # Overview # Role # Latlon Coupler module integrates lateral control command and longitudinal control command. For integration, newest value of each command are used as vehicle control command. Input # from Lateral Control module # autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 (Zero) Acceleration std_msgs/Float64 (Zero) Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 from Longitudinal Control module # autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 (Zero) Steering angle velocity std_msgs/Float64 (Zero) Output # autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 from Longitudinal Control module Acceleration std_msgs/Float64 from Longitudinal Control module Steering angle std_msgs/Float64 from Lateral Control module Steering angle velocity std_msgs/Float64 from Lateral Control module","title":"Lateral longitudinal coupler"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#latlon-coupler","text":"","title":"Latlon Coupler"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#overview","text":"","title":"Overview"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#role","text":"Latlon Coupler module integrates lateral control command and longitudinal control command. For integration, newest value of each command are used as vehicle control command.","title":"Role"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#input","text":"","title":"Input"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#from-lateral-control-module","text":"autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 (Zero) Acceleration std_msgs/Float64 (Zero) Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64","title":"from Lateral Control module"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#from-longitudinal-control-module","text":"autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 (Zero) Steering angle velocity std_msgs/Float64 (Zero)","title":"from Longitudinal Control module"},{"location":"design/software_architecture/Control/TrajectoryFollower/LatLonCoupler/#output","text":"autoware_control_msgs/ControlCommandStamped : Input Data Type Explanation Velocity std_msgs/Float64 from Longitudinal Control module Acceleration std_msgs/Float64 from Longitudinal Control module Steering angle std_msgs/Float64 from Lateral Control module Steering angle velocity std_msgs/Float64 from Lateral Control module","title":"Output"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/","text":"Lateral Controller # Overview # For following target trajectory, control stack needs to output lateral control commands (steering angle, steering angle velocity), and longitudinal control commands (acceleration, velocity). Lateral controller module is responsible for calculation of lateral control commands. Role # Lateral controller module calculates suitable velocity and acceleration for following target trajectory. Input # The input to Lateral Controller module: Input Data Type Explanation Trajectory autoware_planning_msgs::Trajectory Target trajectory to follow (target position, orientation, twist, acceleration) Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist geometry_msgs::TwistStamped Current twist of the vehicle Steer autoware_vehicle_msgs::Steering Current steer of the vehicle Output # The output type from Lateral Controller module is autoware_control_msgs/ControlCommandStamped . The main outputs included in autoware_control_msgs/ControlCommandStamped are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Note that velocity and acceleration are always output as 0 from Lateral Controller module.","title":"Lateral controller"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/#lateral-controller","text":"","title":"Lateral Controller"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/#overview","text":"For following target trajectory, control stack needs to output lateral control commands (steering angle, steering angle velocity), and longitudinal control commands (acceleration, velocity). Lateral controller module is responsible for calculation of lateral control commands.","title":"Overview"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/#role","text":"Lateral controller module calculates suitable velocity and acceleration for following target trajectory.","title":"Role"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/#input","text":"The input to Lateral Controller module: Input Data Type Explanation Trajectory autoware_planning_msgs::Trajectory Target trajectory to follow (target position, orientation, twist, acceleration) Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist geometry_msgs::TwistStamped Current twist of the vehicle Steer autoware_vehicle_msgs::Steering Current steer of the vehicle","title":"Input"},{"location":"design/software_architecture/Control/TrajectoryFollower/LateralController/#output","text":"The output type from Lateral Controller module is autoware_control_msgs/ControlCommandStamped . The main outputs included in autoware_control_msgs/ControlCommandStamped are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Note that velocity and acceleration are always output as 0 from Lateral Controller module.","title":"Output"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/","text":"Longitudinal Controller # Overview # For following target trajectory, control stack needs to output lateral control commands (steering angle, steering angle velocity), and longitudinal control commands (acceleration, velocity). Longitudinal controller module is responsible for calculation of longitudinal control commands. Role # Longitudinal controller module calculates velocity and acceleration for following target velocity profile. This module velocity error from target trajectory. Input # The input to Longitudinal Controller module: Input Data Type Explanation Trajectory autoware_planning_msgs::Trajectory Target trajectory to follow (target position, twist, acceleration) Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist geometry_msgs::TwistStamped Current twist of the vehicle Output # The output type from Longitudinal Controller module is autoware_control_msgs/ControlCommandStamped . The main outputs included in autoware_control_msgs/ControlCommandStamped are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Note that steering angle and steering angle velocity are always output as 0 from Longitudinal Controller module.","title":"Longitudinal controller"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/#longitudinal-controller","text":"","title":"Longitudinal Controller"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/#overview","text":"For following target trajectory, control stack needs to output lateral control commands (steering angle, steering angle velocity), and longitudinal control commands (acceleration, velocity). Longitudinal controller module is responsible for calculation of longitudinal control commands.","title":"Overview"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/#role","text":"Longitudinal controller module calculates velocity and acceleration for following target velocity profile. This module velocity error from target trajectory.","title":"Role"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/#input","text":"The input to Longitudinal Controller module: Input Data Type Explanation Trajectory autoware_planning_msgs::Trajectory Target trajectory to follow (target position, twist, acceleration) Pose /tf ( tf2_msgs::TFMessage ) Current pose of the vehicle Twist geometry_msgs::TwistStamped Current twist of the vehicle","title":"Input"},{"location":"design/software_architecture/Control/TrajectoryFollower/LongitudinalController/#output","text":"The output type from Longitudinal Controller module is autoware_control_msgs/ControlCommandStamped . The main outputs included in autoware_control_msgs/ControlCommandStamped are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Note that steering angle and steering angle velocity are always output as 0 from Longitudinal Controller module.","title":"Output"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/","text":"Vehicle Cmd Gate # Overview # Vehicle Cmd Gate module is responsible for Systematic post-processing. Role # Roles of Vehicle Cmd Gate module are as follows. Reshape the vehicle control command Vehicle Cmd Gate module convert autoware_control_msgs/ControlCommandStamped to autoware_vehicle_msgs/VehicleCommand . This conversion includes the addition of gear shifting command. Reflect engage command to control signal for vehicles Until true command is sent as engage command, Vehicle Cmd Gate module does not pass the input command information as output. Select the command values (Trajectory follow command, Remote manual command) Observe the maximum speed limit, maximum lateral/longitudinal jerk Stop urgently when emergency command is received Input # Control commands from Trajectory Follower module( autoware_control_msgs/ControlCommandStamped ) Remote Control commands(Undefined[TBD]) Engage Commands(std_msgs/Bool) The main inputs included in autoware_control_msgs/ControlCommandStamped are as follows: Input Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Output # Control signal for vehicles (autoware_vehicle_msgs/VehicleCommand) The main inputs included in autoware_vehicle_msgs/VehicleCommand are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Gear shifting command std_msgs/Int32 Emergency command std_msgs/Int32 Note that most of above commands are output as 0 until true command is sent as engage command.","title":"Vehicle cmd gate"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/#vehicle-cmd-gate","text":"","title":"Vehicle Cmd Gate"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/#overview","text":"Vehicle Cmd Gate module is responsible for Systematic post-processing.","title":"Overview"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/#role","text":"Roles of Vehicle Cmd Gate module are as follows. Reshape the vehicle control command Vehicle Cmd Gate module convert autoware_control_msgs/ControlCommandStamped to autoware_vehicle_msgs/VehicleCommand . This conversion includes the addition of gear shifting command. Reflect engage command to control signal for vehicles Until true command is sent as engage command, Vehicle Cmd Gate module does not pass the input command information as output. Select the command values (Trajectory follow command, Remote manual command) Observe the maximum speed limit, maximum lateral/longitudinal jerk Stop urgently when emergency command is received","title":"Role"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/#input","text":"Control commands from Trajectory Follower module( autoware_control_msgs/ControlCommandStamped ) Remote Control commands(Undefined[TBD]) Engage Commands(std_msgs/Bool) The main inputs included in autoware_control_msgs/ControlCommandStamped are as follows: Input Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64","title":"Input"},{"location":"design/software_architecture/Control/VehicleCmdGate/VehicleCmdGate/#output","text":"Control signal for vehicles (autoware_vehicle_msgs/VehicleCommand) The main inputs included in autoware_vehicle_msgs/VehicleCommand are as follows: Output Data Type Velocity std_msgs/Float64 Acceleration std_msgs/Float64 Steering angle std_msgs/Float64 Steering angle velocity std_msgs/Float64 Gear shifting command std_msgs/Int32 Emergency command std_msgs/Int32 Note that most of above commands are output as 0 until true command is sent as engage command.","title":"Output"},{"location":"design/software_architecture/Localization/Localization/","text":"Localization # Overview # The localization stack has a role to recognize where ego vehicle is \"map\" frame . Additionally, this stack estimates twist of ego vehicle for precise velocity planning and control. Role # There are two main roles of Localization stack: Estimation of a self-position and a self-twist Integration of pose and twist information estimated by multiple sensors for robustness Input # Input Data Type LiDAR sensor_msgs::PointCloud2 GNSS geometry_msgs::PoseWithCovarianceStamped IMU sensor_msgs::Imu Pointcloud Map sensor_msgs::PointCloud2 Vehicle CAN geometry_msgs::TwistStamped Sensors # Multiple sensor information described below is considered. LiDAR Pointcloud registration method such as ICP, NDT estimates ego vehicle pose by refining the relative transformation between 3D point cloud from lidar with reference pointcloud map. GNSS The pose information received from GNSS is projected into \"map\" frame. This can be used as one of the pose estimator mentioned below or it can be used to provide an initial guess for a sequential localization method. IMU Angular velocity from IMU is used as the vehicle twist. Vehicle CAN Vehicle CAN outputs useful information such as vehicle velocity, steering wheel angle to estimate vehicle twist. We adapt vehicle velocity from vehicle CAN as vehicle twist. Camera We do not implement camera-based pose or twist estimator. You can easily integrate image-based-estimator such as visual odometry and visual slam into the localization stack. Reference Map # Pointcloud Map Output # Output Topic (Data Type) Use Cases of the output Vehicle Pose /tf ( tf2_msgs/TFMessage ) Perception, Planning, Control Vehicle Pose with Covariance /localization/pose_with_covariance ( geometry_msgs/PoseWithCovarianceStamped ) Control Vehicle Twist /localization/twist ( geometry_msgs/TwistStamped ) Planning, Control Use Cases # Use Cases Requirement in Localization Output How it is used 1. Passing intersection with traffic lights Self pose on the map Perception To detect traffic lights associated with the lane where ego vehicle is in the camera image 2. Changing lane Self pose on the map Perception Planning To predict object motion on the lane with lane information To recognize drivable area based on lane information and the position where ego vehicle is 3. Stopping at crosswalk when a pedestrian is walking Self pose on the map Perception To recognize where the crosswalk is based on ego vehicle position and map information 4. Reaching a goal by driving on lanes Self pose on the map Planning To plan the global path from the position where ego vehicle is to a goal with lane information 5. Driving with following speed limits Self pose on the map Self twist Planning To recognize speed limit on the lane where ego vehicle is To plan target velocity based on the velocity of ego vehicle and speed limit 6. Driving on the target lane Self pose on the map Self twist Control To calculate target throttle/brake value and steering angle based on pose and twist of ego vehicle and target trajectory 7. Driving within the target lane Self pose with covariance on the map Control To check if ego vehicle with covariance is inside the target lane Requirements # The high-level requirements of Localization stack are listed below: Localization stack must provide pose in \"map\" frame. (Use Case 1-6) The output should be provided as TF from \"map\" to \"base_link\". (See TF document for the details) The localization result must be continuous Whenever a localization algorithm fails, the failure must be detected should not update vehicle pose. Localization stack must provide the velocity of the vehicle in \"map\" frame. (Use Case 5,6) Design # The localization stack provides indispensable information to achieve autonomous driving. Therefore, it is not preferable to depend on only one localization algorithm. We insert pose twist fusion filter after pose and twist estimator to improve robustness of the estimated pose and twist. Also, developers can easily add a new estimator based on another sensor, e.g. camera-based visual SLAM and visual odometry, into the localization stack. The localization stack should output the transformation from map to base_link as /tf to utilize its interpolation system. Pose estimator # Role # Pose estimator is a component to estimate ego vehicle pose which includes position and orientation. The final output should also include covariance, which represents the estimator's confidence on the estimated pose. A pose estimator could either estimate pose in a local map, or it can estimate absolute pose using global localizer. The output pose can be published in any frame as long as enough /tf is provided to project into the \"map\" frame. Input # LiDAR GNSS Camera (not implemented yet) Pointcloud Map Output # Pose with Covariance Pose Estimator Diagnostics Twist Estimator # Twist estimator is a component to estimate ego vehicle twist for precise velocity planning and control. The x-axis velocity and z-axis angular velocity of the vehicle are crucial information. These values are preferable to be noise-free and unbiased. Input # Vehicle CAN IMU Camera (not implemented yet) Output # Twist with Covariance Pose Twist Fusion Filter # Role # Pose Twist Fusion Filter is a component to integrate the poses estimated by pose estimators and the twists estimated by twist estimators. This assumes sequential Bayesian Filter, such as EKF and particle filter, which calculates vehicle's pose and twist probabilistically. This should also ensure the following functions: smoothing of estimated pose (see TF.md ) outlier rejection of inputs based on previously calculated pose and it's covariance (see TF.md ) time delay compensation in case pose estimators take time to calculate pose Input # Initial Pose Pose with Covariance Twist with Covariance Output # Ego Vehicle Pose (/tf from map frame to base_link frame) Ego Vehicle Pose with Covariance Ego Vehicle Twist","title":"Overall"},{"location":"design/software_architecture/Localization/Localization/#localization","text":"","title":"Localization"},{"location":"design/software_architecture/Localization/Localization/#overview","text":"The localization stack has a role to recognize where ego vehicle is \"map\" frame . Additionally, this stack estimates twist of ego vehicle for precise velocity planning and control.","title":"Overview"},{"location":"design/software_architecture/Localization/Localization/#role","text":"There are two main roles of Localization stack: Estimation of a self-position and a self-twist Integration of pose and twist information estimated by multiple sensors for robustness","title":"Role"},{"location":"design/software_architecture/Localization/Localization/#input","text":"Input Data Type LiDAR sensor_msgs::PointCloud2 GNSS geometry_msgs::PoseWithCovarianceStamped IMU sensor_msgs::Imu Pointcloud Map sensor_msgs::PointCloud2 Vehicle CAN geometry_msgs::TwistStamped","title":"Input"},{"location":"design/software_architecture/Localization/Localization/#sensors","text":"Multiple sensor information described below is considered. LiDAR Pointcloud registration method such as ICP, NDT estimates ego vehicle pose by refining the relative transformation between 3D point cloud from lidar with reference pointcloud map. GNSS The pose information received from GNSS is projected into \"map\" frame. This can be used as one of the pose estimator mentioned below or it can be used to provide an initial guess for a sequential localization method. IMU Angular velocity from IMU is used as the vehicle twist. Vehicle CAN Vehicle CAN outputs useful information such as vehicle velocity, steering wheel angle to estimate vehicle twist. We adapt vehicle velocity from vehicle CAN as vehicle twist. Camera We do not implement camera-based pose or twist estimator. You can easily integrate image-based-estimator such as visual odometry and visual slam into the localization stack.","title":"Sensors"},{"location":"design/software_architecture/Localization/Localization/#reference-map","text":"Pointcloud Map","title":"Reference Map"},{"location":"design/software_architecture/Localization/Localization/#output","text":"Output Topic (Data Type) Use Cases of the output Vehicle Pose /tf ( tf2_msgs/TFMessage ) Perception, Planning, Control Vehicle Pose with Covariance /localization/pose_with_covariance ( geometry_msgs/PoseWithCovarianceStamped ) Control Vehicle Twist /localization/twist ( geometry_msgs/TwistStamped ) Planning, Control","title":"Output"},{"location":"design/software_architecture/Localization/Localization/#use-cases","text":"Use Cases Requirement in Localization Output How it is used 1. Passing intersection with traffic lights Self pose on the map Perception To detect traffic lights associated with the lane where ego vehicle is in the camera image 2. Changing lane Self pose on the map Perception Planning To predict object motion on the lane with lane information To recognize drivable area based on lane information and the position where ego vehicle is 3. Stopping at crosswalk when a pedestrian is walking Self pose on the map Perception To recognize where the crosswalk is based on ego vehicle position and map information 4. Reaching a goal by driving on lanes Self pose on the map Planning To plan the global path from the position where ego vehicle is to a goal with lane information 5. Driving with following speed limits Self pose on the map Self twist Planning To recognize speed limit on the lane where ego vehicle is To plan target velocity based on the velocity of ego vehicle and speed limit 6. Driving on the target lane Self pose on the map Self twist Control To calculate target throttle/brake value and steering angle based on pose and twist of ego vehicle and target trajectory 7. Driving within the target lane Self pose with covariance on the map Control To check if ego vehicle with covariance is inside the target lane","title":"Use Cases"},{"location":"design/software_architecture/Localization/Localization/#requirements","text":"The high-level requirements of Localization stack are listed below: Localization stack must provide pose in \"map\" frame. (Use Case 1-6) The output should be provided as TF from \"map\" to \"base_link\". (See TF document for the details) The localization result must be continuous Whenever a localization algorithm fails, the failure must be detected should not update vehicle pose. Localization stack must provide the velocity of the vehicle in \"map\" frame. (Use Case 5,6)","title":"Requirements"},{"location":"design/software_architecture/Localization/Localization/#design","text":"The localization stack provides indispensable information to achieve autonomous driving. Therefore, it is not preferable to depend on only one localization algorithm. We insert pose twist fusion filter after pose and twist estimator to improve robustness of the estimated pose and twist. Also, developers can easily add a new estimator based on another sensor, e.g. camera-based visual SLAM and visual odometry, into the localization stack. The localization stack should output the transformation from map to base_link as /tf to utilize its interpolation system.","title":"Design"},{"location":"design/software_architecture/Localization/Localization/#pose-estimator","text":"","title":"Pose estimator"},{"location":"design/software_architecture/Localization/Localization/#role_1","text":"Pose estimator is a component to estimate ego vehicle pose which includes position and orientation. The final output should also include covariance, which represents the estimator's confidence on the estimated pose. A pose estimator could either estimate pose in a local map, or it can estimate absolute pose using global localizer. The output pose can be published in any frame as long as enough /tf is provided to project into the \"map\" frame.","title":"Role"},{"location":"design/software_architecture/Localization/Localization/#input_1","text":"LiDAR GNSS Camera (not implemented yet) Pointcloud Map","title":"Input"},{"location":"design/software_architecture/Localization/Localization/#output_1","text":"Pose with Covariance Pose Estimator Diagnostics","title":"Output"},{"location":"design/software_architecture/Localization/Localization/#twist-estimator","text":"Twist estimator is a component to estimate ego vehicle twist for precise velocity planning and control. The x-axis velocity and z-axis angular velocity of the vehicle are crucial information. These values are preferable to be noise-free and unbiased.","title":"Twist Estimator"},{"location":"design/software_architecture/Localization/Localization/#input_2","text":"Vehicle CAN IMU Camera (not implemented yet)","title":"Input"},{"location":"design/software_architecture/Localization/Localization/#output_2","text":"Twist with Covariance","title":"Output"},{"location":"design/software_architecture/Localization/Localization/#pose-twist-fusion-filter","text":"","title":"Pose Twist Fusion Filter"},{"location":"design/software_architecture/Localization/Localization/#role_2","text":"Pose Twist Fusion Filter is a component to integrate the poses estimated by pose estimators and the twists estimated by twist estimators. This assumes sequential Bayesian Filter, such as EKF and particle filter, which calculates vehicle's pose and twist probabilistically. This should also ensure the following functions: smoothing of estimated pose (see TF.md ) outlier rejection of inputs based on previously calculated pose and it's covariance (see TF.md ) time delay compensation in case pose estimators take time to calculate pose","title":"Role"},{"location":"design/software_architecture/Localization/Localization/#input_3","text":"Initial Pose Pose with Covariance Twist with Covariance","title":"Input"},{"location":"design/software_architecture/Localization/Localization/#output_3","text":"Ego Vehicle Pose (/tf from map frame to base_link frame) Ego Vehicle Pose with Covariance Ego Vehicle Twist","title":"Output"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/","text":"Pose Estimator # Role # Pose estimator is a component to estimate ego vehicle pose which includes position and orientation. The final output should also include covariance, which represents the estimator's confidence on estimated pose. A pose estimator could either be estimate pose on local map, or it can estimate absolute pose using global localizer. The output pose can be published in any frame as long as /tf is provided to project into the \"map\" frame. Also, pose estimator should stop publishing pose if it is possible to calculate reliability of estimated pose(e.g. matching score with map) and the reliability is low. Input # Input Data Type LiDAR sensor_msgs::PointCloud2 GNSS geometry_msgs::PoseWithCovarianceStamped Pointcloud Map sensor_msgs::PointCloud2 Feedback from Pose Twist Fusion Filter geometry_msgs::PoseWithCovarianceStamped Output # Output Data Type Use Cases of the output Initial Pose geometry_msgs::PoseWithCovarianceStamped Pose Twist Fusion Filter Estimated Pose geometry_msgs::PoseWithCovarianceStamped Pose Twist Fusion Filter Design # This is a sample design of our implementation using NDT Scan Matcher. We integrated 3D NDT registration method for sample pose estimation algorithm. The NDT registration method is a local localization method that requires a good initial guess before optimizing pose. In order to realize fully automatic localization, GNSS is used for first initialization. After first loop of pose estimation, the output of pose twist fusion filter is used as next initial guess of NDT registration. Note that NDT scan matcher does not publish pose when matching score calculated in alignment is less than threshold value to avoid publishing wrong estimated pose to Pose Twist Fusion Filter. Lidar sensors usually operate at 10 ~ 20 Hz and therefore NDT alignment should be executed within approximately 100 ms. In order to reduce execution time, We apply two pointcloud preprocessors to raw pointcloud from lidar sensors; Crop Measurement Range and DownSampler. Crop Measurement Range removes points far from ego vehicle. DownSampler reduces the number of points by calculating a centroid of points in each voxelized grid. Pose initializer adds height information into initial pose obtained from GNSS by looking for minimum height point from points within 1 m.","title":"Pose estimator"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/#pose-estimator","text":"","title":"Pose Estimator"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/#role","text":"Pose estimator is a component to estimate ego vehicle pose which includes position and orientation. The final output should also include covariance, which represents the estimator's confidence on estimated pose. A pose estimator could either be estimate pose on local map, or it can estimate absolute pose using global localizer. The output pose can be published in any frame as long as /tf is provided to project into the \"map\" frame. Also, pose estimator should stop publishing pose if it is possible to calculate reliability of estimated pose(e.g. matching score with map) and the reliability is low.","title":"Role"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/#input","text":"Input Data Type LiDAR sensor_msgs::PointCloud2 GNSS geometry_msgs::PoseWithCovarianceStamped Pointcloud Map sensor_msgs::PointCloud2 Feedback from Pose Twist Fusion Filter geometry_msgs::PoseWithCovarianceStamped","title":"Input"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/#output","text":"Output Data Type Use Cases of the output Initial Pose geometry_msgs::PoseWithCovarianceStamped Pose Twist Fusion Filter Estimated Pose geometry_msgs::PoseWithCovarianceStamped Pose Twist Fusion Filter","title":"Output"},{"location":"design/software_architecture/Localization/PoseEstimator/PoseEstimator/#design","text":"This is a sample design of our implementation using NDT Scan Matcher. We integrated 3D NDT registration method for sample pose estimation algorithm. The NDT registration method is a local localization method that requires a good initial guess before optimizing pose. In order to realize fully automatic localization, GNSS is used for first initialization. After first loop of pose estimation, the output of pose twist fusion filter is used as next initial guess of NDT registration. Note that NDT scan matcher does not publish pose when matching score calculated in alignment is less than threshold value to avoid publishing wrong estimated pose to Pose Twist Fusion Filter. Lidar sensors usually operate at 10 ~ 20 Hz and therefore NDT alignment should be executed within approximately 100 ms. In order to reduce execution time, We apply two pointcloud preprocessors to raw pointcloud from lidar sensors; Crop Measurement Range and DownSampler. Crop Measurement Range removes points far from ego vehicle. DownSampler reduces the number of points by calculating a centroid of points in each voxelized grid. Pose initializer adds height information into initial pose obtained from GNSS by looking for minimum height point from points within 1 m.","title":"Design"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/","text":"Pose Twist Fusion Filter # Role # Pose Twist Fusion Filter is a component to integrate the poses estimated by pose estimator and the twists estimated by twist estimator considering the time delay of sensor data. This component improves the robustness of localization, e.g. even when NDT scan matching is unreliable, a vehicle can keep autonomous driving based on vehicle twist information. Input # Input Data Type Initial Pose geometry_msgs::PoseWithCovarianceStamped Estimated Pose geometry_msgs::PoseWithCovarianceStamped Estimated Twist geometry_msgs::TwistWithCovarianceStamped Output # Output Data Type Use Cases of the output Vehicle Pose tf2_msgs::TFMessage Perception, Planning, Control Vehicle Pose with Covariance geometry_msgs::PoseWithCovarianceStamped Control Vehicle Twist geometry_msgs::TwistStamped Planning, Control Design # Estimated pose in Pose Estimator and estimated twist in Twist Estimator basically contain error to some extent. In order to integrate the 2D vehicle dynamics model with the estimated pose and twist of ego vehicle and generate a robust and less noisy pose and twist, we implement an extended kalman filter(EKF) as Pose Twist Fusion Filter shown in the figure above. If you would like to know further details of EKF model, please see this document . The stop filter detects the vehicle's stop state by monitoring both velocity and angular velocity. When so, the stop filter filters the velocity and angular velocity to 0.","title":"Pose twist fusion filter"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/#pose-twist-fusion-filter","text":"","title":"Pose Twist Fusion Filter"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/#role","text":"Pose Twist Fusion Filter is a component to integrate the poses estimated by pose estimator and the twists estimated by twist estimator considering the time delay of sensor data. This component improves the robustness of localization, e.g. even when NDT scan matching is unreliable, a vehicle can keep autonomous driving based on vehicle twist information.","title":"Role"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/#input","text":"Input Data Type Initial Pose geometry_msgs::PoseWithCovarianceStamped Estimated Pose geometry_msgs::PoseWithCovarianceStamped Estimated Twist geometry_msgs::TwistWithCovarianceStamped","title":"Input"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/#output","text":"Output Data Type Use Cases of the output Vehicle Pose tf2_msgs::TFMessage Perception, Planning, Control Vehicle Pose with Covariance geometry_msgs::PoseWithCovarianceStamped Control Vehicle Twist geometry_msgs::TwistStamped Planning, Control","title":"Output"},{"location":"design/software_architecture/Localization/PoseTwistFusionFilter/PoseTwistFusionFilter/#design","text":"Estimated pose in Pose Estimator and estimated twist in Twist Estimator basically contain error to some extent. In order to integrate the 2D vehicle dynamics model with the estimated pose and twist of ego vehicle and generate a robust and less noisy pose and twist, we implement an extended kalman filter(EKF) as Pose Twist Fusion Filter shown in the figure above. If you would like to know further details of EKF model, please see this document . The stop filter detects the vehicle's stop state by monitoring both velocity and angular velocity. When so, the stop filter filters the velocity and angular velocity to 0.","title":"Design"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/","text":"Twist Estimator # Twist Estimator # Twist estimator is a component to estimate ego vehicle twist for precise velocity planning and control. The x-axis velocity and z-axis angular velocity in vehicle twist is mainly considered. These values are preferable to be noise-free and unbiased. Input # Input Data Type Vehicle CAN geometry_msgs::TwistStamped IMU sensor_msgs::Imu Output # Output Data Type Use Cases of the output Estimated Twist geometry_msgs::TwistWithCovarianceStamped Pose Twist Fusion Filter Design # In the figure above, the solid line of output shows our implementation, while the dotted lines of output show feasible implementations. Estimated twist compensates estimated pose in Pose Twist Fusion Filter, and also becomes the alternative to estimated pose when the calculation of Pose Estimator is highly unreliable. Therefore, estimated twist is preferable to be noise-free and unbiased. We adopt this design to merge multiple sensor inputs to generate more accurate twist.","title":"Twist estimator"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/#twist-estimator","text":"","title":"Twist Estimator"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/#twist-estimator_1","text":"Twist estimator is a component to estimate ego vehicle twist for precise velocity planning and control. The x-axis velocity and z-axis angular velocity in vehicle twist is mainly considered. These values are preferable to be noise-free and unbiased.","title":"Twist Estimator"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/#input","text":"Input Data Type Vehicle CAN geometry_msgs::TwistStamped IMU sensor_msgs::Imu","title":"Input"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/#output","text":"Output Data Type Use Cases of the output Estimated Twist geometry_msgs::TwistWithCovarianceStamped Pose Twist Fusion Filter","title":"Output"},{"location":"design/software_architecture/Localization/TwistEstimator/TwistEstimator/#design","text":"In the figure above, the solid line of output shows our implementation, while the dotted lines of output show feasible implementations. Estimated twist compensates estimated pose in Pose Twist Fusion Filter, and also becomes the alternative to estimated pose when the calculation of Pose Estimator is highly unreliable. Therefore, estimated twist is preferable to be noise-free and unbiased. We adopt this design to merge multiple sensor inputs to generate more accurate twist.","title":"Design"},{"location":"design/software_architecture/Map/Map/","text":"Map # Overview # Map is responsible for distributing static information about the environment that autonomous vehicle might drive. Currently, this is separated into two categories: Geometric information about the environment (pointcloud map) Semantic information about roads (vector map) Use Cases # Pointcloud Map # Use cases of the maps are the following: Localization: Autoware must always be aware of its position in Earth frame. Pointcloud map is used for local localization with LiDAR based localization algorithm such as NDT matching. Vector Map # Vector map provides semantic information about roads and is used for various uses cases in both Planning and Perception. Note that some of the following use cases might be removed as performance of perception improves. For example, retrieving lane shapes and detecting traffic lights can be done online e.g. using camera images. However, with consideration of spoofing attacks and reliability of current Perception stack, following uses cases must be supported by vector map for now. Planning: Calculating route from start to goal Creating trajectory along lane during lane following Driving according to traffic rules, such as speed limit, traffic light, and right of way. Perception: predicting other participant's trajectory along lanes Detecting traffic lights Requirements # The role of map is to publish map information to other stacks. In order to satisfy use cases above, the following requirements must be met. PointCloud map # The map should provide geometric information of surrounding environment that includes any region 200m away from any possible route that the vehicle might take. Resolution of pointcloud map must be at least 0.2m. (from past experience) Size of the map should be less than 1GB in binary format. (Limit of ROS message) Pointcloud must be georeferenced. Vector Map # The map should be georeferenced. The map should include region within 200m away from any possible route that autonomous vehicle might drive with following information: Routing: the map should be able to retrieve next lane, previous lane, right lane, and left lane of a lane with availability of lane change. Geometry: shape and position of following objects must be provided: lanes traffic lights stop lines crosswalk parking space and parking lots. Traffic rules: the map should be able to retrieve following information from given lane: traffic lights stop lines traffic signs right of way speed limit direction of lanes Input # The input to Map stack: Input Data Type Explanation PointCloud Map file PCD format This includes the shape of surrounding environment as collection of raster points, including grounds and buildings. It may include other additional information such as intensity and colors for visualization. Vector Map file Lanelet2 format This should describe all semantic information about roads. This includes road connection, road geometry, and traffic rules. Supporting format may change to OpenDRIVE in the future as discussed in Map WG. Output # The table below summarizes the output from Map stack: Output Topic Name(Data Type) Explanation PointCloud map /map/pointcloud_map ( sensor_msgs::PointCloud2 ) This includes the shape of surrounding environment as collection of points. This is assumed to be used by Localization module for map matching with LiDAR pointclouds. Vector Map /map/vector_map ( autoware_lanelet2_msgs::MapBin ) Lanelet2 map information will be dumped as serialized data, and passed down to other stacks. Then, it will be converted back to internal data structure to enable Lanelet2 library API access to each data. This is assumed to be used by Localization stack for lane-based localization, Perception stack for trajectory prediction of other vehicles, and Planning to plan behavior to follow traffic rules. Design # Map module consist of two modules: pointcloud map loader and vector map loader. Since map data are converted into binary data array, it is meant to be converted back to internal data structure using appropriate library, for example PCL for pointcloud and Lanelet2 for vector map. The access to each data element is also assumed to be done through map library. PointCloud Map Loader # Role # Role of this module is to output pointcloud map in map frame to be used by other stacks. Input # PCD File This contains collection of point coordinates from reference point. YAML File This is meant to contain the coordinate of origin of PCD file in earth frame (either in ECEF or WGS84) Output # pointcloud_map: sensor_msgs::PointCloud2 This module should output environment information as pointcloud message type. The points in the message should be projected into map frame since the main user is assumed to be Localization stack. Vector Map Loader # Role # Role of this module is to semantic road information in map frame to be used by other stacks. Input # Lanelet2 Map File (OSM file) This includes all lane-related information. The specification about the format is specified here . Output # vector_map autoware_lanelet_msgs::MapBin This contains serialized data of Lanelet2 map. All coordinate data contained in the map should be already projected into map frame using specified ECEF parameter.","title":"Map"},{"location":"design/software_architecture/Map/Map/#map","text":"","title":"Map"},{"location":"design/software_architecture/Map/Map/#overview","text":"Map is responsible for distributing static information about the environment that autonomous vehicle might drive. Currently, this is separated into two categories: Geometric information about the environment (pointcloud map) Semantic information about roads (vector map)","title":"Overview"},{"location":"design/software_architecture/Map/Map/#use-cases","text":"","title":"Use Cases"},{"location":"design/software_architecture/Map/Map/#pointcloud-map","text":"Use cases of the maps are the following: Localization: Autoware must always be aware of its position in Earth frame. Pointcloud map is used for local localization with LiDAR based localization algorithm such as NDT matching.","title":"Pointcloud Map"},{"location":"design/software_architecture/Map/Map/#vector-map","text":"Vector map provides semantic information about roads and is used for various uses cases in both Planning and Perception. Note that some of the following use cases might be removed as performance of perception improves. For example, retrieving lane shapes and detecting traffic lights can be done online e.g. using camera images. However, with consideration of spoofing attacks and reliability of current Perception stack, following uses cases must be supported by vector map for now. Planning: Calculating route from start to goal Creating trajectory along lane during lane following Driving according to traffic rules, such as speed limit, traffic light, and right of way. Perception: predicting other participant's trajectory along lanes Detecting traffic lights","title":"Vector Map"},{"location":"design/software_architecture/Map/Map/#requirements","text":"The role of map is to publish map information to other stacks. In order to satisfy use cases above, the following requirements must be met.","title":"Requirements"},{"location":"design/software_architecture/Map/Map/#pointcloud-map_1","text":"The map should provide geometric information of surrounding environment that includes any region 200m away from any possible route that the vehicle might take. Resolution of pointcloud map must be at least 0.2m. (from past experience) Size of the map should be less than 1GB in binary format. (Limit of ROS message) Pointcloud must be georeferenced.","title":"PointCloud map"},{"location":"design/software_architecture/Map/Map/#vector-map_1","text":"The map should be georeferenced. The map should include region within 200m away from any possible route that autonomous vehicle might drive with following information: Routing: the map should be able to retrieve next lane, previous lane, right lane, and left lane of a lane with availability of lane change. Geometry: shape and position of following objects must be provided: lanes traffic lights stop lines crosswalk parking space and parking lots. Traffic rules: the map should be able to retrieve following information from given lane: traffic lights stop lines traffic signs right of way speed limit direction of lanes","title":"Vector Map"},{"location":"design/software_architecture/Map/Map/#input","text":"The input to Map stack: Input Data Type Explanation PointCloud Map file PCD format This includes the shape of surrounding environment as collection of raster points, including grounds and buildings. It may include other additional information such as intensity and colors for visualization. Vector Map file Lanelet2 format This should describe all semantic information about roads. This includes road connection, road geometry, and traffic rules. Supporting format may change to OpenDRIVE in the future as discussed in Map WG.","title":"Input"},{"location":"design/software_architecture/Map/Map/#output","text":"The table below summarizes the output from Map stack: Output Topic Name(Data Type) Explanation PointCloud map /map/pointcloud_map ( sensor_msgs::PointCloud2 ) This includes the shape of surrounding environment as collection of points. This is assumed to be used by Localization module for map matching with LiDAR pointclouds. Vector Map /map/vector_map ( autoware_lanelet2_msgs::MapBin ) Lanelet2 map information will be dumped as serialized data, and passed down to other stacks. Then, it will be converted back to internal data structure to enable Lanelet2 library API access to each data. This is assumed to be used by Localization stack for lane-based localization, Perception stack for trajectory prediction of other vehicles, and Planning to plan behavior to follow traffic rules.","title":"Output"},{"location":"design/software_architecture/Map/Map/#design","text":"Map module consist of two modules: pointcloud map loader and vector map loader. Since map data are converted into binary data array, it is meant to be converted back to internal data structure using appropriate library, for example PCL for pointcloud and Lanelet2 for vector map. The access to each data element is also assumed to be done through map library.","title":"Design"},{"location":"design/software_architecture/Map/Map/#pointcloud-map-loader","text":"","title":"PointCloud Map Loader"},{"location":"design/software_architecture/Map/Map/#role","text":"Role of this module is to output pointcloud map in map frame to be used by other stacks.","title":"Role"},{"location":"design/software_architecture/Map/Map/#input_1","text":"PCD File This contains collection of point coordinates from reference point. YAML File This is meant to contain the coordinate of origin of PCD file in earth frame (either in ECEF or WGS84)","title":"Input"},{"location":"design/software_architecture/Map/Map/#output_1","text":"pointcloud_map: sensor_msgs::PointCloud2 This module should output environment information as pointcloud message type. The points in the message should be projected into map frame since the main user is assumed to be Localization stack.","title":"Output"},{"location":"design/software_architecture/Map/Map/#vector-map-loader","text":"","title":"Vector Map Loader"},{"location":"design/software_architecture/Map/Map/#role_1","text":"Role of this module is to semantic road information in map frame to be used by other stacks.","title":"Role"},{"location":"design/software_architecture/Map/Map/#input_2","text":"Lanelet2 Map File (OSM file) This includes all lane-related information. The specification about the format is specified here .","title":"Input"},{"location":"design/software_architecture/Map/Map/#output_2","text":"vector_map autoware_lanelet_msgs::MapBin This contains serialized data of Lanelet2 map. All coordinate data contained in the map should be already projected into map frame using specified ECEF parameter.","title":"Output"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/","text":"Modifying Lanelet2 format for Autoware # Overview # About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document. Additional Conventions # Lane length # Original Lanelet2 format does not specify the minimum or maximum length of lane. However, without the limit, it is very difficult to estimate memory size and computation time using lane information. Therefore, we set general convention that a lanelet should have length between 1m ~ 50m. No Self Crossing Lanes # In order to make geometrical calculation of lane easier (e.g. getting drivable area, operating triangulation, etc) we set a convention that no lanes should overlap itself. Whenever, it overlaps itself, the lane should be separated into two different lanes with same attributes. Additional Mandatory Tags # Elevation Tags # Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node> SpeedLimit Tags # Speed limit(\"speed_limit\") information for lanelet( relation ) is optional in default Lanelet2 format, and there are several ways of setting the information (e.g. using tag or using regulatory element or using default value in traffic rules). To avoid confusion, we will make this information mandatory for all road lanelets, and should not expect Autoware to use default value. Therefore, users must make sure that all lanelets in their osm maps contain speed_limit tags. Here is an example osm syntax for a lanelet object. <relation id= \"34408\" visible= \"true\" version= \"1\" > <member type= \"way\" ref= \"34406\" role= \"left\" /> <member type= \"way\" ref= \"34407\" role= \"right\" /> <tag k= \"speed_limit\" v= \"30m/h\" /> <tag k= \"subtype\" v= \"road\" /> <tag k= \"type\" v= \"lanelet\" /> </relation> TrafficLights # Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction(not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way> Turn Directions # Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation> Optional Tags # Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware. Meta Info # Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm> Local Coordinate Expression # Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node>","title":"Modifying Lanelet2 format for Autoware"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#modifying-lanelet2-format-for-autoware","text":"","title":"Modifying Lanelet2 format for Autoware"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#overview","text":"About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document.","title":"Overview"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#additional-conventions","text":"","title":"Additional Conventions"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#lane-length","text":"Original Lanelet2 format does not specify the minimum or maximum length of lane. However, without the limit, it is very difficult to estimate memory size and computation time using lane information. Therefore, we set general convention that a lanelet should have length between 1m ~ 50m.","title":"Lane length"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#no-self-crossing-lanes","text":"In order to make geometrical calculation of lane easier (e.g. getting drivable area, operating triangulation, etc) we set a convention that no lanes should overlap itself. Whenever, it overlaps itself, the lane should be separated into two different lanes with same attributes.","title":"No Self Crossing Lanes"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#additional-mandatory-tags","text":"","title":"Additional Mandatory Tags"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#elevation-tags","text":"Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node>","title":"Elevation Tags"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#speedlimit-tags","text":"Speed limit(\"speed_limit\") information for lanelet( relation ) is optional in default Lanelet2 format, and there are several ways of setting the information (e.g. using tag or using regulatory element or using default value in traffic rules). To avoid confusion, we will make this information mandatory for all road lanelets, and should not expect Autoware to use default value. Therefore, users must make sure that all lanelets in their osm maps contain speed_limit tags. Here is an example osm syntax for a lanelet object. <relation id= \"34408\" visible= \"true\" version= \"1\" > <member type= \"way\" ref= \"34406\" role= \"left\" /> <member type= \"way\" ref= \"34407\" role= \"right\" /> <tag k= \"speed_limit\" v= \"30m/h\" /> <tag k= \"subtype\" v= \"road\" /> <tag k= \"type\" v= \"lanelet\" /> </relation>","title":"SpeedLimit Tags"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#trafficlights","text":"Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction(not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way>","title":"TrafficLights"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#turn-directions","text":"Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation>","title":"Turn Directions"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#optional-tags","text":"Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware.","title":"Optional Tags"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#meta-info","text":"Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm>","title":"Meta Info"},{"location":"design/software_architecture/Map/SemanticMap/AutowareLanelet2Format/#local-coordinate-expression","text":"Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node>","title":"Local Coordinate Expression"},{"location":"design/software_architecture/Perception/Perception/","text":"Perception # Overview # Perception stack recognizes the surrounding of the vehicle to achieve safe and efficient autonomous driving. The output of Sensing describes environment \"as is\", and is usually too primitive to be used directly for high-level planning. Perception stack will extract key and organize it into more meaningful data for Planning stack. Role # Perception stack has 2 main roles. Object Recognition Traffic Light Recognition Use Cases # Perception must provide enough information to support following use cases: Use case Required output from Perception How the output is used 1. Changing lane Object Recognition-Prediction : - Predicted paths of objects on target lane To decide when and where changing lane depending on objects' predicted paths. when : which timing to trigger lane change depending on obstacles position and velocity. where : where to go depending on objects' position and shape. 2. Turning at intersection Object Recognition- Prediction : - Predicted paths of objects at an intersection To decide when turning at an intersection depending on objects' predicted path. when : which timing to turning depending on objects' future paths. 3. Avoiding parked vehicles Object Recognition- Detection : - Objects' class, shape and, position Object Recognition- Tracking : - Objects' velocity To decide where to avoid objects depending on objects' properties. where : where to avoid objects in given area depending on objects' class, velocity, shape and position. 4. Stopping at a crosswalk when pedestrians are walking Object Recognition- Prediction : - Predicted paths of objects at a crosswalk To decide where stopping based on pedestrians' position and velocity. 5. Passing intersection without traffic lights Object Recognition- Detection : - Objects' shape. Object Recognition- Prediction : - Predicted paths of objects at an intersection To decide when passing intersection depending on objects' properties. when : which timing to pass intersection while negotiating with other objects based on objects' properties like, predicted paths and shape. Merging into another lane Object Recognition- Prediction : - Predicted paths of objects at merging area To decide when merging into another lane depending objects' predicted paths. 6. Taking over Pedestrian/Cyclists Object Recognition- Detection : - Objects' shape, position and orientation. Object Recognition- Tracking : - Objects' velocity To decide when and where taking over depending on objects' predicted paths when : which timing to taking over depending on obstacles position and velocity. where : where to go depending on objects' position and shape. 7. Stopping/yielding to an obstacle Object Recognition- Detection : - Objects' shape, position, and orientation Object Recognition- Tracking : - Objects' velocity To decide where to stop/yield based on pedestrians' position and velocity. 8. Passing intersection with traffic lights Traffic Light Recognition- Classification : - Traffic signal status To decide whether to go or stop based on traffic signal status. Requirements # From above table, high-level requirements of Perception stack are summarized as below: Perception stack should recognize following objects: (Use Case 3, 5, 6) vehicle pedestrian cyclists other objects that is on road or parking lot For each object, Perception stack should provide following information: (Use Case 1-7) Pose (done in ObjectDetection) Shape (done in ObjectDetection) Predicted future path (done in Object Tracking+Prediction) Perception stack should provide traffic light information: (Use Case \ud83d\ude0e The status of traffic light Unique id of traffic light from map Input # Input Topic (Data Type) Explanation LiDAR /sensing/lidar/pointcloud ( sensor_msgs::PointCloud2 ) Pointcloud data captured by LiDAR comes from Sensing stack. Perception stack is allowed to choose subscribing to pointcloud with/without background depending on algorithms. Camera /sensing/{camera_name}/image ( sensor_msgs::Image ) Image data captured by Camera comes from Sensing stack. CameraInfo contains intrinsic parameters for the image. Map /map/vector_map ( autoware_lanelet2_msgs::MapBin ) This is Map data in lanelet2 format. Map stack has utility packages for processing map data. Drive Route (optional) /planning/route ( autoware_planning_msgs::Route ) This is route information for reaching a destination. In Perception stack, it is used for detecting the traffic lights associated with route information. Output # Output Topic Name (Data Type) Explanation Dynamic Object /perception/object_recognition/objects ( autoware_perception_msgs::DynamicObjectArray ) This includes obstacles' information. An obstacle is described by 3 major properties; State, Shape, Semantic. Detail design for these properties is in below Object Recognition section. Traffic Light State /perception/traffic_light_recognition/traffic_light_states ( autoware_perception_msgs::TrafficLightStateArray ) This includes the status of traffic light signals in array format. Design # This Perception stack consists of 2 separated modules and each module can be subdivided into following components: Object Recognition (satisfies Requirement 1 and 2) Detection Tracking Prediction Traffic Light Recognition (satisfies requirement 3) Detection Classification Key points of the structure # Interfaces are separated according to the current algorithm level. Enable complex autonomous driving use cases by including information like objects' future movement. Depends on technology development in the future, this structure might be changed (e.g. E2E). Object Recognition # Role # Recognize obstacles that could potentially move. Provide detail information for obstacles required in the Planning stack. The motivation behind recognizing obstacles comes from a requirement for balancing safety and efficiency in autonomous driving. If emphasizing safety too much, it needs to consider every possible movement of obstacles. Autonomous vehicles could end up freezing. If emphasizing efficiency too much, recognize every object as static obstacles. A car could hit a pedestrian in an intersection because of the efficient drive to a destination. Balanced autonomous driving is achieved by recognizing obstacles. Requirement # Detection # Detection component detects objects from sensor data. Detection component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type type Class information uint8 autoware_perception_msgs::Semantic confidence Class's confidence 0.0~1.0. float64 autoware_perception_msgs::Semantic pose Position and orientation geometry_msgs::PoseWithCovariance autoware_perception_msgs::State orientation_reliable Boolean for stable orientation or not bool autoware_perception_msgs::State shape Shape in 3D bounding box, cylinder or polygon autoware_perception_msgs::Shape autoware_perception_msgs::DynamicObject Tracking # Tracking component deals with time-series processing. Tracking component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type id Unique object id over frames uuid_msgs::UniqueID autoware_perception_msgs::DynamicObject twist Velocity in ROS twist format. geometry_msgs::TwistWithCovariance autoware_perception_msgs::State twist_reliable Boolean for stable twist or not. bool autoware_perception_msgs::State acceleration Acceleration in ROS accel format. geometry_msgs::AccelWithCovariance autoware_perception_msgs::State acceleration_reliable Boolean for stable acceleration or not. bool autoware_perception_msgs::State Prediction # Prediction component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type predicted_path Predicted future paths for an object. autoware_perception_msgs::PredictedPath[] autoware_perception_msgs::State Necessary information is defined in autoware_perception_msg::DynamicObjectArray.msg with layered msg structure. Input # LiDAR # sensor_msgs::PointCloud2 LiDAR input is an essential input for recognizing objects. Its ability to describe the 3D world is utilized in detecting obstacles surrounding the vehicle. Camera (optional) # sensor_msgs::Image Camera input is used to obtain details about obstacles. Fine resolution camera enables to detect objects at far distance with high accuracy. Map # autoware_lanelet2_msgs::MapBin Assuming the obstacles which follow rules in a map, Perception stack can infer objects' property such as orientation or future movements by using map information. Output # autoware_perception_msgs::DynamicObjectArray Recognized objects with predicted paths are used in situations like intersection, crosswalk and lane change. Planning stack uses objects' information for avoiding objects or following a vehicle ahead. Traffic light recognition # Make sense of traffic light's signal. Definition # Not only classifying its color but also understanding unique signals like arrow signals. It needs to recognize traffic light signals in order to ensure safe autonomous driving. Requirement # Need to fill lamp_states in autoware_traffic_light_msg::TrafficLightState.msg Property Definition Data Type Parent Data Type lamp_states Sequence of traffic light result autoware_perception_msgs::LampState[] autoware_perception_msgs::TrafficLightState id This id corresponds to the traffic light id defined in a map int32 autoware_perception_msgs::TrafficLightState Input # Camera # sensor_msgs::Image Mainly using camera data to make sense of traffic light's color. Map # autoware_lanelet2_msgs::MapBin By using a map with traffic light's location, clarify which part of an image needs to be paid attention. Drive Route (optional) # autoware_planning_msgs::Route With the route associated with the traffic light, improve the accuracy of traffic light recognition. Output # autoware_perception_msgs::TrafficLightStateArray Planning stack receives data from this module. It changes the vehicle maneuver based on the result of traffic light recognition.","title":"Overall"},{"location":"design/software_architecture/Perception/Perception/#perception","text":"","title":"Perception"},{"location":"design/software_architecture/Perception/Perception/#overview","text":"Perception stack recognizes the surrounding of the vehicle to achieve safe and efficient autonomous driving. The output of Sensing describes environment \"as is\", and is usually too primitive to be used directly for high-level planning. Perception stack will extract key and organize it into more meaningful data for Planning stack.","title":"Overview"},{"location":"design/software_architecture/Perception/Perception/#role","text":"Perception stack has 2 main roles. Object Recognition Traffic Light Recognition","title":"Role"},{"location":"design/software_architecture/Perception/Perception/#use-cases","text":"Perception must provide enough information to support following use cases: Use case Required output from Perception How the output is used 1. Changing lane Object Recognition-Prediction : - Predicted paths of objects on target lane To decide when and where changing lane depending on objects' predicted paths. when : which timing to trigger lane change depending on obstacles position and velocity. where : where to go depending on objects' position and shape. 2. Turning at intersection Object Recognition- Prediction : - Predicted paths of objects at an intersection To decide when turning at an intersection depending on objects' predicted path. when : which timing to turning depending on objects' future paths. 3. Avoiding parked vehicles Object Recognition- Detection : - Objects' class, shape and, position Object Recognition- Tracking : - Objects' velocity To decide where to avoid objects depending on objects' properties. where : where to avoid objects in given area depending on objects' class, velocity, shape and position. 4. Stopping at a crosswalk when pedestrians are walking Object Recognition- Prediction : - Predicted paths of objects at a crosswalk To decide where stopping based on pedestrians' position and velocity. 5. Passing intersection without traffic lights Object Recognition- Detection : - Objects' shape. Object Recognition- Prediction : - Predicted paths of objects at an intersection To decide when passing intersection depending on objects' properties. when : which timing to pass intersection while negotiating with other objects based on objects' properties like, predicted paths and shape. Merging into another lane Object Recognition- Prediction : - Predicted paths of objects at merging area To decide when merging into another lane depending objects' predicted paths. 6. Taking over Pedestrian/Cyclists Object Recognition- Detection : - Objects' shape, position and orientation. Object Recognition- Tracking : - Objects' velocity To decide when and where taking over depending on objects' predicted paths when : which timing to taking over depending on obstacles position and velocity. where : where to go depending on objects' position and shape. 7. Stopping/yielding to an obstacle Object Recognition- Detection : - Objects' shape, position, and orientation Object Recognition- Tracking : - Objects' velocity To decide where to stop/yield based on pedestrians' position and velocity. 8. Passing intersection with traffic lights Traffic Light Recognition- Classification : - Traffic signal status To decide whether to go or stop based on traffic signal status.","title":"Use Cases"},{"location":"design/software_architecture/Perception/Perception/#requirements","text":"From above table, high-level requirements of Perception stack are summarized as below: Perception stack should recognize following objects: (Use Case 3, 5, 6) vehicle pedestrian cyclists other objects that is on road or parking lot For each object, Perception stack should provide following information: (Use Case 1-7) Pose (done in ObjectDetection) Shape (done in ObjectDetection) Predicted future path (done in Object Tracking+Prediction) Perception stack should provide traffic light information: (Use Case \ud83d\ude0e The status of traffic light Unique id of traffic light from map","title":"Requirements"},{"location":"design/software_architecture/Perception/Perception/#input","text":"Input Topic (Data Type) Explanation LiDAR /sensing/lidar/pointcloud ( sensor_msgs::PointCloud2 ) Pointcloud data captured by LiDAR comes from Sensing stack. Perception stack is allowed to choose subscribing to pointcloud with/without background depending on algorithms. Camera /sensing/{camera_name}/image ( sensor_msgs::Image ) Image data captured by Camera comes from Sensing stack. CameraInfo contains intrinsic parameters for the image. Map /map/vector_map ( autoware_lanelet2_msgs::MapBin ) This is Map data in lanelet2 format. Map stack has utility packages for processing map data. Drive Route (optional) /planning/route ( autoware_planning_msgs::Route ) This is route information for reaching a destination. In Perception stack, it is used for detecting the traffic lights associated with route information.","title":"Input"},{"location":"design/software_architecture/Perception/Perception/#output","text":"Output Topic Name (Data Type) Explanation Dynamic Object /perception/object_recognition/objects ( autoware_perception_msgs::DynamicObjectArray ) This includes obstacles' information. An obstacle is described by 3 major properties; State, Shape, Semantic. Detail design for these properties is in below Object Recognition section. Traffic Light State /perception/traffic_light_recognition/traffic_light_states ( autoware_perception_msgs::TrafficLightStateArray ) This includes the status of traffic light signals in array format.","title":"Output"},{"location":"design/software_architecture/Perception/Perception/#design","text":"This Perception stack consists of 2 separated modules and each module can be subdivided into following components: Object Recognition (satisfies Requirement 1 and 2) Detection Tracking Prediction Traffic Light Recognition (satisfies requirement 3) Detection Classification","title":"Design"},{"location":"design/software_architecture/Perception/Perception/#key-points-of-the-structure","text":"Interfaces are separated according to the current algorithm level. Enable complex autonomous driving use cases by including information like objects' future movement. Depends on technology development in the future, this structure might be changed (e.g. E2E).","title":"Key points of the structure"},{"location":"design/software_architecture/Perception/Perception/#object-recognition","text":"","title":"Object Recognition"},{"location":"design/software_architecture/Perception/Perception/#role_1","text":"Recognize obstacles that could potentially move. Provide detail information for obstacles required in the Planning stack. The motivation behind recognizing obstacles comes from a requirement for balancing safety and efficiency in autonomous driving. If emphasizing safety too much, it needs to consider every possible movement of obstacles. Autonomous vehicles could end up freezing. If emphasizing efficiency too much, recognize every object as static obstacles. A car could hit a pedestrian in an intersection because of the efficient drive to a destination. Balanced autonomous driving is achieved by recognizing obstacles.","title":"Role"},{"location":"design/software_architecture/Perception/Perception/#requirement","text":"","title":"Requirement"},{"location":"design/software_architecture/Perception/Perception/#detection","text":"Detection component detects objects from sensor data. Detection component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type type Class information uint8 autoware_perception_msgs::Semantic confidence Class's confidence 0.0~1.0. float64 autoware_perception_msgs::Semantic pose Position and orientation geometry_msgs::PoseWithCovariance autoware_perception_msgs::State orientation_reliable Boolean for stable orientation or not bool autoware_perception_msgs::State shape Shape in 3D bounding box, cylinder or polygon autoware_perception_msgs::Shape autoware_perception_msgs::DynamicObject","title":"Detection"},{"location":"design/software_architecture/Perception/Perception/#tracking","text":"Tracking component deals with time-series processing. Tracking component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type id Unique object id over frames uuid_msgs::UniqueID autoware_perception_msgs::DynamicObject twist Velocity in ROS twist format. geometry_msgs::TwistWithCovariance autoware_perception_msgs::State twist_reliable Boolean for stable twist or not. bool autoware_perception_msgs::State acceleration Acceleration in ROS accel format. geometry_msgs::AccelWithCovariance autoware_perception_msgs::State acceleration_reliable Boolean for stable acceleration or not. bool autoware_perception_msgs::State","title":"Tracking"},{"location":"design/software_architecture/Perception/Perception/#prediction","text":"Prediction component is responsible for clarifying the following objects' property. Property Definition Data Type Parent Data Type predicted_path Predicted future paths for an object. autoware_perception_msgs::PredictedPath[] autoware_perception_msgs::State Necessary information is defined in autoware_perception_msg::DynamicObjectArray.msg with layered msg structure.","title":"Prediction"},{"location":"design/software_architecture/Perception/Perception/#input_1","text":"","title":"Input"},{"location":"design/software_architecture/Perception/Perception/#lidar","text":"sensor_msgs::PointCloud2 LiDAR input is an essential input for recognizing objects. Its ability to describe the 3D world is utilized in detecting obstacles surrounding the vehicle.","title":"LiDAR"},{"location":"design/software_architecture/Perception/Perception/#camera-optional","text":"sensor_msgs::Image Camera input is used to obtain details about obstacles. Fine resolution camera enables to detect objects at far distance with high accuracy.","title":"Camera (optional)"},{"location":"design/software_architecture/Perception/Perception/#map","text":"autoware_lanelet2_msgs::MapBin Assuming the obstacles which follow rules in a map, Perception stack can infer objects' property such as orientation or future movements by using map information.","title":"Map"},{"location":"design/software_architecture/Perception/Perception/#output_1","text":"autoware_perception_msgs::DynamicObjectArray Recognized objects with predicted paths are used in situations like intersection, crosswalk and lane change. Planning stack uses objects' information for avoiding objects or following a vehicle ahead.","title":"Output"},{"location":"design/software_architecture/Perception/Perception/#traffic-light-recognition","text":"Make sense of traffic light's signal.","title":"Traffic light recognition"},{"location":"design/software_architecture/Perception/Perception/#definition","text":"Not only classifying its color but also understanding unique signals like arrow signals. It needs to recognize traffic light signals in order to ensure safe autonomous driving.","title":"Definition"},{"location":"design/software_architecture/Perception/Perception/#requirement_1","text":"Need to fill lamp_states in autoware_traffic_light_msg::TrafficLightState.msg Property Definition Data Type Parent Data Type lamp_states Sequence of traffic light result autoware_perception_msgs::LampState[] autoware_perception_msgs::TrafficLightState id This id corresponds to the traffic light id defined in a map int32 autoware_perception_msgs::TrafficLightState","title":"Requirement"},{"location":"design/software_architecture/Perception/Perception/#input_2","text":"","title":"Input"},{"location":"design/software_architecture/Perception/Perception/#camera","text":"sensor_msgs::Image Mainly using camera data to make sense of traffic light's color.","title":"Camera"},{"location":"design/software_architecture/Perception/Perception/#map_1","text":"autoware_lanelet2_msgs::MapBin By using a map with traffic light's location, clarify which part of an image needs to be paid attention.","title":"Map"},{"location":"design/software_architecture/Perception/Perception/#drive-route-optional","text":"autoware_planning_msgs::Route With the route associated with the traffic light, improve the accuracy of traffic light recognition.","title":"Drive Route (optional)"},{"location":"design/software_architecture/Perception/Perception/#output_2","text":"autoware_perception_msgs::TrafficLightStateArray Planning stack receives data from this module. It changes the vehicle maneuver based on the result of traffic light recognition.","title":"Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/","text":"Detection # Use Cases and Requirements # Detection in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Avoiding parked vehicles Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack . Role # Detection in Object Recognition detects objects by processing sensor data. Detection is triggered on every sensor data callback independently from previous detection results. The Detection module is responsible for calculating objects' pose, class, and shape. Input # Input Data Type Topic LiDAR sensor_msgs::PointCloud2 /sensing/lidar/pointcloud Camera sensor_msgs::Image /sensing/camera/*/image_raw Camera info sensor_msgs::CameraInfo /sensing/camera/*/camera_info TF tf2_msgs::TFMessage /tf Output # Output Data Type Output Module TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Object Recognition: Tracking base_link /perception/object_recognition/detection/objects Design # The Detection module is designed to adopt various detection algorithms. This is one of our sample implementations for the Detection module. Requirement in Output # Designated objects' properties in autoware_perception_msgs::DynamicObject need to be filled in the Detection module before passing to the Tracking module. Property Definition Data Type Parent Data Type type Class information uint8 autoware_perception_msgs::Semantic confidence Class's confidence 0.0~1.0. float64 autoware_perception_msgs::Semantic pose Position and orientation geometry_msgs::PoseWithCovariance autoware_perception_msgs::State orientation_reliable Boolean for stable orientation or not bool autoware_perception_msgs::State shape Shape in 3D bounding box, cylinder or polygon autoware_perception_msgs::Shape autoware_perception_msgs::DynamicObject","title":"Detection"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#detection","text":"","title":"Detection"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#use-cases-and-requirements","text":"Detection in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Avoiding parked vehicles Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#role","text":"Detection in Object Recognition detects objects by processing sensor data. Detection is triggered on every sensor data callback independently from previous detection results. The Detection module is responsible for calculating objects' pose, class, and shape.","title":"Role"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#input","text":"Input Data Type Topic LiDAR sensor_msgs::PointCloud2 /sensing/lidar/pointcloud Camera sensor_msgs::Image /sensing/camera/*/image_raw Camera info sensor_msgs::CameraInfo /sensing/camera/*/camera_info TF tf2_msgs::TFMessage /tf","title":"Input"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#output","text":"Output Data Type Output Module TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Object Recognition: Tracking base_link /perception/object_recognition/detection/objects","title":"Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#design","text":"The Detection module is designed to adopt various detection algorithms. This is one of our sample implementations for the Detection module.","title":"Design"},{"location":"design/software_architecture/Perception/ObjectRecognition/Detection/Detection/#requirement-in-output","text":"Designated objects' properties in autoware_perception_msgs::DynamicObject need to be filled in the Detection module before passing to the Tracking module. Property Definition Data Type Parent Data Type type Class information uint8 autoware_perception_msgs::Semantic confidence Class's confidence 0.0~1.0. float64 autoware_perception_msgs::Semantic pose Position and orientation geometry_msgs::PoseWithCovariance autoware_perception_msgs::State orientation_reliable Boolean for stable orientation or not bool autoware_perception_msgs::State shape Shape in 3D bounding box, cylinder or polygon autoware_perception_msgs::Shape autoware_perception_msgs::DynamicObject","title":"Requirement in Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/","text":"Detection # Use Cases and Requirements # Prediction in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack . Role # Prediction in Object Recognition estimate objects' intention. Intentions are represented as objects' future trajectories with covariance. The Planning module makes a decision and plans a future ego-motion based on the results of predicted objects. Input # Input Data Type Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray /perception/object_recognition/tracking/objects Map autoware_lanelet2_msgs::MapBin /map/vector_map TF tf2_msgs::TFMessage /tf Output # Output Data Type Output Component TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Planning map /perception/object_recognition/objects Design # This is our sample implementation for the Tracking module. Requirement in Output # Designated objects' property in autoware_perception_msgs::DynamicObject needs to be filled in the Prediction module before passing to the Planning component. Property Definition Data Type Parent Data Type predicted_path Predicted future paths for an object. autoware_perception_msgs::PredictedPath[] autoware_perception_msgs::State","title":"Prediction"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#detection","text":"","title":"Detection"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#use-cases-and-requirements","text":"Prediction in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#role","text":"Prediction in Object Recognition estimate objects' intention. Intentions are represented as objects' future trajectories with covariance. The Planning module makes a decision and plans a future ego-motion based on the results of predicted objects.","title":"Role"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#input","text":"Input Data Type Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray /perception/object_recognition/tracking/objects Map autoware_lanelet2_msgs::MapBin /map/vector_map TF tf2_msgs::TFMessage /tf","title":"Input"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#output","text":"Output Data Type Output Component TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Planning map /perception/object_recognition/objects","title":"Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#design","text":"This is our sample implementation for the Tracking module.","title":"Design"},{"location":"design/software_architecture/Perception/ObjectRecognition/Prediction/Prediction/#requirement-in-output","text":"Designated objects' property in autoware_perception_msgs::DynamicObject needs to be filled in the Prediction module before passing to the Planning component. Property Definition Data Type Parent Data Type predicted_path Predicted future paths for an object. autoware_perception_msgs::PredictedPath[] autoware_perception_msgs::State","title":"Requirement in Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/","text":"Detection # Use Cases and Requirements # Tracking in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Avoiding parked vehicles Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack . Role # Tracking in Object Recognition keeps objects' unique id over time. This time series processing leads to estimating objects' property such as their velocity and/or acceleration. Furthermore, it could estimate more accurate objects' orientation by leveraging the Detection results over time. Input # Input Data Type Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray /perception/object_recognition/detection/objects TF tf2_msgs::TFMessage /tf Output # Output Data Type Output Module TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Object Recognition: Prediction map /perception/object_recognition/tracking/objects Design # This is our sample implementation for the Tracking module. Requirement in Output # Designated objects' properties in autoware_perception_msgs::DynamicObject need to be filled in the Tracking module before passing to the Prediction module. Property Definition Data Type Parent Data Type id Unique object id over frames uuid_msgs::UniqueID autoware_perception_msgs::DynamicObject twist Velocity in ROS twist format. geometry_msgs::TwistWithCovariance autoware_perception_msgs::State twist_reliable Boolean for stable twist or not. bool autoware_perception_msgs::State acceleration Acceleration in ROS accel format. geometry_msgs::AccelWithCovariance autoware_perception_msgs::State acceleration_reliable Boolean for stable acceleration or not. bool autoware_perception_msgs::State","title":"Tracking"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#detection","text":"","title":"Detection"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#use-cases-and-requirements","text":"Tracking in Object Recognition is required for use cases involved with obstacles: Changing lane Turning at intersection Avoiding parked vehicles Stopping at a crosswalk when pedestrians are walking Passing intersection without traffic lights Merging into another lane Taking over Pedestrian/Cyclists Stopping/yielding to an obstacle For the details about related requirements, please refer to the document for Perception stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#role","text":"Tracking in Object Recognition keeps objects' unique id over time. This time series processing leads to estimating objects' property such as their velocity and/or acceleration. Furthermore, it could estimate more accurate objects' orientation by leveraging the Detection results over time.","title":"Role"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#input","text":"Input Data Type Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray /perception/object_recognition/detection/objects TF tf2_msgs::TFMessage /tf","title":"Input"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#output","text":"Output Data Type Output Module TF Frame Topic Dynamic Objects autoware_perception_msgs::DynamicObjectArray Object Recognition: Prediction map /perception/object_recognition/tracking/objects","title":"Output"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#design","text":"This is our sample implementation for the Tracking module.","title":"Design"},{"location":"design/software_architecture/Perception/ObjectRecognition/Tracking/Tracking/#requirement-in-output","text":"Designated objects' properties in autoware_perception_msgs::DynamicObject need to be filled in the Tracking module before passing to the Prediction module. Property Definition Data Type Parent Data Type id Unique object id over frames uuid_msgs::UniqueID autoware_perception_msgs::DynamicObject twist Velocity in ROS twist format. geometry_msgs::TwistWithCovariance autoware_perception_msgs::State twist_reliable Boolean for stable twist or not. bool autoware_perception_msgs::State acceleration Acceleration in ROS accel format. geometry_msgs::AccelWithCovariance autoware_perception_msgs::State acceleration_reliable Boolean for stable acceleration or not. bool autoware_perception_msgs::State","title":"Requirement in Output"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/","text":"Classification # Use Cases and Requirements # Classification in Traffic Light Recognition is required for use cases involved with traffic light: Passing the intersection when traffic light is green Stopping at the intersection when traffic signal is red For the details about related requirements, please refer to the document for Perception stack . Role # Classification module recognizes traffic signal status. Unique signal types are handled in LampState.msg definition. Input # Input Data Type Topic Cropped traffic light information autoware_perception_msgs::TrafficLightRoiArray.msg /perception/traffic_light_recognition/rois Camera sensor_msgs::Image /sensing/camera/*/image_raw Output # Output Data Type Output Component Topic Traffic signal status autoware_traffic_light_msgs::TrafficLightStateArray Planning /perception/traffic_light_recognition/traffic_light_states Design # This is our sample implementation for the Classification module. Unique signals are handled in autoware_traffic_light_msgs::LampState . When requiring to detect local unique signals which are not defined here, need to add them in autoware_traffic_light_msgs::LampState .","title":"Classification"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#classification","text":"","title":"Classification"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#use-cases-and-requirements","text":"Classification in Traffic Light Recognition is required for use cases involved with traffic light: Passing the intersection when traffic light is green Stopping at the intersection when traffic signal is red For the details about related requirements, please refer to the document for Perception stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#role","text":"Classification module recognizes traffic signal status. Unique signal types are handled in LampState.msg definition.","title":"Role"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#input","text":"Input Data Type Topic Cropped traffic light information autoware_perception_msgs::TrafficLightRoiArray.msg /perception/traffic_light_recognition/rois Camera sensor_msgs::Image /sensing/camera/*/image_raw","title":"Input"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#output","text":"Output Data Type Output Component Topic Traffic signal status autoware_traffic_light_msgs::TrafficLightStateArray Planning /perception/traffic_light_recognition/traffic_light_states","title":"Output"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Classification/Classification/#design","text":"This is our sample implementation for the Classification module. Unique signals are handled in autoware_traffic_light_msgs::LampState . When requiring to detect local unique signals which are not defined here, need to add them in autoware_traffic_light_msgs::LampState .","title":"Design"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/","text":"Detection # Use Cases and Requirements # Detection in Traffic Light Recognition is required for use cases involved with traffic light: Passing intersection when traffic signal is green Stopping at intersection when traffic signal is red For the details about related requirements, please refer to the document for Perception stack . Role # Detection module in Traffic Light Recognition finds traffic lights' region of interest(ROI) in the image. For example, one image could contain many traffic signals at intersection. However, the number of traffic signals, in which an autonomous vehicle is interested, is limited. Map information is used to point the part of an image which needs to be paid attention to. Input # Input Data Type Topic Camera sensor_msgs::Image /sensing/camera/*/image_raw Camera info sensor_msgs::CameraInfo /sensing/camera/*/camera_info Map autoware_lanelet2_msgs::MapBin /map/vector_map TF tf2_msgs::TFMessage /tf Output # Output Data Type Output Module Topic Cropped traffic light ROI information autoware_perception_msgs::TrafficLightRoiArray.msg Traffic Light Recognition: Classification /perception/traffic_light_recognition/rois Design # The Detection module is designed to modularize some patterns of detecting traffic lights' ROI. This is our sample implementation for the Detection module. Our sample implementation has one advantage over Map-only Detection method, which sometimes suffers from calibration error. In our approach, Map Based Detection passes rough ROIs to Fine Detection so that it would not care minor calibration error. Fine Detection refines the passed rough ROI to accurately cropped traffic signals' ROI.","title":"Detection"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#detection","text":"","title":"Detection"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#use-cases-and-requirements","text":"Detection in Traffic Light Recognition is required for use cases involved with traffic light: Passing intersection when traffic signal is green Stopping at intersection when traffic signal is red For the details about related requirements, please refer to the document for Perception stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#role","text":"Detection module in Traffic Light Recognition finds traffic lights' region of interest(ROI) in the image. For example, one image could contain many traffic signals at intersection. However, the number of traffic signals, in which an autonomous vehicle is interested, is limited. Map information is used to point the part of an image which needs to be paid attention to.","title":"Role"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#input","text":"Input Data Type Topic Camera sensor_msgs::Image /sensing/camera/*/image_raw Camera info sensor_msgs::CameraInfo /sensing/camera/*/camera_info Map autoware_lanelet2_msgs::MapBin /map/vector_map TF tf2_msgs::TFMessage /tf","title":"Input"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#output","text":"Output Data Type Output Module Topic Cropped traffic light ROI information autoware_perception_msgs::TrafficLightRoiArray.msg Traffic Light Recognition: Classification /perception/traffic_light_recognition/rois","title":"Output"},{"location":"design/software_architecture/Perception/TrafficLightRecognition/Detection/Detection/#design","text":"The Detection module is designed to modularize some patterns of detecting traffic lights' ROI. This is our sample implementation for the Detection module. Our sample implementation has one advantage over Map-only Detection method, which sometimes suffers from calibration error. In our approach, Map Based Detection passes rough ROIs to Fine Detection so that it would not care minor calibration error. Fine Detection refines the passed rough ROI to accurately cropped traffic signals' ROI.","title":"Design"},{"location":"design/software_architecture/Planning/DesignRationale/","text":"Planning Architecture Rationale # Requirements for Planning # Planning architecture must be able to support any functions required to achieve the overall use case stated in Overview . This includes: Calculates route that navigates to the desired goal. Plans maneuver to follow the route (e.g. when to lane change, when to turn at intersection). To make sure that vehicle does not collide with obstacles, including pedestrians and other vehicles). To make sure that the vehicle follows traffic rules during the navigation. This includes following traffic light, stopping at stoplines, stopping at crosswalks, etc. Also, since Autoware is open source software and is meant to be used/developed by anyone around the world, the architecture must be: Extensible enough to integrate new algorithms without changing the interface. Extensible enough to adapt to new traffic rules for different countries. Considered Architecture # Before designing planning architecture, we have looked into papers including ones from the participants of the DARPA Urban Challenge. We have also investigated the planning architecture of Apollo-Auto (version 5.0). The summary is explained below. Boss # System overview of Boss is explained in this paper . The planner is decomposed into three layers: mission, behavior, and motion. Mission calculates the high-level global path from starting point to goal point, behavior makes tactical decisions such as lane change decisions and maneuvers at an intersection, and motion calculates low-level trajectory with consideration of kinematic model of the vehicle. Pros # It is intuitive. Data flow is one-directional from mission to behavior to motion. Similar approach was taken by different teams in the DARPA Urban Challenge. It is suitable for OSS used world-wide since all traffic rule handling is done in the behavior layer, and developers only need to modify the behavior layer to support their local rules. Cons # Behavior layer can only make \"conservative\" decisions. Since the behavior layer has no way of knowing the actual trajectory that is calculated by following motion layer, the behavior layer cannot be certain about the validity of decisions. For example, the behavior planner can command lane change only when it is obvious that there is no obstacle in the target lane, and it is not possible to do \"aggressive\" lane change as an architecture. Unified Behavior and Motion # This paper reviews the planning architecture used in the DARPA Urban Challenge and addresses the demerit of splitting behavior layer and motion layer. It also proposes an algorithm to handle decision making and trajectory optimization simultaneously. Pros # It can make more \"aggressive\" decisions compared to Boss type architecture. Cons # The paper focuses on lane change, but the real environment requires a variety of decision making, such as traffic lights and crosswalks. It is questionable that we can handle different kinds of traffic rules with one optimization algorithm. Also, since decision making is also the target of optimization, it is usually difficult to add user-specific constraints to decision making. Victor Tango Type # System overview of Victor Tango is explained in this paper . Victor Tango split Behavior and Motion layer like Boss type. However, unlike Boss type, there is feedback from motion whether a decision made by behavior layer is achievable or not. Pros # It overcomes the weakness of Boss type and can consider trajectory at the behavior level. Cons # The interface between behavior and motion would be tight and dense. It has the risk of having heavy inter-dependency as new traffic rules are added, making it difficult to replace one of the modules with new algorithms in the future. Apollo # Here is the link . Apollo kept updating the planning module at each version update. In version 5.0, they have taken a different approach from others. Apollo split the behavior planner into scenarios, such as intersection, parking, and lane following. In each scenario module, it calls decider and optimizer libraries to achieve specific scenarios. Pros # Specific parameter tunings are available for different scenarios, making it relatively easier to add new traffic rules for different countries. Different optimizers can be used for a different purpose. Cons # As the number of scenario increases, planning selector(or scenario selector) may get very complex as the number of scenario increases. Autoware Planning Architecture # Considering pros and cons of different approaches, we have concluded to take the hybrid approach of Apollo and Boss. We divided planning modules into scenarios just like Apollo, but into a more high-level scenario, such as LaneDriving and Parking. Apollo has smaller units of scenarios, such as intersection and traffic lights, but we expect that those scenarios may occur in parallel(e.g. intersection with traffic lights and crosswalks) and preparing combined-scenario modules would make scenario-selector too complex to be maintained. Instead, we made a scenario to be a more broad concept to keep the number of scenarios to be lower to keep scenario selector comprehensible. Currently, we only have LaneDriving and Parking, and we anticipate to have HighWay and InEmergency scenarios in the future. More investigation is required to establish the definition of a \u201cScenario\u201d, but the convention is that a new scenario must only be added whenever a different \"paradigm\" is needed for planning. For example, LaneDriving is used to drive along public roads, whereas Parking is used for driving free space, and it would be difficult to develop an algorithm to support requirements for both scenarios. LaneDriving wouldn't require complex path planner as the shape of lanes are already given from HD map, but it must be done at a higher frequency to drive at higher speed, whereas parking requires more complex maneuvers with cut-backs but with lower constraint about computation time. Therefore, it would make more sense to split them into different scenarios, and design a planner to support contradicting requirements. Note that we didn't split LaneDriving into smaller scenarios, unlike Apollo. We have considered all traffic rule related scenes on public roads, including yielding, traffic lights, crosswalks, and bare intersections, to be essentially velocity planning along lanes and can be handled within a single scheme. Reference # Boss: https://www.ri.cmu.edu/pub_files/pub4/urmson_christopher_2008_1/urmson_christopher_2008_1.pdf CMU Doctor Thesis: https://www.researchgate.net/publication/315067229_Improved_Trajectory_Planning_for_On-Road_Self-Driving_Vehicles_Via_Combined_Graph_Search_Optimization_Topology_Analysis Victor Tango: https://archive.darpa.mil/grandchallenge/TechPapers/Victor_Tango.pdf Apollo Auto: https://github.com/ApolloAuto/apollo/tree/r5.0.0/modules/planning","title":"Design rationale"},{"location":"design/software_architecture/Planning/DesignRationale/#planning-architecture-rationale","text":"","title":"Planning Architecture Rationale"},{"location":"design/software_architecture/Planning/DesignRationale/#requirements-for-planning","text":"Planning architecture must be able to support any functions required to achieve the overall use case stated in Overview . This includes: Calculates route that navigates to the desired goal. Plans maneuver to follow the route (e.g. when to lane change, when to turn at intersection). To make sure that vehicle does not collide with obstacles, including pedestrians and other vehicles). To make sure that the vehicle follows traffic rules during the navigation. This includes following traffic light, stopping at stoplines, stopping at crosswalks, etc. Also, since Autoware is open source software and is meant to be used/developed by anyone around the world, the architecture must be: Extensible enough to integrate new algorithms without changing the interface. Extensible enough to adapt to new traffic rules for different countries.","title":"Requirements for Planning"},{"location":"design/software_architecture/Planning/DesignRationale/#considered-architecture","text":"Before designing planning architecture, we have looked into papers including ones from the participants of the DARPA Urban Challenge. We have also investigated the planning architecture of Apollo-Auto (version 5.0). The summary is explained below.","title":"Considered Architecture"},{"location":"design/software_architecture/Planning/DesignRationale/#boss","text":"System overview of Boss is explained in this paper . The planner is decomposed into three layers: mission, behavior, and motion. Mission calculates the high-level global path from starting point to goal point, behavior makes tactical decisions such as lane change decisions and maneuvers at an intersection, and motion calculates low-level trajectory with consideration of kinematic model of the vehicle.","title":"Boss"},{"location":"design/software_architecture/Planning/DesignRationale/#pros","text":"It is intuitive. Data flow is one-directional from mission to behavior to motion. Similar approach was taken by different teams in the DARPA Urban Challenge. It is suitable for OSS used world-wide since all traffic rule handling is done in the behavior layer, and developers only need to modify the behavior layer to support their local rules.","title":"Pros"},{"location":"design/software_architecture/Planning/DesignRationale/#cons","text":"Behavior layer can only make \"conservative\" decisions. Since the behavior layer has no way of knowing the actual trajectory that is calculated by following motion layer, the behavior layer cannot be certain about the validity of decisions. For example, the behavior planner can command lane change only when it is obvious that there is no obstacle in the target lane, and it is not possible to do \"aggressive\" lane change as an architecture.","title":"Cons"},{"location":"design/software_architecture/Planning/DesignRationale/#unified-behavior-and-motion","text":"This paper reviews the planning architecture used in the DARPA Urban Challenge and addresses the demerit of splitting behavior layer and motion layer. It also proposes an algorithm to handle decision making and trajectory optimization simultaneously.","title":"Unified Behavior and Motion"},{"location":"design/software_architecture/Planning/DesignRationale/#pros_1","text":"It can make more \"aggressive\" decisions compared to Boss type architecture.","title":"Pros"},{"location":"design/software_architecture/Planning/DesignRationale/#cons_1","text":"The paper focuses on lane change, but the real environment requires a variety of decision making, such as traffic lights and crosswalks. It is questionable that we can handle different kinds of traffic rules with one optimization algorithm. Also, since decision making is also the target of optimization, it is usually difficult to add user-specific constraints to decision making.","title":"Cons"},{"location":"design/software_architecture/Planning/DesignRationale/#victor-tango-type","text":"System overview of Victor Tango is explained in this paper . Victor Tango split Behavior and Motion layer like Boss type. However, unlike Boss type, there is feedback from motion whether a decision made by behavior layer is achievable or not.","title":"Victor Tango Type"},{"location":"design/software_architecture/Planning/DesignRationale/#pros_2","text":"It overcomes the weakness of Boss type and can consider trajectory at the behavior level.","title":"Pros"},{"location":"design/software_architecture/Planning/DesignRationale/#cons_2","text":"The interface between behavior and motion would be tight and dense. It has the risk of having heavy inter-dependency as new traffic rules are added, making it difficult to replace one of the modules with new algorithms in the future.","title":"Cons"},{"location":"design/software_architecture/Planning/DesignRationale/#apollo","text":"Here is the link . Apollo kept updating the planning module at each version update. In version 5.0, they have taken a different approach from others. Apollo split the behavior planner into scenarios, such as intersection, parking, and lane following. In each scenario module, it calls decider and optimizer libraries to achieve specific scenarios.","title":"Apollo"},{"location":"design/software_architecture/Planning/DesignRationale/#pros_3","text":"Specific parameter tunings are available for different scenarios, making it relatively easier to add new traffic rules for different countries. Different optimizers can be used for a different purpose.","title":"Pros"},{"location":"design/software_architecture/Planning/DesignRationale/#cons_3","text":"As the number of scenario increases, planning selector(or scenario selector) may get very complex as the number of scenario increases.","title":"Cons"},{"location":"design/software_architecture/Planning/DesignRationale/#autoware-planning-architecture","text":"Considering pros and cons of different approaches, we have concluded to take the hybrid approach of Apollo and Boss. We divided planning modules into scenarios just like Apollo, but into a more high-level scenario, such as LaneDriving and Parking. Apollo has smaller units of scenarios, such as intersection and traffic lights, but we expect that those scenarios may occur in parallel(e.g. intersection with traffic lights and crosswalks) and preparing combined-scenario modules would make scenario-selector too complex to be maintained. Instead, we made a scenario to be a more broad concept to keep the number of scenarios to be lower to keep scenario selector comprehensible. Currently, we only have LaneDriving and Parking, and we anticipate to have HighWay and InEmergency scenarios in the future. More investigation is required to establish the definition of a \u201cScenario\u201d, but the convention is that a new scenario must only be added whenever a different \"paradigm\" is needed for planning. For example, LaneDriving is used to drive along public roads, whereas Parking is used for driving free space, and it would be difficult to develop an algorithm to support requirements for both scenarios. LaneDriving wouldn't require complex path planner as the shape of lanes are already given from HD map, but it must be done at a higher frequency to drive at higher speed, whereas parking requires more complex maneuvers with cut-backs but with lower constraint about computation time. Therefore, it would make more sense to split them into different scenarios, and design a planner to support contradicting requirements. Note that we didn't split LaneDriving into smaller scenarios, unlike Apollo. We have considered all traffic rule related scenes on public roads, including yielding, traffic lights, crosswalks, and bare intersections, to be essentially velocity planning along lanes and can be handled within a single scheme.","title":"Autoware Planning Architecture"},{"location":"design/software_architecture/Planning/DesignRationale/#reference","text":"Boss: https://www.ri.cmu.edu/pub_files/pub4/urmson_christopher_2008_1/urmson_christopher_2008_1.pdf CMU Doctor Thesis: https://www.researchgate.net/publication/315067229_Improved_Trajectory_Planning_for_On-Road_Self-Driving_Vehicles_Via_Combined_Graph_Search_Optimization_Topology_Analysis Victor Tango: https://archive.darpa.mil/grandchallenge/TechPapers/Victor_Tango.pdf Apollo Auto: https://github.com/ApolloAuto/apollo/tree/r5.0.0/modules/planning","title":"Reference"},{"location":"design/software_architecture/Planning/Planning/","text":"Planning # Overview # Planning stack acts as the \u201cbrain\u201d of autonomous driving. It uses all the results from Localization, Perception, and Map stacks to decide its maneuver and gives final trajectory to Control stack. Role # These are high-level roles of Planning stack: Calculates route that navigates to desired goal Plans trajectory to follow the route Make sure that vehicle does not collide with obstacles, including pedestrians and other vehicles) Make sure that the vehicle follows traffic rules during the navigation. This includes following traffic light, stopping at stoplines, stopping at crosswalks, etc. Plan sequences of trajectories that is feasible for the vehicle. (e.g. no sharp turns that is kinematically impossible) Use Cases # Planning stack must satisfy following use cases: Navigate vehicle from start to goal Operating lane change Driving along lane Following speed limit of lane Follow traffic light Follow yield/stop signs Turning left/right at intersections Park vehicle at parking space (either reverse parking or forward first parking) Requirements # Planning route from start to goal (Use Case 1) Planning stack should be able to get starting lane and goal lane from given start pose and goal pose either in earth frame or map frame Planning stack should be able to calculate sequences of lanes that navigates vehicle from start lane to goal lane that minimizes cost function(either time based cost or distance based cost) Driving along lane (Use Case 2) Vehicle must drive between left boundary and right boundary of driving lane The vehicle must have at least 2 seconds margin between other vehicles so that it has enough distance to stop without collision. reference Operating lane change (Use Case 3) Vehicle must change lane when lane change is necessary to follow planned route If current driving lane is blocked (e.g. by parked vehicle) Vehicle must turn on appropriate turn signal 3 seconds before lane change and it must be turned on until lane change is finished Vehicle should stay in lane at least for 3 second before operating lane change for other participants to recognize ego vehicle's turn signal. there must be 2 seconds margin between any other vehicles during lane change lane change finishes 30m before any intersections vehicle should abort lane change when all of the following conditions are satisfied: Vehicle(base_link) is still in the original lane there is no longer 2 seconds margin between other n vehicles during lane change e.g. due to newly detected vehicles Follow speed limit of lane (Use Case 4) Speed profile of trajectory points in a lane must be below speed limit of the lane. Follow traffic light (Use Case 5) Planning stack should refer to Perception output of the traffic light associated to driving lane. Speed profile of a trajectory at the associated stopline must be zero when relevant traffic light is red and it has enough distance to stop before the stopline with given deceleration configuration Turning left/right at intersections (Use Case 6) Vehicle must stop before entering intersection whenever other vehicles are entering intersection unless ego vehicle has right of way Parking (Use Case 7) Vehicle must not hit other vehicle, curbs, or other obstacle during parking i.e. All points in planned trajectory has enough distance from other objects with ego vehicle's footprint taken into account General requirements to trajectory Planned trajectory must satisfy requirements from Control stack: Planned trajectory must have speed profile that satisfies given acceleration and jerk limits unless vehicle is under emergency e.g. when pedestrian suddenly jumps into driving lane or front vehicle suddenly stops. Planned trajectory must be feasible by the given vehicle kinematic model Planned trajectory must satisfy given lateral acceleration and jerk limit Planned trajectory points within n [m] from ego vehicle should not change over time unless sudden steering or sudden acceleration is required to avoid collision with other vehicles. n [m] = velocity_of_ego_vehicle * configured_time_horizon Input # The table below summarizes the overall input into Planning stack: Input Topic Name(Data Type) Explanation Vehicle Pose /tf (map->base_link) ( tf::tfMessage ) Planning requires vehicle pose in map frame, which is the frame where all planning takes place. Vehicle Velocity /localization/twist ( geometry_msgs::Twist ) This includes vehicle's velocity information. It is used to predict future pose on trajectory to detect collision with other objects. Map data /map/vector_map ( autoware_lanelet2_msgs::LaneletMapBin ) This includes all static information about the environment, such as: Lane connection information used for route planning from starting position to goal position Lane geometry to generate reference path used to calculate trajectory All information related to traffic rules Detected Obstacle Information /perception/object_recognition/objects ( autoware_planning_msgs::DynamicObjectsArray ) This includes information that cannot be known beforehand such as pedestrians and other vehicles. Planning stack will plan maneuvers to avoid collision with such objects. Goal position /planning/goal_pose ( geometry_msgs::PoseStamped ) This is the final pose that Planning stack will try to achieve. TrafficLight recognition result /perception/traffic_light_recognition/traffic_light_states ( autoware_traffic_light_msgs::TrafficLightStateArray ) This is the real time information about the state of each traffic light. Planning stack will extract the one that is relevant to planned path and use it to decide whether to stop at intersections. Output # The table below summarizes the final output from Planning stack: Output Topic(Data Type) Explanation Trajectory /planning/trajectory ( autoware_planning_msgs::Trajectory ) This is the sequence of pose that Control stack must follow. This must be smooth, and kinematically possible to follow by the Control stack. Turn Signal /vehicle/turn_signal_cmd ( autoware_vehicle_msgs::TurnSignal ) This is the output to control turn signals of the vehicle. Planning stack will make sure that turn signal will be turned on according to planned maneuver. Design # In order to achieve the requirements stated above, Planning stack is decomposed into the diagram below. Each requirement is met in following modules: Requirement 1: Mission calculates the overall route to reach goal from starting position Requirement 2-7: LaneDriving scenario plans trajectory along lanes in planned route Requirement 8: Parking scenario plans trajectory in free space to park into parking space Requirement 9: Both LaneDriving and Parking should output trajectory that satisfies the requirement We have looked into different autonomous driving stacks and concluded that it is technically difficult to use unified planner to handle every possible situation. (See here for more details). Therefore, we have decided to set different planners in parallel dedicated for each use case, and let scenario selector to decide depending on situations. Currently, we have reference implementation with two scenarios, on-road planner and parking planner, but any scenarios (e.g. highway, in-emergency, etc.) can be added as needed. It may be controversial whether new scenario is needed or existing scenario should be enhanced when adding new feature, and we still need more investigation to set the definition of \u201cScenario\u201d module. Scenarios # Role # The role of Scenario module is to calculate trajectory message from route message. It should only plan when the module is selected by the scenario selector module. This is where all behavior planning is done. Input # Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic. Output # Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the use cases for the scenario module. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack. Reference Implementation # The reference implementation of the planning module in the latest version is shown as below. For more details, please refer to the design documents in each package. mission_planner : calculate route from start to goal based on the map information. behavior_path_planner : calculates path and drivable area based on the traffic rules. lane_following lane_change avoidance side_shift behavior_velocity_planner : calculates max speed based on the traffic rules. detection_area blind_spot cross_walk stop_line traffic_light intersection occlusion_spot obstacle_avoidance_planner : calculate path shape under obstacle and drivable area constraints surround_obstacle_checker : keeps the vehicle being stopped when there are obstacles around the ego-vehicle. It works only when the vehicle is stopped. obstacle_stop_planner : (NOTE: link is temporal) When there are obstacles on or near the trajectory, it calculates the maximum velocity of the trajectory points depending on the situation: stopping, slowing down, or adaptive cruise (following the car). stop slow_down adaptive_cruise costmap_generator : generates a costmap for path generation from dynamic objects and lane information. freespace_planner : calculates trajectory considering the feasibility (e.g. curvature) for the freespace scene. scenario_selector : chooses a trajectory according to the current scenario. motion_velocity_smoother : calculates final velocity considering velocity, acceleration, and jerk constraints.","title":"Overall"},{"location":"design/software_architecture/Planning/Planning/#planning","text":"","title":"Planning"},{"location":"design/software_architecture/Planning/Planning/#overview","text":"Planning stack acts as the \u201cbrain\u201d of autonomous driving. It uses all the results from Localization, Perception, and Map stacks to decide its maneuver and gives final trajectory to Control stack.","title":"Overview"},{"location":"design/software_architecture/Planning/Planning/#role","text":"These are high-level roles of Planning stack: Calculates route that navigates to desired goal Plans trajectory to follow the route Make sure that vehicle does not collide with obstacles, including pedestrians and other vehicles) Make sure that the vehicle follows traffic rules during the navigation. This includes following traffic light, stopping at stoplines, stopping at crosswalks, etc. Plan sequences of trajectories that is feasible for the vehicle. (e.g. no sharp turns that is kinematically impossible)","title":"Role"},{"location":"design/software_architecture/Planning/Planning/#use-cases","text":"Planning stack must satisfy following use cases: Navigate vehicle from start to goal Operating lane change Driving along lane Following speed limit of lane Follow traffic light Follow yield/stop signs Turning left/right at intersections Park vehicle at parking space (either reverse parking or forward first parking)","title":"Use Cases"},{"location":"design/software_architecture/Planning/Planning/#requirements","text":"Planning route from start to goal (Use Case 1) Planning stack should be able to get starting lane and goal lane from given start pose and goal pose either in earth frame or map frame Planning stack should be able to calculate sequences of lanes that navigates vehicle from start lane to goal lane that minimizes cost function(either time based cost or distance based cost) Driving along lane (Use Case 2) Vehicle must drive between left boundary and right boundary of driving lane The vehicle must have at least 2 seconds margin between other vehicles so that it has enough distance to stop without collision. reference Operating lane change (Use Case 3) Vehicle must change lane when lane change is necessary to follow planned route If current driving lane is blocked (e.g. by parked vehicle) Vehicle must turn on appropriate turn signal 3 seconds before lane change and it must be turned on until lane change is finished Vehicle should stay in lane at least for 3 second before operating lane change for other participants to recognize ego vehicle's turn signal. there must be 2 seconds margin between any other vehicles during lane change lane change finishes 30m before any intersections vehicle should abort lane change when all of the following conditions are satisfied: Vehicle(base_link) is still in the original lane there is no longer 2 seconds margin between other n vehicles during lane change e.g. due to newly detected vehicles Follow speed limit of lane (Use Case 4) Speed profile of trajectory points in a lane must be below speed limit of the lane. Follow traffic light (Use Case 5) Planning stack should refer to Perception output of the traffic light associated to driving lane. Speed profile of a trajectory at the associated stopline must be zero when relevant traffic light is red and it has enough distance to stop before the stopline with given deceleration configuration Turning left/right at intersections (Use Case 6) Vehicle must stop before entering intersection whenever other vehicles are entering intersection unless ego vehicle has right of way Parking (Use Case 7) Vehicle must not hit other vehicle, curbs, or other obstacle during parking i.e. All points in planned trajectory has enough distance from other objects with ego vehicle's footprint taken into account General requirements to trajectory Planned trajectory must satisfy requirements from Control stack: Planned trajectory must have speed profile that satisfies given acceleration and jerk limits unless vehicle is under emergency e.g. when pedestrian suddenly jumps into driving lane or front vehicle suddenly stops. Planned trajectory must be feasible by the given vehicle kinematic model Planned trajectory must satisfy given lateral acceleration and jerk limit Planned trajectory points within n [m] from ego vehicle should not change over time unless sudden steering or sudden acceleration is required to avoid collision with other vehicles. n [m] = velocity_of_ego_vehicle * configured_time_horizon","title":"Requirements"},{"location":"design/software_architecture/Planning/Planning/#input","text":"The table below summarizes the overall input into Planning stack: Input Topic Name(Data Type) Explanation Vehicle Pose /tf (map->base_link) ( tf::tfMessage ) Planning requires vehicle pose in map frame, which is the frame where all planning takes place. Vehicle Velocity /localization/twist ( geometry_msgs::Twist ) This includes vehicle's velocity information. It is used to predict future pose on trajectory to detect collision with other objects. Map data /map/vector_map ( autoware_lanelet2_msgs::LaneletMapBin ) This includes all static information about the environment, such as: Lane connection information used for route planning from starting position to goal position Lane geometry to generate reference path used to calculate trajectory All information related to traffic rules Detected Obstacle Information /perception/object_recognition/objects ( autoware_planning_msgs::DynamicObjectsArray ) This includes information that cannot be known beforehand such as pedestrians and other vehicles. Planning stack will plan maneuvers to avoid collision with such objects. Goal position /planning/goal_pose ( geometry_msgs::PoseStamped ) This is the final pose that Planning stack will try to achieve. TrafficLight recognition result /perception/traffic_light_recognition/traffic_light_states ( autoware_traffic_light_msgs::TrafficLightStateArray ) This is the real time information about the state of each traffic light. Planning stack will extract the one that is relevant to planned path and use it to decide whether to stop at intersections.","title":"Input"},{"location":"design/software_architecture/Planning/Planning/#output","text":"The table below summarizes the final output from Planning stack: Output Topic(Data Type) Explanation Trajectory /planning/trajectory ( autoware_planning_msgs::Trajectory ) This is the sequence of pose that Control stack must follow. This must be smooth, and kinematically possible to follow by the Control stack. Turn Signal /vehicle/turn_signal_cmd ( autoware_vehicle_msgs::TurnSignal ) This is the output to control turn signals of the vehicle. Planning stack will make sure that turn signal will be turned on according to planned maneuver.","title":"Output"},{"location":"design/software_architecture/Planning/Planning/#design","text":"In order to achieve the requirements stated above, Planning stack is decomposed into the diagram below. Each requirement is met in following modules: Requirement 1: Mission calculates the overall route to reach goal from starting position Requirement 2-7: LaneDriving scenario plans trajectory along lanes in planned route Requirement 8: Parking scenario plans trajectory in free space to park into parking space Requirement 9: Both LaneDriving and Parking should output trajectory that satisfies the requirement We have looked into different autonomous driving stacks and concluded that it is technically difficult to use unified planner to handle every possible situation. (See here for more details). Therefore, we have decided to set different planners in parallel dedicated for each use case, and let scenario selector to decide depending on situations. Currently, we have reference implementation with two scenarios, on-road planner and parking planner, but any scenarios (e.g. highway, in-emergency, etc.) can be added as needed. It may be controversial whether new scenario is needed or existing scenario should be enhanced when adding new feature, and we still need more investigation to set the definition of \u201cScenario\u201d module.","title":"Design"},{"location":"design/software_architecture/Planning/Planning/#scenarios","text":"","title":"Scenarios"},{"location":"design/software_architecture/Planning/Planning/#role_1","text":"The role of Scenario module is to calculate trajectory message from route message. It should only plan when the module is selected by the scenario selector module. This is where all behavior planning is done.","title":"Role"},{"location":"design/software_architecture/Planning/Planning/#input_1","text":"Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic.","title":"Input"},{"location":"design/software_architecture/Planning/Planning/#output_1","text":"Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the use cases for the scenario module. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack.","title":"Output"},{"location":"design/software_architecture/Planning/Planning/#reference-implementation","text":"The reference implementation of the planning module in the latest version is shown as below. For more details, please refer to the design documents in each package. mission_planner : calculate route from start to goal based on the map information. behavior_path_planner : calculates path and drivable area based on the traffic rules. lane_following lane_change avoidance side_shift behavior_velocity_planner : calculates max speed based on the traffic rules. detection_area blind_spot cross_walk stop_line traffic_light intersection occlusion_spot obstacle_avoidance_planner : calculate path shape under obstacle and drivable area constraints surround_obstacle_checker : keeps the vehicle being stopped when there are obstacles around the ego-vehicle. It works only when the vehicle is stopped. obstacle_stop_planner : (NOTE: link is temporal) When there are obstacles on or near the trajectory, it calculates the maximum velocity of the trajectory points depending on the situation: stopping, slowing down, or adaptive cruise (following the car). stop slow_down adaptive_cruise costmap_generator : generates a costmap for path generation from dynamic objects and lane information. freespace_planner : calculates trajectory considering the feasibility (e.g. curvature) for the freespace scene. scenario_selector : chooses a trajectory according to the current scenario. motion_velocity_smoother : calculates final velocity considering velocity, acceleration, and jerk constraints.","title":"Reference Implementation"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/","text":"Lane Driving Scenario # Use Cases and Requirements # Lane Driving Scenario must satisfy the following use cases: Driving along lane Operating lane change Following speed limit of lane Follow traffic light Turning left/right at intersections For the details about related requirements, please refer to the document for Planning stack . Input # Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic. Output # Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the use cases for the scenario module. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack. Design # Lane Driving scenario is decomposed into following modules: LaneChangePlanner, BehaviorVelocityPlanner, MotionPlanner and TurnSignalDecider. Behavior Planner # Behavior Planner plans the path, which includes reference trajectory(i.e. path points) for motion planner to optimize and drivable area. General idea is that behavior layer sets constraints according to traffic rules to ensure optimized trajectory follows traffic rules. It is decomposed into: LaneChangePlanner that decides lane change BehaviorVelocityPlanner that plans the velocity profile according to traffic rules TurnSignalDecider that decides turn signals according to planned behavior Input # Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic. Output # Path: autoware_planning_msgs::Path This message contains path points, which are reference points that will be optimized by motion planner, and drivable area which indicates the space in which motion planner is allowed to change position of path points during optimization. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack. MotionPlanner # Motion planner is responsible for following functions: Optimize the shape of trajectory with given lateral acceleration and jerk limit Motion planner should not manipulate position of trajectory points from behavior planner outside given drivable area Optimize the velocity of trajectory with given acceleration and jerk limit Motion planner is only allowed to decrease the speed profile given from behavior planner since all traffic rules(such as speed limits) are considered in behavior planner. Interpolate trajectory points with enough resolution for Control Input # Path: autoware_planning_msgs::Path This contains reference trajectory points, speed constraints, and geometrical constraints for optimization. Output # Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy given acceleration and jerk limits.","title":"Overall"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#lane-driving-scenario","text":"","title":"Lane Driving Scenario"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#use-cases-and-requirements","text":"Lane Driving Scenario must satisfy the following use cases: Driving along lane Operating lane change Following speed limit of lane Follow traffic light Turning left/right at intersections For the details about related requirements, please refer to the document for Planning stack .","title":"Use Cases and Requirements"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#input","text":"Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic.","title":"Input"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#output","text":"Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the use cases for the scenario module. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack.","title":"Output"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#design","text":"Lane Driving scenario is decomposed into following modules: LaneChangePlanner, BehaviorVelocityPlanner, MotionPlanner and TurnSignalDecider.","title":"Design"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#behavior-planner","text":"Behavior Planner plans the path, which includes reference trajectory(i.e. path points) for motion planner to optimize and drivable area. General idea is that behavior layer sets constraints according to traffic rules to ensure optimized trajectory follows traffic rules. It is decomposed into: LaneChangePlanner that decides lane change BehaviorVelocityPlanner that plans the velocity profile according to traffic rules TurnSignalDecider that decides turn signals according to planned behavior","title":"Behavior Planner"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#input_1","text":"Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment, including lane connection, lane geometry, and traffic rules. Scenario module should plan trajectory such that vehicle follows all traffic rules specified in map. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. Scenario modules only run when the module is selected by this topic.","title":"Input"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#output_1","text":"Path: autoware_planning_msgs::Path This message contains path points, which are reference points that will be optimized by motion planner, and drivable area which indicates the space in which motion planner is allowed to change position of path points during optimization. Turn Signal: autoware_vehicle_msgs::TurnSignal Turn signal command should also be published because Scenario module is only aware of the traffic rules and operating maneuvers in the whole Autoware stack.","title":"Output"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#motionplanner","text":"Motion planner is responsible for following functions: Optimize the shape of trajectory with given lateral acceleration and jerk limit Motion planner should not manipulate position of trajectory points from behavior planner outside given drivable area Optimize the velocity of trajectory with given acceleration and jerk limit Motion planner is only allowed to decrease the speed profile given from behavior planner since all traffic rules(such as speed limits) are considered in behavior planner. Interpolate trajectory points with enough resolution for Control","title":"MotionPlanner"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#input_2","text":"Path: autoware_planning_msgs::Path This contains reference trajectory points, speed constraints, and geometrical constraints for optimization.","title":"Input"},{"location":"design/software_architecture/Planning/LaneDriving/LaneDrivingScenario/#output_2","text":"Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy given acceleration and jerk limits.","title":"Output"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/","text":"Overview # Behavior velocity planner is responsible for modifying velocity so that ego vehicle drives according to traffic rules. Inputs # topic name: \"path_with_lane_id\" type: autoware_planning_msgs::PathWithLaneId frequency: 10Hz Outputs # Behavior velocity planner should output path with velocity profile using following message type. topic name: \"path\" type: autoware_planning_msgs::Path frequency: 10Hz Use Cases # Behavior planner is responsible for handling different traffic rules present in the environment. Note that traffic rules might be different depending on country. Therefore, behavior velocity planner node must be designed to allow developers to easily modify the traffic rules. Currently the following traffic rules are considered in the reference implementation: crosswalk intersection stop line traffic light occlusion spot Requirements # Again, traffic rules might be different depending on country, and the requirement described here is for Japanese traffic rules. Crosswalk Ego vehicle must stop if a pedestrian is within stop area shown in the image. Ego vehicle must stop if a pedestrian is expected to enter the stop area. (e.g. within 3 seconds) Ego vehicle must slow down (e.g. to 5km/h) when there is a pedestrian in the deceleration area shown in the image. rationale: We don't want to make vehicle keep waiting for pedestrian to pass crosswalk completely. We want vehicle to start driving when pedestrian walks past the car. Intersection Unless ego vehicle has right of way, Vehicle must stop before entering intersection whenever other vehicles are entering intersection. Stop Line (Stop Sign) Vehicle must stop at stopline when it passes through the line. Stop line should be specified in the Semantic Map. Follow traffic light Planning stack should refer to Perception output of the traffic light associated to driving lane. Speed profile of a trajectory at the associated stopline must be zero when relevant traffic light is red and it has enough distance to stop before the stopline with given deceleration configuration Occlusion spot Vehicle must slow down before possible collision for occlusion spots. Design # The example node diagram of the behavior velocity planner is shown as the diagram below. The node consist of two steps. Module Instance Update From the given path, the planner will look up the HD Map to see if there are upcoming traffic rule features in the map. For each detected traffic rule features, respective scene module instance will be generated. Also, scene module instances from past features will be deleted. Velocity Update Actual modification of the velocity profile of the path happens in this step. Path message will be passed to each traffic rule scene modules, and each module will update velocity according to its traffic rule. Minimum value of all scene modules will become the final velocity profile. Adding/Modifying Traffic Rules # The actual rule for modifying velocity of the path is defined in scene modules. Whenever a developer want to add new rules to the planner, the developer must define a new traffic rule scene module class which inherits SceneModuleInterface class. For the details, please refer to the source code .","title":"Behavior velocity planning"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#overview","text":"Behavior velocity planner is responsible for modifying velocity so that ego vehicle drives according to traffic rules.","title":"Overview"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#inputs","text":"topic name: \"path_with_lane_id\" type: autoware_planning_msgs::PathWithLaneId frequency: 10Hz","title":"Inputs"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#outputs","text":"Behavior velocity planner should output path with velocity profile using following message type. topic name: \"path\" type: autoware_planning_msgs::Path frequency: 10Hz","title":"Outputs"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#use-cases","text":"Behavior planner is responsible for handling different traffic rules present in the environment. Note that traffic rules might be different depending on country. Therefore, behavior velocity planner node must be designed to allow developers to easily modify the traffic rules. Currently the following traffic rules are considered in the reference implementation: crosswalk intersection stop line traffic light occlusion spot","title":"Use Cases"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#requirements","text":"Again, traffic rules might be different depending on country, and the requirement described here is for Japanese traffic rules. Crosswalk Ego vehicle must stop if a pedestrian is within stop area shown in the image. Ego vehicle must stop if a pedestrian is expected to enter the stop area. (e.g. within 3 seconds) Ego vehicle must slow down (e.g. to 5km/h) when there is a pedestrian in the deceleration area shown in the image. rationale: We don't want to make vehicle keep waiting for pedestrian to pass crosswalk completely. We want vehicle to start driving when pedestrian walks past the car. Intersection Unless ego vehicle has right of way, Vehicle must stop before entering intersection whenever other vehicles are entering intersection. Stop Line (Stop Sign) Vehicle must stop at stopline when it passes through the line. Stop line should be specified in the Semantic Map. Follow traffic light Planning stack should refer to Perception output of the traffic light associated to driving lane. Speed profile of a trajectory at the associated stopline must be zero when relevant traffic light is red and it has enough distance to stop before the stopline with given deceleration configuration Occlusion spot Vehicle must slow down before possible collision for occlusion spots.","title":"Requirements"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#design","text":"The example node diagram of the behavior velocity planner is shown as the diagram below. The node consist of two steps. Module Instance Update From the given path, the planner will look up the HD Map to see if there are upcoming traffic rule features in the map. For each detected traffic rule features, respective scene module instance will be generated. Also, scene module instances from past features will be deleted. Velocity Update Actual modification of the velocity profile of the path happens in this step. Path message will be passed to each traffic rule scene modules, and each module will update velocity according to its traffic rule. Minimum value of all scene modules will become the final velocity profile.","title":"Design"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/BehaviorVelocityPlanner/#addingmodifying-traffic-rules","text":"The actual rule for modifying velocity of the path is defined in scene modules. Whenever a developer want to add new rules to the planner, the developer must define a new traffic rule scene module class which inherits SceneModuleInterface class. For the details, please refer to the source code .","title":"Adding/Modifying Traffic Rules"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/","text":"Overview # Lane Change Planner should create path from Route message, and navigates to given goal by either following lane or changing lanes. Inputs # Input : topic \"route\" with type autoware_planning_msgs::Route Outputs # topic name: \"path_with_lane_id\" type: autoware_planning_msgs::PathWithLaneId frequency: 10Hz Assumptions # Vehicle travels at constant speed Requirements # About goal pose # If the given goal pose in route message is within route, then goal pose is used directly If the given goal pose in route message is not within route (e.g. in parking space), then the goal shall be replaced by closest point along the centerline of goal lane About Drivable Area # drivable area should be the shape of lanes that vehicle is driving when BPP is following lane drivable area should be the shape of current lane and lane change target lanes when BPP is operating lane change About Path Points # LaneChangePlanner should publish reference path that leads to goal pose Path should start from n [m] behind vehicle position Path should have length of at least 100[m] unless path surpasses goal All points in Path should be placed within drivable area Path within the n [m] away from vehicle should not change over time to avoid sudden change in steering. About Lane Change # Vehicle follows lane if vehicle is on preferred lanes Vehicle should stay in lane at least for 3 second before operating lane change for other participants to recognize ego vehicle's turn signal. The planner attempts lane change towards preferred lane if vehicle is not within preferred lanes, and candidate lane change path is valid. The path is valid if it satisfies all the following conditions: there is 2seconds margin between any other vehicles assuming that ego vehicle follows the candidate path at constant speed lane change finishes x [m] before any intersections lane change finishes x [m] before any crosswalks LaneChangePlanner shall abort lane change and go back to original lane when all of the following conditions are satisfied: Vehicle(base_link) is still in the original lane there is no n seconds margin between all other vehicles during lane change, assuming that ego vehicle follows the candidate path at constant speed. (due to newly detected vehicles)","title":"Behavior path planning"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#overview","text":"Lane Change Planner should create path from Route message, and navigates to given goal by either following lane or changing lanes.","title":"Overview"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#inputs","text":"Input : topic \"route\" with type autoware_planning_msgs::Route","title":"Inputs"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#outputs","text":"topic name: \"path_with_lane_id\" type: autoware_planning_msgs::PathWithLaneId frequency: 10Hz","title":"Outputs"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#assumptions","text":"Vehicle travels at constant speed","title":"Assumptions"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#requirements","text":"","title":"Requirements"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#about-goal-pose","text":"If the given goal pose in route message is within route, then goal pose is used directly If the given goal pose in route message is not within route (e.g. in parking space), then the goal shall be replaced by closest point along the centerline of goal lane","title":"About goal pose"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#about-drivable-area","text":"drivable area should be the shape of lanes that vehicle is driving when BPP is following lane drivable area should be the shape of current lane and lane change target lanes when BPP is operating lane change","title":"About Drivable Area"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#about-path-points","text":"LaneChangePlanner should publish reference path that leads to goal pose Path should start from n [m] behind vehicle position Path should have length of at least 100[m] unless path surpasses goal All points in Path should be placed within drivable area Path within the n [m] away from vehicle should not change over time to avoid sudden change in steering.","title":"About Path Points"},{"location":"design/software_architecture/Planning/LaneDriving/Behavior/LaneChangePlanner/#about-lane-change","text":"Vehicle follows lane if vehicle is on preferred lanes Vehicle should stay in lane at least for 3 second before operating lane change for other participants to recognize ego vehicle's turn signal. The planner attempts lane change towards preferred lane if vehicle is not within preferred lanes, and candidate lane change path is valid. The path is valid if it satisfies all the following conditions: there is 2seconds margin between any other vehicles assuming that ego vehicle follows the candidate path at constant speed lane change finishes x [m] before any intersections lane change finishes x [m] before any crosswalks LaneChangePlanner shall abort lane change and go back to original lane when all of the following conditions are satisfied: Vehicle(base_link) is still in the original lane there is no n seconds margin between all other vehicles during lane change, assuming that ego vehicle follows the candidate path at constant speed. (due to newly detected vehicles)","title":"About Lane Change"},{"location":"design/software_architecture/Planning/MissionPlanner/MissionPlanner/","text":"Mission planner # Role # The role of mission planner is to calculate route that navigates from current vehicle pose to goal pose. The route is made of sequence of lanes that vehicle must follow to reach goal pose. This module is responsible for calculating full route to goal, and therefore only use static map information. Any dynamic obstacle information (e.g. pedestrians and vehicles) is not considered during route planning. Therefore, output route topic is only published when goal pose is given and will be latched until next goal is provided. remark : Dynamic map information, such as road construction blocking some lanes, may be considered in the future. However, this feature becomes more reasonable unless we have multiple vehicle, where each vehicle updates map online and share it with other vehicles. Therefore, we only consider static map information for now. Input # current pose: /tf (map->base_link): This is current pose in map frame calculated by Localization stack. goal pose: geometry_msgs::PoseStamped This is goal pose given from the Operator/Fleet Management Software map: autoware_lanelet_msgs::MapBin This is binary data of map from Map stack. This should include geometry information of each lane to match input start/goal pose to corresponding lane, and lane connection information to calculate sequence of lanes to reach goal lane. Output # route: autoware_planning_msgs::Route Message type is described below. Route is made of sequence of route section that vehicle must follow in order to reach goal, where a route section is a \u201cslice\u201d of a road that bundles lane changeable lanes. Note that the most atomic unit of route is lane_id, which is the unique id of a lane in vector map. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure.","title":"Mission planner"},{"location":"design/software_architecture/Planning/MissionPlanner/MissionPlanner/#mission-planner","text":"","title":"Mission planner"},{"location":"design/software_architecture/Planning/MissionPlanner/MissionPlanner/#role","text":"The role of mission planner is to calculate route that navigates from current vehicle pose to goal pose. The route is made of sequence of lanes that vehicle must follow to reach goal pose. This module is responsible for calculating full route to goal, and therefore only use static map information. Any dynamic obstacle information (e.g. pedestrians and vehicles) is not considered during route planning. Therefore, output route topic is only published when goal pose is given and will be latched until next goal is provided. remark : Dynamic map information, such as road construction blocking some lanes, may be considered in the future. However, this feature becomes more reasonable unless we have multiple vehicle, where each vehicle updates map online and share it with other vehicles. Therefore, we only consider static map information for now.","title":"Role"},{"location":"design/software_architecture/Planning/MissionPlanner/MissionPlanner/#input","text":"current pose: /tf (map->base_link): This is current pose in map frame calculated by Localization stack. goal pose: geometry_msgs::PoseStamped This is goal pose given from the Operator/Fleet Management Software map: autoware_lanelet_msgs::MapBin This is binary data of map from Map stack. This should include geometry information of each lane to match input start/goal pose to corresponding lane, and lane connection information to calculate sequence of lanes to reach goal lane.","title":"Input"},{"location":"design/software_architecture/Planning/MissionPlanner/MissionPlanner/#output","text":"route: autoware_planning_msgs::Route Message type is described below. Route is made of sequence of route section that vehicle must follow in order to reach goal, where a route section is a \u201cslice\u201d of a road that bundles lane changeable lanes. Note that the most atomic unit of route is lane_id, which is the unique id of a lane in vector map. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure.","title":"Output"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/","text":"Parking Scenario # This scenario is meant to be used to plan maneuvers to park vehicle in parking space. Compared to LaneDrivingScenario, this scenario has relative fewer constraints about the shape of trajectory. Requirements # Lane Driving Scenario must satisfy the following use cases: Park vehicle in parking space For the details about related requirements, please refer to the document for Planning stack . Input # Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning, but only goal pose is used for planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment. This is meant to be used to generate drivable area. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. All modules only run when Parking scenario is selected by this topic. Outputs # Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the requirements. Design # Costmap Generator # This gives spacial constraints to freespace planner. Input # Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment. This is meant to be used to generate drivable area. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. All modules only run when Parking scenario is selected by this topic. Output # Costmap: nav_msgs::OccupancyGrid.msg This contains spaces that can be used for trajectory planning. The grid is considered not passable(occupied) if it is outside of parking lot polygon specified map, and perceived objects lie on the grid. Freespace Planner # Freespace planner calculates trajectory that navigates vehicle to goal pose given by route. It must consider vehicle's kinematic model. Input # Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning, but only goal pose is used for planning. Vehicle Pose: tf_msgs::tf Vehicle Velocity: geometry_msgs::Twist Output # Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the requirements.","title":"Parking"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#parking-scenario","text":"This scenario is meant to be used to plan maneuvers to park vehicle in parking space. Compared to LaneDrivingScenario, this scenario has relative fewer constraints about the shape of trajectory.","title":"Parking Scenario"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#requirements","text":"Lane Driving Scenario must satisfy the following use cases: Park vehicle in parking space For the details about related requirements, please refer to the document for Planning stack .","title":"Requirements"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#input","text":"Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning, but only goal pose is used for planning. Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment. This is meant to be used to generate drivable area. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. All modules only run when Parking scenario is selected by this topic.","title":"Input"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#outputs","text":"Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the requirements.","title":"Outputs"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#design","text":"","title":"Design"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#costmap-generator","text":"This gives spacial constraints to freespace planner.","title":"Costmap Generator"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#input_1","text":"Map: autoware_lanelet_msgs::MapBin This provides all static information about the environment. This is meant to be used to generate drivable area. Dynamic Objects: autoware_perception_msgs::DynamicObjectArray This provides all obstacle information calculated from sensors. Scenario module should calculate trajectory such that vehicle does not collide with other objects. This can be either done by planning velocity so that it stops before hitting obstacle, or by calculate path so that vehicle avoids the obstacle. Scenario: autoware_planning_msgs::Scenario This is the message from scenario selector. All modules only run when Parking scenario is selected by this topic.","title":"Input"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#output","text":"Costmap: nav_msgs::OccupancyGrid.msg This contains spaces that can be used for trajectory planning. The grid is considered not passable(occupied) if it is outside of parking lot polygon specified map, and perceived objects lie on the grid.","title":"Output"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#freespace-planner","text":"Freespace planner calculates trajectory that navigates vehicle to goal pose given by route. It must consider vehicle's kinematic model.","title":"Freespace Planner"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#input_2","text":"Route: autoware_planning_msgs::Route This includes the final goal pose and which lanes are available for trajectory planning, but only goal pose is used for planning. Vehicle Pose: tf_msgs::tf Vehicle Velocity: geometry_msgs::Twist","title":"Input"},{"location":"design/software_architecture/Planning/Parking/ParkingScenario/#output_1","text":"Trajectory: autoware_planning_msgs::Trajectory This contains trajectory that Control must follow. The shape and velocity of the trajectory must satisfy all the requirements.","title":"Output"},{"location":"design/software_architecture/Planning/ScenarioSelector/ScenarioSelector/","text":"Scenario Selector # Role # The role of scenario selector is to select appropriate scenario planner depending on situation. For example, if current pose is within road, then scenario selector should choose on-road planner, and if vehicle is within parking lot, then scenario selector should choose parking scenario. Note that all trajectory calculated by each scenario module passes is collected by scenario selector, and scenario selector chooses which trajectory to be passed down to Control module. This ensures that trajectory from unselected scenario is not passed down to Control when scenario is changed even if there is a delay when scenario planner receives notification that it is unselected by the scenario selector. Input # map: autoware_lanelet_msgs::MapBin vehicle pose: /tf (map->base_link) route: autoware_planning_msgs::Route Scenario planner uses above three topics to decide which scenario to use. In general it should decide scenarios based on where in the map vehicle is located(map+vehicle pose) and where it is trying to go(route). trajectory: autoware_planning_msgs::Trajectory Scenario planner gets the output from all the scenarios and passes the trajectory from selected scenario down to following stacks. This must be done within scenario_selector module in order to sync with the timing of scenario changing. Output # scenario: autoware_planning_msgs::Scenario This contains current available scenario and selected scenario. Each Scenario modules read this topic and chooses to plan trajectory Trajectory: autoware_planning_msgs::Trajectory This is the final trajectory of Planning stack, which is the trajectory from selected Scenario module.","title":"Scenario selector"},{"location":"design/software_architecture/Planning/ScenarioSelector/ScenarioSelector/#scenario-selector","text":"","title":"Scenario Selector"},{"location":"design/software_architecture/Planning/ScenarioSelector/ScenarioSelector/#role","text":"The role of scenario selector is to select appropriate scenario planner depending on situation. For example, if current pose is within road, then scenario selector should choose on-road planner, and if vehicle is within parking lot, then scenario selector should choose parking scenario. Note that all trajectory calculated by each scenario module passes is collected by scenario selector, and scenario selector chooses which trajectory to be passed down to Control module. This ensures that trajectory from unselected scenario is not passed down to Control when scenario is changed even if there is a delay when scenario planner receives notification that it is unselected by the scenario selector.","title":"Role"},{"location":"design/software_architecture/Planning/ScenarioSelector/ScenarioSelector/#input","text":"map: autoware_lanelet_msgs::MapBin vehicle pose: /tf (map->base_link) route: autoware_planning_msgs::Route Scenario planner uses above three topics to decide which scenario to use. In general it should decide scenarios based on where in the map vehicle is located(map+vehicle pose) and where it is trying to go(route). trajectory: autoware_planning_msgs::Trajectory Scenario planner gets the output from all the scenarios and passes the trajectory from selected scenario down to following stacks. This must be done within scenario_selector module in order to sync with the timing of scenario changing.","title":"Input"},{"location":"design/software_architecture/Planning/ScenarioSelector/ScenarioSelector/#output","text":"scenario: autoware_planning_msgs::Scenario This contains current available scenario and selected scenario. Each Scenario modules read this topic and chooses to plan trajectory Trajectory: autoware_planning_msgs::Trajectory This is the final trajectory of Planning stack, which is the trajectory from selected Scenario module.","title":"Output"},{"location":"design/software_architecture/Sensing/Sensing/","text":"Sensing # Overview # For autonomous driving, a vehicle needs to be aware of its state and surrounding environment. Sensing stack collects the environment information through various sensors and manipulates data appropriately to be used by other stacks. Role # There are two main roles of Sensing stack: Abstraction of sensor data to ROS message Sensing stack unifies the output format of same type of sensors so that following stacks (e.g. Perception) do not have to be aware of the hardware. Preprocessing of sensor data Raw data from sensors usually contains errors/noise due to hardware limitations. Sensing stack is responsible for removing such inaccuracies as much as possible before distributing sensor outputs to following stacks. Sensing stack may also do extra restructuring/formatting of data for so that there will be no duplicate data preprocessing done in different stacks. Use Cases # The use cases of Sensing stack are the followings: Estimating pose of the vehicle (Localization) Recognizing surrounding objects (Perception) Traffic light recognition (Perception) Recording log data Requirements # For the use cases 1-3mentioned above, the actual computation is done in either Localization stack and Perception stack, but the data used for computation must be provided by Sensing stack. Therefore, requirement of the Sensing stack is: Sensing stack should provide enough information for selected Localization method. e.g. LiDAR pointcloud, GNSS, and IMU (Use Case 1) Sensing stack should provide enough information for object recognition. e.g. LiDAR pointcloud and camera image (Use Case 2) Sensing stack should provide enough information for traffic light recognition. e.g. camera image (Use Case 3) Sensing stack should convert all sensor specific data into ROS message for logging. (Use Case 4) In order to abstract sensor specification from other stacks, Sensing stack should provide sensor data as ROS message specified described in Output section. Since the architecture of Localization stack and Perception stack leaves choices of using different combinations of algorithms, Autoware does not set any requirements about input sensor configurations. It is the user's responsibility to come up with appropriate sensor configuration to achieve the user's use cases. As a reference, the recommended sensors from past experience with Autoware.AI is listed below: LiDAR It should cover 360 FOV with minimal blind spots Camera It should cover 360 FOV with minimal blind spots. It should have high dynamic range. There should be at least one facing front for traffic light recognition. GNSS IMU Note that recommended sensor configuration may change depending on future enhancement of Localization/Perception algorithms. More details about Autoware's reference platform will be discussed in here . Input # As mentioned above, the combination of sensor inputs can vary depending on user's sensor configurations. Therefore, incoming sensor data can also come in as different formats with various interface(USB, CAN bus, ethernet, etc.). Sensor drivers are usually made specific to the hardware, and Autoware only provides drivers for recommended sensor configuration mentioned above. Autoware does not limit the use of other sensors, but it is user's responsibility to prepare ROS sensor driver for the sensor in such case. Output # Since we don't set requirements about the sensor configuration of the vehicle, outputs of Sensing stack also varies. However, output message type must be defined for each type of sensors in order to abstract sensor outputs for other modules, which is one of the roles of Sensing stack. The table below summarizes the output message for recommended sensors. More sensors will be added into above table after appropriate investigation about how the data will be used in the following stack. In general, the final output of Sensing stack should be in sensor_msgs type which is de facto standard in ROS systems. This allows developers to utilize default ROS tools such as RVIZ to visualize outputs. A reason should be provided if any other data type is used. Sensor Topic (Data Type) Explanation LiDAR /sensing/lidar/pointcloud ( sensor_msgs::PointCloud2 ) This contains 3D shape information of surrounding environment as a collection of rasterized points. It is usually used for map matching in Localization stack and object detection in Perception stack. Camera /sensing/{camera_name}/image ( sensor_msgs::Image ) /sensing/{camera_name}/camera_info ( sensor_msgs::CameraInfo ) Camera should provide both Image and CameraInfo topics. Image message contains 2D light intensity information (usually RGB). It is commonly used in Perception (Traffic Light Recognition, Object Recognition) and in Localization(Visual Odometry). By convention, image topic must be published in optical frame of the camera. CameraInfo message contains camera intrinsic parameters which is usually used to fuse pointcloud and image information in Perception stack. GNSS /sensing/gnss/gnss_pose ( geometry_msgs::PoseWithCovariance ) This contains absolute 3D pose on earth. The output should be converted into map frame to be used in Localization stack. IMU /sensing/imu/imu_data ( sensor_msgs::Imu ) This contains angular velocity and acceleration. The main use case is Twist estimation for Localization. The output data may also include estimated orientation as an option. rationale : GNSS data is published as geometry_msgs::PoseWithCovariance instead of sensor_msgs/NavSatFix . geometry_msgs are also one of de facto message type, and PoseWithCovariance message essentially contains the same information and is more convenient for Localization stack(the most likely user of the data) since localization is done in Cartesian coordinate. Design # In order to support the requirements, Sensing stack is decomposed as below. Depending on the use case and hardware configuration of the vehicle, users may choose to use a subset of the components stated in the diagram. General convention is that for each sensor, there will be a driver and optionally a preprocessor component. Drivers are responsible for converting sensor data into ROS message and modification of the data during conversion should be minimal. It is preprocessors' responsibility to manipulate sensor data for ease of use. Drivers # Driver components act as interface between the hardware and ROS software, and they are responsible for converting sensor data into ROS messages. In order to support Requirement 4, drivers should focus on converting raw data to ROS message with minimal modification as much as possible. Ideally, the output message type of driver should be the same as the final output of Sensing stack, but exceptions are allowed in order to avoid loss of information during conversion or to achieve faster computation time in preprocessor. LiDAR driver Input: Raw data from LiDAR. Usually, it is list of range information with time stamp. Output: sensor_msgs::PointCloud2 that includes XYZ coordinates in sensor frame. If a single scan of LiDAR contains points with different timestamp, then accurate timestamp should be specified as an additional field for each point. Camera driver Input: Raw data from camera Calibration file of the camera that contains intrinsic camera parameter information Output: Image data in sensor_msgs::Image . Camera parameter information in sensor_msgs::CameraInfo . Although sensor_msgs::CameraInfo is not direct output from a camera, these information are published should be published with image since it contains essential information for image processing. GNSS driver Input: Raw data from GNSS. Usually contains latitude and longitude information. Output: Output should be in sensor_msgs::NavSatFix which contains calculated latitude and longitude information with addition of satellite fix information. IMU driver Input: Raw data from IMU. Output: measured linear acceleration and angular velocity values in sensor_msgs::Imu . (Optional) Orientation field in orientation in sensor_msgs::Imu . This field should be filled only when orientation is direct output from the hardware (e.g. by using magnetometer). It is very common to estimate orientation from reported linear acceleration and angular velocity, but they must be done in preprocessor module rather than in a driver component. rationale : Estimating orientation in driver makes recorded data ambiguous whether the orientation comes from hardware or from software. Preprocessors # Preprocessors are responsible for manipulating ROS message data to be more \"useful\" for following Autonomous Driving stacks. Actual implementation depends on how sensor data is used in other stacks. This may include: conversion of data format removing unnecessary information complementing necessary information removing noise improving accuracies Since the output of preprocessors will the final output of Sensing stack, it must follow the output ROS message type stated above. Pointcloud Preprocessor Possible preprocessing functions: Self cropping: Removal of detected points from ego vehicle. Distortion correction: Compensation of ego vehicle's movement during 1 scan rationale : This may cause inaccuracy in reported shape/position relative to the sensor origin. Outlier filter: Most LiDAR data contains random noise due to hardware constraints. Detected points from flying insects or rain drops are usually considered as noise. Concatenation: Combine points from multiple LiDAR outputs Ground filter: Removing grounds from pointcloud for easier object detection Multiplexer: Selecting pointclouds from LiDAR that is specific for certain use case Input: ROS message from the LiDAR driver. There may be multiple inputs if the vehicle has multiple LiDARs. Output: PointCloud preprocessor may output multiple topics in sensor_msgs::PointCloud2 depending on the use case. Some examples may be: Concatenated pointcloud: Pointcloud from all available LiDARs may have less blind spots Pointcloud without ground points: ground is usually out of interest when detecting obstacles, which helps perception. Camera Preprocessor Possible preprocessing functions: Rectification Resizing Input: sensor_msgs::Image image data from driver sensor_msgs::CameraInfo from driver Output: The preprocessor may have multiple outputs depending on the selected hardware and selected algorithms in perception/localization. Some examples might be: rectified image: It is possible to rectify image using sensor_msgs::CameraInfo so that cameras can be treated as a pinhole camera model , which is useful for projecting 3D information into 2D image( or vice versa). This enables fusion of sensor data in Perception stack to improve perception result. resized image: Smaller images might be useful to fasten computation time. sensor_msgs::CameraInfo : Camera preprocessor should relay camera information driver node without modifying any values since all parameters should be constant. GNSS Preprocessor Possible preprocessing functions: conversion of (latitude, longitude, altitude) to (x,y,z) in map coordinate (Optional) Deriving orientation using multiple GNSS inputs (Optional) Filter out unreliable data Input: sensor_msgs::NavSatFix message from driver. Output: Pose in geometry_msgs::PoseWithCovariance . Unreliable data can also be filtered out based on satellite fix information. Each fields in the message should be calculated as following: Pose: This must be projected into map frame from latitude and longitude information Orientation: This should be derived from calculating changes in position over time or by using multiple GNSS sensors on vehicle. Covariance: Covariance should reflect reliability of GNSS output. It may be relaying covariance from the input or reflect satellite fix status. IMU Preprocessor Possible preprocessing functions: Bias removal orientation estimation Input: sensor_msgs::Imu topic from IMU drivers. Output: preprocessed sensor_msgs::Imu either relayed or modified from the input with functions stated above. Modification depends on hardware specification of IMU, and requirements from Localization algorithm.","title":"Sensing"},{"location":"design/software_architecture/Sensing/Sensing/#sensing","text":"","title":"Sensing"},{"location":"design/software_architecture/Sensing/Sensing/#overview","text":"For autonomous driving, a vehicle needs to be aware of its state and surrounding environment. Sensing stack collects the environment information through various sensors and manipulates data appropriately to be used by other stacks.","title":"Overview"},{"location":"design/software_architecture/Sensing/Sensing/#role","text":"There are two main roles of Sensing stack: Abstraction of sensor data to ROS message Sensing stack unifies the output format of same type of sensors so that following stacks (e.g. Perception) do not have to be aware of the hardware. Preprocessing of sensor data Raw data from sensors usually contains errors/noise due to hardware limitations. Sensing stack is responsible for removing such inaccuracies as much as possible before distributing sensor outputs to following stacks. Sensing stack may also do extra restructuring/formatting of data for so that there will be no duplicate data preprocessing done in different stacks.","title":"Role"},{"location":"design/software_architecture/Sensing/Sensing/#use-cases","text":"The use cases of Sensing stack are the followings: Estimating pose of the vehicle (Localization) Recognizing surrounding objects (Perception) Traffic light recognition (Perception) Recording log data","title":"Use Cases"},{"location":"design/software_architecture/Sensing/Sensing/#requirements","text":"For the use cases 1-3mentioned above, the actual computation is done in either Localization stack and Perception stack, but the data used for computation must be provided by Sensing stack. Therefore, requirement of the Sensing stack is: Sensing stack should provide enough information for selected Localization method. e.g. LiDAR pointcloud, GNSS, and IMU (Use Case 1) Sensing stack should provide enough information for object recognition. e.g. LiDAR pointcloud and camera image (Use Case 2) Sensing stack should provide enough information for traffic light recognition. e.g. camera image (Use Case 3) Sensing stack should convert all sensor specific data into ROS message for logging. (Use Case 4) In order to abstract sensor specification from other stacks, Sensing stack should provide sensor data as ROS message specified described in Output section. Since the architecture of Localization stack and Perception stack leaves choices of using different combinations of algorithms, Autoware does not set any requirements about input sensor configurations. It is the user's responsibility to come up with appropriate sensor configuration to achieve the user's use cases. As a reference, the recommended sensors from past experience with Autoware.AI is listed below: LiDAR It should cover 360 FOV with minimal blind spots Camera It should cover 360 FOV with minimal blind spots. It should have high dynamic range. There should be at least one facing front for traffic light recognition. GNSS IMU Note that recommended sensor configuration may change depending on future enhancement of Localization/Perception algorithms. More details about Autoware's reference platform will be discussed in here .","title":"Requirements"},{"location":"design/software_architecture/Sensing/Sensing/#input","text":"As mentioned above, the combination of sensor inputs can vary depending on user's sensor configurations. Therefore, incoming sensor data can also come in as different formats with various interface(USB, CAN bus, ethernet, etc.). Sensor drivers are usually made specific to the hardware, and Autoware only provides drivers for recommended sensor configuration mentioned above. Autoware does not limit the use of other sensors, but it is user's responsibility to prepare ROS sensor driver for the sensor in such case.","title":"Input"},{"location":"design/software_architecture/Sensing/Sensing/#output","text":"Since we don't set requirements about the sensor configuration of the vehicle, outputs of Sensing stack also varies. However, output message type must be defined for each type of sensors in order to abstract sensor outputs for other modules, which is one of the roles of Sensing stack. The table below summarizes the output message for recommended sensors. More sensors will be added into above table after appropriate investigation about how the data will be used in the following stack. In general, the final output of Sensing stack should be in sensor_msgs type which is de facto standard in ROS systems. This allows developers to utilize default ROS tools such as RVIZ to visualize outputs. A reason should be provided if any other data type is used. Sensor Topic (Data Type) Explanation LiDAR /sensing/lidar/pointcloud ( sensor_msgs::PointCloud2 ) This contains 3D shape information of surrounding environment as a collection of rasterized points. It is usually used for map matching in Localization stack and object detection in Perception stack. Camera /sensing/{camera_name}/image ( sensor_msgs::Image ) /sensing/{camera_name}/camera_info ( sensor_msgs::CameraInfo ) Camera should provide both Image and CameraInfo topics. Image message contains 2D light intensity information (usually RGB). It is commonly used in Perception (Traffic Light Recognition, Object Recognition) and in Localization(Visual Odometry). By convention, image topic must be published in optical frame of the camera. CameraInfo message contains camera intrinsic parameters which is usually used to fuse pointcloud and image information in Perception stack. GNSS /sensing/gnss/gnss_pose ( geometry_msgs::PoseWithCovariance ) This contains absolute 3D pose on earth. The output should be converted into map frame to be used in Localization stack. IMU /sensing/imu/imu_data ( sensor_msgs::Imu ) This contains angular velocity and acceleration. The main use case is Twist estimation for Localization. The output data may also include estimated orientation as an option. rationale : GNSS data is published as geometry_msgs::PoseWithCovariance instead of sensor_msgs/NavSatFix . geometry_msgs are also one of de facto message type, and PoseWithCovariance message essentially contains the same information and is more convenient for Localization stack(the most likely user of the data) since localization is done in Cartesian coordinate.","title":"Output"},{"location":"design/software_architecture/Sensing/Sensing/#design","text":"In order to support the requirements, Sensing stack is decomposed as below. Depending on the use case and hardware configuration of the vehicle, users may choose to use a subset of the components stated in the diagram. General convention is that for each sensor, there will be a driver and optionally a preprocessor component. Drivers are responsible for converting sensor data into ROS message and modification of the data during conversion should be minimal. It is preprocessors' responsibility to manipulate sensor data for ease of use.","title":"Design"},{"location":"design/software_architecture/Sensing/Sensing/#drivers","text":"Driver components act as interface between the hardware and ROS software, and they are responsible for converting sensor data into ROS messages. In order to support Requirement 4, drivers should focus on converting raw data to ROS message with minimal modification as much as possible. Ideally, the output message type of driver should be the same as the final output of Sensing stack, but exceptions are allowed in order to avoid loss of information during conversion or to achieve faster computation time in preprocessor. LiDAR driver Input: Raw data from LiDAR. Usually, it is list of range information with time stamp. Output: sensor_msgs::PointCloud2 that includes XYZ coordinates in sensor frame. If a single scan of LiDAR contains points with different timestamp, then accurate timestamp should be specified as an additional field for each point. Camera driver Input: Raw data from camera Calibration file of the camera that contains intrinsic camera parameter information Output: Image data in sensor_msgs::Image . Camera parameter information in sensor_msgs::CameraInfo . Although sensor_msgs::CameraInfo is not direct output from a camera, these information are published should be published with image since it contains essential information for image processing. GNSS driver Input: Raw data from GNSS. Usually contains latitude and longitude information. Output: Output should be in sensor_msgs::NavSatFix which contains calculated latitude and longitude information with addition of satellite fix information. IMU driver Input: Raw data from IMU. Output: measured linear acceleration and angular velocity values in sensor_msgs::Imu . (Optional) Orientation field in orientation in sensor_msgs::Imu . This field should be filled only when orientation is direct output from the hardware (e.g. by using magnetometer). It is very common to estimate orientation from reported linear acceleration and angular velocity, but they must be done in preprocessor module rather than in a driver component. rationale : Estimating orientation in driver makes recorded data ambiguous whether the orientation comes from hardware or from software.","title":"Drivers"},{"location":"design/software_architecture/Sensing/Sensing/#preprocessors","text":"Preprocessors are responsible for manipulating ROS message data to be more \"useful\" for following Autonomous Driving stacks. Actual implementation depends on how sensor data is used in other stacks. This may include: conversion of data format removing unnecessary information complementing necessary information removing noise improving accuracies Since the output of preprocessors will the final output of Sensing stack, it must follow the output ROS message type stated above. Pointcloud Preprocessor Possible preprocessing functions: Self cropping: Removal of detected points from ego vehicle. Distortion correction: Compensation of ego vehicle's movement during 1 scan rationale : This may cause inaccuracy in reported shape/position relative to the sensor origin. Outlier filter: Most LiDAR data contains random noise due to hardware constraints. Detected points from flying insects or rain drops are usually considered as noise. Concatenation: Combine points from multiple LiDAR outputs Ground filter: Removing grounds from pointcloud for easier object detection Multiplexer: Selecting pointclouds from LiDAR that is specific for certain use case Input: ROS message from the LiDAR driver. There may be multiple inputs if the vehicle has multiple LiDARs. Output: PointCloud preprocessor may output multiple topics in sensor_msgs::PointCloud2 depending on the use case. Some examples may be: Concatenated pointcloud: Pointcloud from all available LiDARs may have less blind spots Pointcloud without ground points: ground is usually out of interest when detecting obstacles, which helps perception. Camera Preprocessor Possible preprocessing functions: Rectification Resizing Input: sensor_msgs::Image image data from driver sensor_msgs::CameraInfo from driver Output: The preprocessor may have multiple outputs depending on the selected hardware and selected algorithms in perception/localization. Some examples might be: rectified image: It is possible to rectify image using sensor_msgs::CameraInfo so that cameras can be treated as a pinhole camera model , which is useful for projecting 3D information into 2D image( or vice versa). This enables fusion of sensor data in Perception stack to improve perception result. resized image: Smaller images might be useful to fasten computation time. sensor_msgs::CameraInfo : Camera preprocessor should relay camera information driver node without modifying any values since all parameters should be constant. GNSS Preprocessor Possible preprocessing functions: conversion of (latitude, longitude, altitude) to (x,y,z) in map coordinate (Optional) Deriving orientation using multiple GNSS inputs (Optional) Filter out unreliable data Input: sensor_msgs::NavSatFix message from driver. Output: Pose in geometry_msgs::PoseWithCovariance . Unreliable data can also be filtered out based on satellite fix information. Each fields in the message should be calculated as following: Pose: This must be projected into map frame from latitude and longitude information Orientation: This should be derived from calculating changes in position over time or by using multiple GNSS sensors on vehicle. Covariance: Covariance should reflect reliability of GNSS output. It may be relaying covariance from the input or reflect satellite fix status. IMU Preprocessor Possible preprocessing functions: Bias removal orientation estimation Input: sensor_msgs::Imu topic from IMU drivers. Output: preprocessed sensor_msgs::Imu either relayed or modified from the input with functions stated above. Modification depends on hardware specification of IMU, and requirements from Localization algorithm.","title":"Preprocessors"},{"location":"design/software_architecture/Vehicle/Vehicle/","text":"Vehicle # Overview # Vehicle stack is an interface between Autoware and vehicle. This layer converts signals from Autoware to vehicle-specific, and vice versa. This module needs to be designed according to the vehicle to be used. How to implement a new interface is described below. Role # There are two main roles of Vehicle stack: Conversion of Autoware commands to a vehicle-specific format Conversion of vehicle status in a vehicle-specific format to Autoware messages Assumption # It is assumed that the vehicle has one of the following control interfaces. Type A. target steering and target velocity/acceleration interface. Type B. generalized target command interface (e.g. accel/brake pedal, steering torque). The use case and requirements change according to this type. Use Cases # Vehicle stack supports the following use cases. Speed control with desired velocity or acceleration (for type A only) Steering control with desired steering angle and/or steering angle velocity (for type A only) Speed control with desired actuation commands (for type B only) Steering control with desired actuation commands (for type B only) Shift control (for both) Turn signal control (for both) Requirement # To achieve the above use case, the vehicle stack requires the following conditions. Vehicle control with desired steering and velocity/acceleration (for type A) # The vehicle can be controlled in longitudinal direction by the target velocity or acceleration. The vehicle can be controlled in lateral direction by the target steering. The output to the vehicle includes desired velocity or acceleration in a vehicle-specific format. The output to the vehicle includes desired steering in a vehicle-specific format. Vehicle control with the desired actuation command (for type B) # The vehicle can be controlled in longitudinal direction by the specific target command (e.g. accel/brake pedal) The vehicle can be controlled in lateral direction by the specific target command (e.g. steering torque) The output to the vehicle includes desired target command in a vehicle-specific format. Shift control # The vehicle can be controlled by the target shift mode. The input vehicle command includes the desired shift. The output to the vehicle includes the desired shift in a vehicle-specific format. Turn signal control # The vehicle can be controlled by the target turn signal mode. The input vehicle command includes the desired turn signal. The output to the vehicle includes the desired turn signal in a vehicle-specific format. Input # The input to Vehicle stack: Input Topic(Data Type) Explanation Vehicle Command /control/vehicle_cmd ( autoware_vehicle_msgs/VehicleCommand ) Table Below The detailed contents in Vehicle Command are as follows. Input Data Type Explanation Velocity std_msgs/Float64 Target velocity [m/s] Acceleration std_msgs/Float64 Target acceleration [m/s2] Steering angle std_msgs/Float64 Target steering angle [rad] Steering angle velocity std_msgs/Float64 Target steering angle velocity [rad/s] Gear shifting command std_msgs/Int32 Target Gear shift Emergency command std_msgs/Int32 Emergency status of Autoware Output # There are two types of outputs from Vehicle stack: vehicle status to Autoware and a control command to the vehicle. The table below summarizes the output from Vehicle stack: Output (to Autoware) Topic(Data Type) Explanation velocity status /vehicle/status/twist ( geometry_msgs/TwistStamped ) vehicle velocity status to Autoware [m/s] steering status (optional) /vehicle/status/steering ( autoware_vehicle_msgs/Steering ) vehicle steering status to Autoware [rad] shift status (optional) /vehicle/status/Shift ( autoware_vehicle_msgs/ShiftStamped ) vehicle shift to Autoware [-] turn signal status (optional) /vehicle/status/turn_signal ( autoware_vehicle_msgs/TurnSignal ) vehicle turn signal status to Autoware [m/s] actuation status (optional) /vehicle/status/actuation_status ( autoware_vehicle_msgs/ActuationStatusStamped ) vehicle actuation status to Autoware [m/s] The output to the vehicle depends on each vehicle interface. Output (to vehicle) Topic(Data Type) Explanation vehicle control messages Depends on each vehicle Control signals to drive the vehicle Internal Messages # If the vehicle does not support the TypeA interface (steering angle and velocity/acceleration), the following actuation command/status are used. These message is defined according to the interface for each vehicle, and the raw vehicle command converter is responsible for the conversion between the output from the autoware control module and the actuation message. Input Data Type Explanation accel_cmd std_msgs/Float64 Vehicle-specific commands to accelerate the vehicle. brake_cmd std_msgs/Float64 Vehicle-specific commands to decelerate the vehicle. steer_cmd std_msgs/Float64 Vehicle-specific commands to control the vehicle steering. Output Data Type Explanation accel_status std_msgs/Float64 Status of the accel-actuator corresponding to the actuation command (e.g. accel pedal) brake_status std_msgs/Float64 Status of the brake-actuator corresponding to the actuation command (e.g. brake pedal) steer_status std_msgs/Float64 Status of the steering-actuator corresponding to the actuation command (e.g. steering torque) Design # For vehicles of the type controlled by the target velocity or acceleration (type A) For vehicles of the type controlled by the target accel, brake and steer commands (type B) Vehicle Interface # Role # To convert Autoware control messages to vehicle-specific format, and generate vehicle status messages from vehicle-specific format. Input # Vehicle Command ( autoware_vehicle_msgs/VehicleCommand ) (type A only) includes target velocity, acceleration, steering angle, steering angle velocity, gear shift, and emergency. Actuation Command ( autoware_vehicle_msgs/ActuationCommandStamped ) (type B only) includes target accel_cmd, brake_cmd, steer_cmd. Turn signal ( autoware_vehicle_msgs/TurnSignal ) (optional) Output # Velocity status ( geometry_msgs/TwistStamped ) Steering status ( autoware_vehicle_msgs/Steering ) (optional) Shift status ( autoware_vehicle_msgs/ShiftStamped ) (optional) Turn signal status ( autoware_vehicle_msgs/TurnSignal ) (optional) Actuation Status ( autoware_vehicle_msgs/ActuationStatusStamped ) (optional) NOTE: Lane driving is possible without the optional part. Design vehicle interface according to the purpose. Raw Vehicle Cmd Converter # Role # To convert the target acceleration to the actuation commands with the given conversion map. This node is used only for the case of vehicle type B. Input # Vehicle Command ( autoware_vehicle_msgs/VehicleCommand ) Current velocity ( geometry_msgs/TwistStamped ) Output # Actuation Command ( autoware_vehicle_msgs/ActuationCommandStamped ) includes target accel_cmd, brake_cmd, steer_cmd. How to design a new vehicle interface # For type A # Create a module that satisfies the following two requirements Receives autoware_vehicle_msg/VehicleCommand and sends control commands to the vehicle. Converts the information from the vehicle, publishes vehicle speed to Autoware with geometry_msgs/TwistStamped . For example, if the vehicle has an interface to be controlled with a target velocity, the velocity in autoware_vehicle_msg/VehicleCommand is sent to the vehicle as the target velocity. If the vehicle control interface is steering wheel angle, it is necessary to convert steering angle to steering wheel angle in this vehicle_interface. For type B # Since autoware_vehicle_msg/VehicleCommand contains only the target steering, velocity and acceleration, you need to convert these values for the vehicle-specific interface. In this case, use the RawVehicleCmdConverter . The RawVehicleCmdConverter converts the VehicleCommand to the ActuationCommand which is specific to each vehicle based on the given conversion map. You need to create this conversion map in advance from vehicle data sheets and experiments. With the use of RawVehicleCmdConverter , you need to create a module that satisfies the following two requirements Receives autoware_vehicle_msg/ActuationCommandStamped and sends control commands to the vehicle. Converts the information from the vehicle, publishes vehicle speed to Autoware with geometry_msgs/TwistStamped . How to make an conversion map (for type B) # When using the RawVehicleCmdConverter described above, it is necessary to create an conversion map for each vehicle. The conversion map is data in CSV format. In the case of conversion between accel pedal and acceleration, it describes how much acceleration is produced when the accel command is on in each vehicle speed range. You can find the default conversion map data in src/vehicle/raw_vehicle_cmd_converter/data as a reference. In the CSV data, the horizontal axis is the current velocity [m/s], the vertical axis is the vehicle-specific command value [-], and the element is the acceleration [m/ss] as described below. This is the reference data created by TierIV with the following steps. Press the command to a constant value on a flat road to accelerate/decelerate the vehicle. Save IMU acceleration and vehicle velocity data during acceleration/deceleration. Create a CSV file with the relationship between command values and acceleration at each vehicle speed. After your acceleration map is created, load it when RawVehicleCmdConverter is launched (the file path is defined at the launch file). Control of additional elements, such as turn signals # If you need to control parts that are not related to the vehicle drive (turn signals, doors, window opening and closing, headlights, etc.), the vehicle interface will handle them separately. The current Autoware supports and implements only turn signals.","title":"Vehicle"},{"location":"design/software_architecture/Vehicle/Vehicle/#vehicle","text":"","title":"Vehicle"},{"location":"design/software_architecture/Vehicle/Vehicle/#overview","text":"Vehicle stack is an interface between Autoware and vehicle. This layer converts signals from Autoware to vehicle-specific, and vice versa. This module needs to be designed according to the vehicle to be used. How to implement a new interface is described below.","title":"Overview"},{"location":"design/software_architecture/Vehicle/Vehicle/#role","text":"There are two main roles of Vehicle stack: Conversion of Autoware commands to a vehicle-specific format Conversion of vehicle status in a vehicle-specific format to Autoware messages","title":"Role"},{"location":"design/software_architecture/Vehicle/Vehicle/#assumption","text":"It is assumed that the vehicle has one of the following control interfaces. Type A. target steering and target velocity/acceleration interface. Type B. generalized target command interface (e.g. accel/brake pedal, steering torque). The use case and requirements change according to this type.","title":"Assumption"},{"location":"design/software_architecture/Vehicle/Vehicle/#use-cases","text":"Vehicle stack supports the following use cases. Speed control with desired velocity or acceleration (for type A only) Steering control with desired steering angle and/or steering angle velocity (for type A only) Speed control with desired actuation commands (for type B only) Steering control with desired actuation commands (for type B only) Shift control (for both) Turn signal control (for both)","title":"Use Cases"},{"location":"design/software_architecture/Vehicle/Vehicle/#requirement","text":"To achieve the above use case, the vehicle stack requires the following conditions.","title":"Requirement"},{"location":"design/software_architecture/Vehicle/Vehicle/#vehicle-control-with-desired-steering-and-velocityacceleration-for-type-a","text":"The vehicle can be controlled in longitudinal direction by the target velocity or acceleration. The vehicle can be controlled in lateral direction by the target steering. The output to the vehicle includes desired velocity or acceleration in a vehicle-specific format. The output to the vehicle includes desired steering in a vehicle-specific format.","title":"Vehicle control with desired steering and velocity/acceleration (for type A)"},{"location":"design/software_architecture/Vehicle/Vehicle/#vehicle-control-with-the-desired-actuation-command-for-type-b","text":"The vehicle can be controlled in longitudinal direction by the specific target command (e.g. accel/brake pedal) The vehicle can be controlled in lateral direction by the specific target command (e.g. steering torque) The output to the vehicle includes desired target command in a vehicle-specific format.","title":"Vehicle control with the desired actuation command (for type B)"},{"location":"design/software_architecture/Vehicle/Vehicle/#shift-control","text":"The vehicle can be controlled by the target shift mode. The input vehicle command includes the desired shift. The output to the vehicle includes the desired shift in a vehicle-specific format.","title":"Shift control"},{"location":"design/software_architecture/Vehicle/Vehicle/#turn-signal-control","text":"The vehicle can be controlled by the target turn signal mode. The input vehicle command includes the desired turn signal. The output to the vehicle includes the desired turn signal in a vehicle-specific format.","title":"Turn signal control"},{"location":"design/software_architecture/Vehicle/Vehicle/#input","text":"The input to Vehicle stack: Input Topic(Data Type) Explanation Vehicle Command /control/vehicle_cmd ( autoware_vehicle_msgs/VehicleCommand ) Table Below The detailed contents in Vehicle Command are as follows. Input Data Type Explanation Velocity std_msgs/Float64 Target velocity [m/s] Acceleration std_msgs/Float64 Target acceleration [m/s2] Steering angle std_msgs/Float64 Target steering angle [rad] Steering angle velocity std_msgs/Float64 Target steering angle velocity [rad/s] Gear shifting command std_msgs/Int32 Target Gear shift Emergency command std_msgs/Int32 Emergency status of Autoware","title":"Input"},{"location":"design/software_architecture/Vehicle/Vehicle/#output","text":"There are two types of outputs from Vehicle stack: vehicle status to Autoware and a control command to the vehicle. The table below summarizes the output from Vehicle stack: Output (to Autoware) Topic(Data Type) Explanation velocity status /vehicle/status/twist ( geometry_msgs/TwistStamped ) vehicle velocity status to Autoware [m/s] steering status (optional) /vehicle/status/steering ( autoware_vehicle_msgs/Steering ) vehicle steering status to Autoware [rad] shift status (optional) /vehicle/status/Shift ( autoware_vehicle_msgs/ShiftStamped ) vehicle shift to Autoware [-] turn signal status (optional) /vehicle/status/turn_signal ( autoware_vehicle_msgs/TurnSignal ) vehicle turn signal status to Autoware [m/s] actuation status (optional) /vehicle/status/actuation_status ( autoware_vehicle_msgs/ActuationStatusStamped ) vehicle actuation status to Autoware [m/s] The output to the vehicle depends on each vehicle interface. Output (to vehicle) Topic(Data Type) Explanation vehicle control messages Depends on each vehicle Control signals to drive the vehicle","title":"Output"},{"location":"design/software_architecture/Vehicle/Vehicle/#internal-messages","text":"If the vehicle does not support the TypeA interface (steering angle and velocity/acceleration), the following actuation command/status are used. These message is defined according to the interface for each vehicle, and the raw vehicle command converter is responsible for the conversion between the output from the autoware control module and the actuation message. Input Data Type Explanation accel_cmd std_msgs/Float64 Vehicle-specific commands to accelerate the vehicle. brake_cmd std_msgs/Float64 Vehicle-specific commands to decelerate the vehicle. steer_cmd std_msgs/Float64 Vehicle-specific commands to control the vehicle steering. Output Data Type Explanation accel_status std_msgs/Float64 Status of the accel-actuator corresponding to the actuation command (e.g. accel pedal) brake_status std_msgs/Float64 Status of the brake-actuator corresponding to the actuation command (e.g. brake pedal) steer_status std_msgs/Float64 Status of the steering-actuator corresponding to the actuation command (e.g. steering torque)","title":"Internal Messages"},{"location":"design/software_architecture/Vehicle/Vehicle/#design","text":"For vehicles of the type controlled by the target velocity or acceleration (type A) For vehicles of the type controlled by the target accel, brake and steer commands (type B)","title":"Design"},{"location":"design/software_architecture/Vehicle/Vehicle/#vehicle-interface","text":"","title":"Vehicle Interface"},{"location":"design/software_architecture/Vehicle/Vehicle/#role_1","text":"To convert Autoware control messages to vehicle-specific format, and generate vehicle status messages from vehicle-specific format.","title":"Role"},{"location":"design/software_architecture/Vehicle/Vehicle/#input_1","text":"Vehicle Command ( autoware_vehicle_msgs/VehicleCommand ) (type A only) includes target velocity, acceleration, steering angle, steering angle velocity, gear shift, and emergency. Actuation Command ( autoware_vehicle_msgs/ActuationCommandStamped ) (type B only) includes target accel_cmd, brake_cmd, steer_cmd. Turn signal ( autoware_vehicle_msgs/TurnSignal ) (optional)","title":"Input"},{"location":"design/software_architecture/Vehicle/Vehicle/#output_1","text":"Velocity status ( geometry_msgs/TwistStamped ) Steering status ( autoware_vehicle_msgs/Steering ) (optional) Shift status ( autoware_vehicle_msgs/ShiftStamped ) (optional) Turn signal status ( autoware_vehicle_msgs/TurnSignal ) (optional) Actuation Status ( autoware_vehicle_msgs/ActuationStatusStamped ) (optional) NOTE: Lane driving is possible without the optional part. Design vehicle interface according to the purpose.","title":"Output"},{"location":"design/software_architecture/Vehicle/Vehicle/#raw-vehicle-cmd-converter","text":"","title":"Raw Vehicle Cmd Converter"},{"location":"design/software_architecture/Vehicle/Vehicle/#role_2","text":"To convert the target acceleration to the actuation commands with the given conversion map. This node is used only for the case of vehicle type B.","title":"Role"},{"location":"design/software_architecture/Vehicle/Vehicle/#input_2","text":"Vehicle Command ( autoware_vehicle_msgs/VehicleCommand ) Current velocity ( geometry_msgs/TwistStamped )","title":"Input"},{"location":"design/software_architecture/Vehicle/Vehicle/#output_2","text":"Actuation Command ( autoware_vehicle_msgs/ActuationCommandStamped ) includes target accel_cmd, brake_cmd, steer_cmd.","title":"Output"},{"location":"design/software_architecture/Vehicle/Vehicle/#how-to-design-a-new-vehicle-interface","text":"","title":"How to design a new vehicle interface"},{"location":"design/software_architecture/Vehicle/Vehicle/#for-type-a","text":"Create a module that satisfies the following two requirements Receives autoware_vehicle_msg/VehicleCommand and sends control commands to the vehicle. Converts the information from the vehicle, publishes vehicle speed to Autoware with geometry_msgs/TwistStamped . For example, if the vehicle has an interface to be controlled with a target velocity, the velocity in autoware_vehicle_msg/VehicleCommand is sent to the vehicle as the target velocity. If the vehicle control interface is steering wheel angle, it is necessary to convert steering angle to steering wheel angle in this vehicle_interface.","title":"For type A"},{"location":"design/software_architecture/Vehicle/Vehicle/#for-type-b","text":"Since autoware_vehicle_msg/VehicleCommand contains only the target steering, velocity and acceleration, you need to convert these values for the vehicle-specific interface. In this case, use the RawVehicleCmdConverter . The RawVehicleCmdConverter converts the VehicleCommand to the ActuationCommand which is specific to each vehicle based on the given conversion map. You need to create this conversion map in advance from vehicle data sheets and experiments. With the use of RawVehicleCmdConverter , you need to create a module that satisfies the following two requirements Receives autoware_vehicle_msg/ActuationCommandStamped and sends control commands to the vehicle. Converts the information from the vehicle, publishes vehicle speed to Autoware with geometry_msgs/TwistStamped .","title":"For type B"},{"location":"design/software_architecture/Vehicle/Vehicle/#how-to-make-an-conversion-map-for-type-b","text":"When using the RawVehicleCmdConverter described above, it is necessary to create an conversion map for each vehicle. The conversion map is data in CSV format. In the case of conversion between accel pedal and acceleration, it describes how much acceleration is produced when the accel command is on in each vehicle speed range. You can find the default conversion map data in src/vehicle/raw_vehicle_cmd_converter/data as a reference. In the CSV data, the horizontal axis is the current velocity [m/s], the vertical axis is the vehicle-specific command value [-], and the element is the acceleration [m/ss] as described below. This is the reference data created by TierIV with the following steps. Press the command to a constant value on a flat road to accelerate/decelerate the vehicle. Save IMU acceleration and vehicle velocity data during acceleration/deceleration. Create a CSV file with the relationship between command values and acceleration at each vehicle speed. After your acceleration map is created, load it when RawVehicleCmdConverter is launched (the file path is defined at the launch file).","title":"How to make an conversion map (for type B)"},{"location":"design/software_architecture/Vehicle/Vehicle/#control-of-additional-elements-such-as-turn-signals","text":"If you need to control parts that are not related to the vehicle drive (turn signals, doors, window opening and closing, headlights, etc.), the vehicle interface will handle them separately. The current Autoware supports and implements only turn signals.","title":"Control of additional elements, such as turn signals"},{"location":"developer_guide/ClangTidyGuideline/","text":"Clang-Tidy Guidelines # Weekly Clang-Tidy analysis report for main branch is here (updated every Sunday). Information: Due to the lack of the header files of CUDA or NVML, Perception modules and NVML GPU Monitor can not be analyzed correctly by clang-tidy-pr in autoware.iv repository. You may find some clang-errors and false positives. This document follows the convention of RFC2119 . How to use Clang-Tidy on your local machine # First, install Clang-Tidy. # Ubuntu/Debian sudo apt install clang-tidy Next, build your package with -DCMAKE_EXPORT_COMPILE_COMMANDS=ON option. cd ~/autoware.proj colcon build --cmake-args -DCMAKE_BUILD_TYPE = Release -DCMAKE_EXPORT_COMPILE_COMMANDS = ON Finally, run ./scripts/clang-tidy-package.sh . ./scripts/clang-tidy-package.sh <path-to-your-package> For example: ./scripts/clang-tidy-package.sh src/autoware/autoware.iv/common/util/autoware_utils/ Or, execute Clang-Tidy directly. clang-tidy -p build/compile_commands.json <your-source-code> Severity Level # The severity level of check rules are defined by CodeChecker, not by Clang-Tidy itself. The level definitions of rules can be found here . High # We MUST fix the code problems pointed out by Clang-Tidy. Middle # We SHOULD fix the code problems pointed out by Clang-Tidy. Low # We MAY fix the code problems pointed out by Clang-Tidy. But some rules SHOULD NOT be ignored. See Rules section. Style # We MAY fix the code problems pointed out by Clang-Tidy. But some rules SHOULD NOT be ignored. See Rules section. Rules # Some rules are disabled by default to prevent false-positives that are annoying. clang-diagnostic-* # We MUST fix the code problems detected by clang-diagnostic-* rules. These rules come from Clang++ compile-time errors and warnings. boost-* # We MUST fix the code problems detected by boost-_ rules if these are not false-positives. boost-use-to-string [LOW] (doc) # bugprone-* # We SHOULD fix the code problems detected by bugprone-* rules if these are not false-positives. bugprone-argument-comment [LOW] (doc) # bugprone-assert-side-effect [MEDIUM] (doc) # bugprone-bad-signal-to-kill-thread [MEDIUM] (doc) # bugprone-bool-pointer-implicit-conversion [LOW] (doc) # bugprone-branch-clone [LOW] (doc) # Sometimes the clones are intentional as follows. We MAY ignore safely. (Note that the following code violates the modernize-make-unique rule) } else if ( type == autoware_perception_msgs :: msg :: Semantic :: MOTORBIKE ) { model_ptr . reset ( new normal :: BoundingBoxModel ); } else if ( type == autoware_perception_msgs :: msg :: Semantic :: BICYCLE ) { model_ptr . reset ( new normal :: BoundingBoxModel ); } bugprone-copy-constructor-init [MEDIUM] (doc) # bugprone-dangling-handle [HIGH] (doc) # bugprone-dynamic-static-initializers [MEDIUM] (doc) # bugprone-exception-escape [MEDIUM] (doc) # bugprone-fold-init-type [HIGH] (doc) # bugprone-forward-declaration-namespace [LOW] (doc) # bugprone-forwarding-reference-overload [LOW] (doc) # bugprone-inaccurate-erase [HIGH] (doc) # bugprone-incorrect-roundings [HIGH] (doc) # (bugprone-infinite-loop) [MEDIUM] (doc) # Disabled because the rule falls into the infinite-loop when checking some ROS functions, such as spin() . bugprone-integer-division [MEDIUM] (doc) # bugprone-lambda-function-name [LOW] (doc) # bugprone-macro-parentheses [MEDIUM] (doc) # bugprone-macro-repeated-side-effects [MEDIUM] (doc) # bugprone-misplaced-operator-in-strlen-in-alloc [MEDIUM] (doc) # (bugprone-misplaced-pointer-arithmetic-in-alloc) [MEDIUM] (doc) # Disabled. bugprone-misplaced-widening-cast [HIGH] (doc) # bugprone-move-forwarding-reference [MEDIUM] (doc) # bugprone-multiple-statement-macro [MEDIUM] (doc) # (bugprone-no-escape) [undefined] (doc) # To be added to .clang-tidy. bugprone-not-null-terminated-result [MEDIUM] (doc) # bugprone-parent-virtual-call [MEDIUM] (doc) # bugprone-posix-return [undefined] (doc) # (bugprone-redundant-branch-condition) [LOW] (doc) # Disabled. (bugprone-reserved-identifier) [LOW] (doc) # Disabled because ROS 2 coding naming style for inclusion guards violates the rule by default. (bugprone-signal-handler) [undefined] (doc) # To be added to .clang-tidy. bugprone-signed-char-misuse [MEDIUM] (doc) # bugprone-sizeof-container [HIGH] (doc) # bugprone-sizeof-expression [HIGH] (doc) # (bugprone-spuriously-wake-up-functions) [undefined] (doc) # bugprone-string-constructor [HIGH] (doc) # bugprone-string-integer-assignment [LOW] (doc) # bugprone-string-literal-with-embedded-nul [MEDIUM] (doc) # bugprone-suspicious-enum-usage [HIGH] (doc) # (bugprone-suspicious-include) [LOW] (doc) # Disabled. bugprone-suspicious-memset-usage [HIGH] (doc) # bugprone-suspicious-missing-comma [HIGH] (doc) # bugprone-suspicious-semicolon [HIGH] (doc) # bugprone-suspicious-string-compare [MEDIUM] (doc) # bugprone-swapped-arguments [HIGH] (doc) # bugprone-terminating-continue [MEDIUM] (doc) # bugprone-throw-keyword-missing [MEDIUM] (doc) # bugprone-too-small-loop-variable [MEDIUM] (doc) # bugprone-undefined-memory-manipulation [MEDIUM] (doc) # bugprone-undelegated-constructor [MEDIUM] (doc) # (bugprone-unhandled-exception-at-new) [undefined] (doc) # To be added. bugprone-unhandled-self-assignment [MEDIUM] (doc) # bugprone-unused-raii [HIGH] (doc) # bugprone-unused-return-value [MEDIUM] (doc) # bugprone-use-after-move [HIGH] (doc) # bugprone-virtual-near-miss [MEDIUM] (doc) # cppcoreguidelines-* # We SHOULD fix the code problems detected by cppcoreguidelines-* rules if these are not false-positives. cppcoreguidelines-avoid-goto [STYLE] (doc) # (cppcoreguidelines-avoid-non-const-global-variables) [LOW] (doc) # Disabled. cppcoreguidelines-init-variables [MEDIUM] (doc) # cppcoreguidelines-interfaces-global-init [LOW] (doc) # cppcoreguidelines-macro-usage [LOW] (doc) # cppcoreguidelines-narrowing-conversions [MEDIUM] (doc) # cppcoreguidelines-no-malloc [LOW] (doc) # (cppcoreguidelines-owning-memory) [STYLE] (doc) # Disabled. cppcoreguidelines-prefer-member-initializer [STYLE] (doc) # Disabled. (cppcoreguidelines-pro-bounds-array-to-pointer-decay) [LOW] (doc) # Disabled. (cppcoreguidelines-pro-bounds-constant-array-index) [LOW] (doc) # Disabled. cppcoreguidelines-pro-bounds-pointer-arithmetic [LOW] (doc) # cppcoreguidelines-pro-type-const-cast [LOW] (doc) # cppcoreguidelines-pro-type-cstyle-cast [LOW] (doc) # cppcoreguidelines-pro-type-member-init [LOW] (doc) # cppcoreguidelines-pro-type-reinterpret-cast [LOW] (doc) # cppcoreguidelines-pro-type-static-cast-downcast [LOW] (doc) # cppcoreguidelines-pro-type-union-access [LOW] (doc) # (cppcoreguidelines-pro-type-vararg) [LOW] (doc) # Disabled. cppcoreguidelines-slicing [LOW] (doc) # cppcoreguidelines-special-member-functions [LOW] (doc) # google-* # We SHOULD fix the code problems detected by google-* rules if these are not false-positives. google-build-explicit-make-pair [MEDIUM] (doc) # google-build-namespaces [MEDIUM] (doc) # google-build-using-namespace [STYLE] (doc) # (google-default-arguments) [LOW] (doc) # Disabled. google-explicit-constructor [MEDIUM] (doc) # google-global-names-in-headers [STYLE] (doc) # (google-objc-avoid-nsobject-new) [undefined] (doc) # Disabled. (google-objc-avoid-throwing-exception) [undefined] (doc) # Disabled. (google-objc-function-naming) [undefined] (doc) # Disabled. (google-objc-global-variable-declaration) [undefined] (doc) # Disabled. (google-readability-avoid-underscore-in-googletest-name) [STYLE] (doc) # (google-readability-casting) [LOW] (doc) # Disabled. (google-readability-todo) [STYLE] (doc) # Disabled. (google-runtime-int) [LOW] (doc) # Disabled. (google-runtime-operator) [MEDIUM] (doc) # Disabled. google-upgrade-googletest-case [STYLE] (doc) # hicpp-* # We SHOULD fix the code problems detected by hicpp-* rules if these are not false-positives. hicpp-exception-baseclass [LOW] (doc) # hicpp-multiway-paths-covered [STYLE] (doc) # hicpp-no-assembler [LOW] (doc) # hicpp-signed-bitwise [LOW] (doc) # llvm-* # We MUST fix the code problems detected by llvm-* rules if these are not false-positives. llvm-namespace-comment [LOW] (doc) # misc-* # We SHOULD fix the code problems detected by misc-* rules if these are not false-positives. misc-definitions-in-headers [MEDIUM] (doc) # misc-misplaced-const [LOW] (doc) # misc-new-delete-overloads [MEDIUM] (doc) # misc-no-recursion [LOW] (doc) # misc-non-copyable-objects [HIGH] (doc) # (misc-non-private-member-variables-in-classes) [LOW] (doc) # Disabled because of annoyance. misc-redundant-expression [MEDIUM] (doc) # MUST. misc-static-assert [LOW] (doc) # misc-throw-by-value-catch-by-reference [HIGH] (doc) # MUST. misc-unconventional-assign-operator [MEDIUM] (doc) # misc-uniqueptr-reset-release [MEDIUM] (doc) # misc-unused-alias-decls [LOW] (doc) # MUST. misc-unused-parameters [LOW] (doc) # MUST. misc-unused-using-decls [LOW] (doc) # MUST. modernize-* # We SHOULD fix the code problems detected by modernize-* rules if these are not false-positives. (modernize-avoid-bind) [STYLE] (doc) # Disabled because of incompatibility with ROS. modernize-avoid-c-arrays [LOW] (doc) # modernize-concat-nested-namespaces [STYLE] (doc) # modernize-deprecated-headers [LOW] (doc) # MUST. modernize-deprecated-ios-base-aliases [LOW] (doc) # modernize-loop-convert [STYLE] (doc) # modernize-make-shared [LOW] (doc) # modernize-make-unique [LOW] (doc) # modernize-pass-by-value [LOW] (doc) # modernize-raw-string-literal [STYLE] (doc) # modernize-redundant-void-arg [STYLE] (doc) # MUST. modernize-replace-auto-ptr [LOW] (doc) # modernize-replace-disallow-copy-and-assign-macro [LOW] (doc) # modernize-replace-random-shuffle [LOW] (doc) # modernize-return-braced-init-list [STYLE] (doc) # modernize-shrink-to-fit [STYLE] (doc) # modernize-unary-static-assert [STYLE] (doc) # modernize-use-auto [STYLE] (doc) # modernize-use-bool-literals [STYLE] (doc) # MUST. modernize-use-default-member-init [STYLE] (doc) # modernize-use-emplace [STYLE] (doc) # MUST. modernize-use-equals-default [STYLE] (doc) # MUST. modernize-use-equals-delete [STYLE] (doc) # MUST. modernize-use-nodiscard [LOW] (doc) # We SHOULD follow the rule if we use C++17. modernize-use-noexcept [LOW] (doc) # modernize-use-nullptr [LOW] (doc) # MUST. modernize-use-override [LOW] (doc) # MUST. (modernize-use-trailing-return-type) [LOW] (doc) # Disabled. modernize-use-transparent-functors [LOW] (doc) # modernize-use-uncaught-exceptions [STYLE] (doc) # modernize-use-using [STYLE] (doc) # MUST. openmp-* # openmp-use-default-none [undefined] (doc) # performance-* # We SHOULD fix the code problems detected by performance-* rules if these are not false-positives. performance-faster-string-find [LOW] (doc) # MUST. performance-for-range-copy [LOW] (doc) # performance-implicit-conversion-in-loop [LOW] (doc) # performance-inefficient-algorithm [MEDIUM] (doc) # MUST. performance-inefficient-string-concatenation [LOW] (doc) # performance-inefficient-vector-operation [LOW] (doc) # performance-move-const-arg [MEDIUM] (doc) # performance-move-constructor-init [MEDIUM] (doc) # performance-no-automatic-move [LOW] (doc) # performance-no-int-to-ptr [LOW] (doc) # performance-noexcept-move-constructor [MEDIUM] (doc) # performance-trivially-destructible [LOW] (doc) # performance-type-promotion-in-math-fn [LOW] (doc) # performance-unnecessary-copy-initialization [LOW] (doc) # performance-unnecessary-value-param [LOW] (doc) # We MAY ignore the warning safely if Ptr, SharedPtr, and ConstSharedPtr are pointed out as follows (which is currently suppressed by the AllowedTypes option in .clang-tidy). src/autoware/AutowareArchitectureProposal.iv/planning/scenario_planning/lane_driving/motion_planning/obstacle_stop_planner/src/adaptive_cruise_control.cpp:176:58: warning: the const qualified parameter 'current_velocity_ptr' is copied for each invocation ; consider making it a reference [ performance-unnecessary-value-param ] const geometry_msgs::msg::TwistStamped::ConstSharedPtr current_velocity_ptr, bool _ need_to_stop, ^ & portability-* # portability-simd-intrinsics [STYLE] (doc) # readability-* # We SHOULD fix the code problems detected by bugprone-* rules if these are not false-positives. (readability-avoid-const-params-in-decls) [STYLE] (doc) # Disabled. (readability-braces-around-statements) [STYLE] (doc) # Disabled. readability-const-return-type [LOW] (doc) # readability-container-size-empty [STYLE] (doc) # readability-convert-member-functions-to-static [STYLE] (doc) # Optional. It depends on the design of the class. We MAY ignore this warning safely. readability-delete-null-pointer [STYLE] (doc) # readability-else-after-return [STYLE] (doc) # readability-function-cognitive-complexity [STYLE] (doc) # (readability-function-size) [STYLE] (doc) # (readability-identifier-length) [STYLE] (doc) # To be added. (readability-identifier-naming) [STYLE] (doc) # To be added. (readability-implicit-bool-conversion) [STYLE] (doc) # readability-inconsistent-declaration-parameter-name [STYLE] (doc) # readability-isolate-declaration [STYLE] (doc) # (readability-magic-numbers) [STYLE] (doc) # Disabled because of annoyance. readability-make-member-function-const [STYLE] (doc) # MUST. readability-misleading-indentation [LOW] (doc) # MUST. readability-misplaced-array-index [STYLE] (doc) # MUST. (readability-named-parameter) [STYLE] (doc) # readability-non-const-parameter [STYLE] (doc) # (readability-qualified-auto) [LOW] (doc) # To be added. readability-redundant-access-specifiers [STYLE] (doc) # readability-redundant-control-flow [STYLE] (doc) # readability-redundant-declaration [STYLE] (doc) # readability-redundant-function-ptr-dereference [STYLE] (doc) # readability-redundant-member-init [STYLE] (doc) # (readability-redundant-preprocessor) [STYLE] (doc) # readability-redundant-smartptr-get [STYLE] (doc) # readability-redundant-string-cstr [STYLE] (doc) # readability-redundant-string-init [STYLE] (doc) # readability-simplify-boolean-expr [MEDIUM] (doc) # readability-simplify-subscript-expr [STYLE] (doc) # readability-static-accessed-through-instance [STYLE] (doc) # readability-static-definition-in-anonymous-namespace [STYLE] (doc) # readability-string-compare [LOW] (doc) # MUST. (readability-suspicious-call-argument) [LOW] (doc) # Disabled. readability-uniqueptr-delete-release [STYLE] (doc) # (readability-uppercase-literal-suffix) [STYLE] (doc) # Disabled temporarily. Will be enabled to merge the code to Autoware.Auto. readability-use-anyofallof [STYLE] (doc) # Optional. We MAY ignore the warning safely.","title":"Clang-Tidy guideline"},{"location":"developer_guide/ClangTidyGuideline/#clang-tidy-guidelines","text":"Weekly Clang-Tidy analysis report for main branch is here (updated every Sunday). Information: Due to the lack of the header files of CUDA or NVML, Perception modules and NVML GPU Monitor can not be analyzed correctly by clang-tidy-pr in autoware.iv repository. You may find some clang-errors and false positives. This document follows the convention of RFC2119 .","title":"Clang-Tidy Guidelines"},{"location":"developer_guide/ClangTidyGuideline/#how-to-use-clang-tidy-on-your-local-machine","text":"First, install Clang-Tidy. # Ubuntu/Debian sudo apt install clang-tidy Next, build your package with -DCMAKE_EXPORT_COMPILE_COMMANDS=ON option. cd ~/autoware.proj colcon build --cmake-args -DCMAKE_BUILD_TYPE = Release -DCMAKE_EXPORT_COMPILE_COMMANDS = ON Finally, run ./scripts/clang-tidy-package.sh . ./scripts/clang-tidy-package.sh <path-to-your-package> For example: ./scripts/clang-tidy-package.sh src/autoware/autoware.iv/common/util/autoware_utils/ Or, execute Clang-Tidy directly. clang-tidy -p build/compile_commands.json <your-source-code>","title":"How to use Clang-Tidy on your local machine"},{"location":"developer_guide/ClangTidyGuideline/#severity-level","text":"The severity level of check rules are defined by CodeChecker, not by Clang-Tidy itself. The level definitions of rules can be found here .","title":"Severity Level"},{"location":"developer_guide/ClangTidyGuideline/#high","text":"We MUST fix the code problems pointed out by Clang-Tidy.","title":"High"},{"location":"developer_guide/ClangTidyGuideline/#middle","text":"We SHOULD fix the code problems pointed out by Clang-Tidy.","title":"Middle"},{"location":"developer_guide/ClangTidyGuideline/#low","text":"We MAY fix the code problems pointed out by Clang-Tidy. But some rules SHOULD NOT be ignored. See Rules section.","title":"Low"},{"location":"developer_guide/ClangTidyGuideline/#style","text":"We MAY fix the code problems pointed out by Clang-Tidy. But some rules SHOULD NOT be ignored. See Rules section.","title":"Style"},{"location":"developer_guide/ClangTidyGuideline/#rules","text":"Some rules are disabled by default to prevent false-positives that are annoying.","title":"Rules"},{"location":"developer_guide/ClangTidyGuideline/#clang-diagnostic-","text":"We MUST fix the code problems detected by clang-diagnostic-* rules. These rules come from Clang++ compile-time errors and warnings.","title":"clang-diagnostic-*"},{"location":"developer_guide/ClangTidyGuideline/#boost-","text":"We MUST fix the code problems detected by boost-_ rules if these are not false-positives.","title":"boost-*"},{"location":"developer_guide/ClangTidyGuideline/#boost-use-to-string-low-doc","text":"","title":"boost-use-to-string [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-","text":"We SHOULD fix the code problems detected by bugprone-* rules if these are not false-positives.","title":"bugprone-*"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-argument-comment-low-doc","text":"","title":"bugprone-argument-comment [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-assert-side-effect-medium-doc","text":"","title":"bugprone-assert-side-effect [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-bad-signal-to-kill-thread-medium-doc","text":"","title":"bugprone-bad-signal-to-kill-thread [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-bool-pointer-implicit-conversion-low-doc","text":"","title":"bugprone-bool-pointer-implicit-conversion [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-branch-clone-low-doc","text":"Sometimes the clones are intentional as follows. We MAY ignore safely. (Note that the following code violates the modernize-make-unique rule) } else if ( type == autoware_perception_msgs :: msg :: Semantic :: MOTORBIKE ) { model_ptr . reset ( new normal :: BoundingBoxModel ); } else if ( type == autoware_perception_msgs :: msg :: Semantic :: BICYCLE ) { model_ptr . reset ( new normal :: BoundingBoxModel ); }","title":"bugprone-branch-clone [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-copy-constructor-init-medium-doc","text":"","title":"bugprone-copy-constructor-init [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-dangling-handle-high-doc","text":"","title":"bugprone-dangling-handle [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-dynamic-static-initializers-medium-doc","text":"","title":"bugprone-dynamic-static-initializers [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-exception-escape-medium-doc","text":"","title":"bugprone-exception-escape [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-fold-init-type-high-doc","text":"","title":"bugprone-fold-init-type [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-forward-declaration-namespace-low-doc","text":"","title":"bugprone-forward-declaration-namespace [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-forwarding-reference-overload-low-doc","text":"","title":"bugprone-forwarding-reference-overload [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-inaccurate-erase-high-doc","text":"","title":"bugprone-inaccurate-erase [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-incorrect-roundings-high-doc","text":"","title":"bugprone-incorrect-roundings [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-infinite-loop-medium-doc","text":"Disabled because the rule falls into the infinite-loop when checking some ROS functions, such as spin() .","title":"(bugprone-infinite-loop) [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-integer-division-medium-doc","text":"","title":"bugprone-integer-division [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-lambda-function-name-low-doc","text":"","title":"bugprone-lambda-function-name [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-macro-parentheses-medium-doc","text":"","title":"bugprone-macro-parentheses [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-macro-repeated-side-effects-medium-doc","text":"","title":"bugprone-macro-repeated-side-effects [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-misplaced-operator-in-strlen-in-alloc-medium-doc","text":"","title":"bugprone-misplaced-operator-in-strlen-in-alloc [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-misplaced-pointer-arithmetic-in-alloc-medium-doc","text":"Disabled.","title":"(bugprone-misplaced-pointer-arithmetic-in-alloc) [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-misplaced-widening-cast-high-doc","text":"","title":"bugprone-misplaced-widening-cast [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-move-forwarding-reference-medium-doc","text":"","title":"bugprone-move-forwarding-reference [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-multiple-statement-macro-medium-doc","text":"","title":"bugprone-multiple-statement-macro [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-no-escape-undefined-doc","text":"To be added to .clang-tidy.","title":"(bugprone-no-escape) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-not-null-terminated-result-medium-doc","text":"","title":"bugprone-not-null-terminated-result [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-parent-virtual-call-medium-doc","text":"","title":"bugprone-parent-virtual-call [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-posix-return-undefined-doc","text":"","title":"bugprone-posix-return [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-redundant-branch-condition-low-doc","text":"Disabled.","title":"(bugprone-redundant-branch-condition) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-reserved-identifier-low-doc","text":"Disabled because ROS 2 coding naming style for inclusion guards violates the rule by default.","title":"(bugprone-reserved-identifier) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-signal-handler-undefined-doc","text":"To be added to .clang-tidy.","title":"(bugprone-signal-handler) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-signed-char-misuse-medium-doc","text":"","title":"bugprone-signed-char-misuse [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-sizeof-container-high-doc","text":"","title":"bugprone-sizeof-container [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-sizeof-expression-high-doc","text":"","title":"bugprone-sizeof-expression [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-spuriously-wake-up-functions-undefined-doc","text":"","title":"(bugprone-spuriously-wake-up-functions) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-string-constructor-high-doc","text":"","title":"bugprone-string-constructor [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-string-integer-assignment-low-doc","text":"","title":"bugprone-string-integer-assignment [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-string-literal-with-embedded-nul-medium-doc","text":"","title":"bugprone-string-literal-with-embedded-nul [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-enum-usage-high-doc","text":"","title":"bugprone-suspicious-enum-usage [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-include-low-doc","text":"Disabled.","title":"(bugprone-suspicious-include) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-memset-usage-high-doc","text":"","title":"bugprone-suspicious-memset-usage [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-missing-comma-high-doc","text":"","title":"bugprone-suspicious-missing-comma [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-semicolon-high-doc","text":"","title":"bugprone-suspicious-semicolon [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-suspicious-string-compare-medium-doc","text":"","title":"bugprone-suspicious-string-compare [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-swapped-arguments-high-doc","text":"","title":"bugprone-swapped-arguments [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-terminating-continue-medium-doc","text":"","title":"bugprone-terminating-continue [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-throw-keyword-missing-medium-doc","text":"","title":"bugprone-throw-keyword-missing [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-too-small-loop-variable-medium-doc","text":"","title":"bugprone-too-small-loop-variable [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-undefined-memory-manipulation-medium-doc","text":"","title":"bugprone-undefined-memory-manipulation [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-undelegated-constructor-medium-doc","text":"","title":"bugprone-undelegated-constructor [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-unhandled-exception-at-new-undefined-doc","text":"To be added.","title":"(bugprone-unhandled-exception-at-new) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-unhandled-self-assignment-medium-doc","text":"","title":"bugprone-unhandled-self-assignment [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-unused-raii-high-doc","text":"","title":"bugprone-unused-raii [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-unused-return-value-medium-doc","text":"","title":"bugprone-unused-return-value [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-use-after-move-high-doc","text":"","title":"bugprone-use-after-move [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#bugprone-virtual-near-miss-medium-doc","text":"","title":"bugprone-virtual-near-miss [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-","text":"We SHOULD fix the code problems detected by cppcoreguidelines-* rules if these are not false-positives.","title":"cppcoreguidelines-*"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-avoid-goto-style-doc","text":"","title":"cppcoreguidelines-avoid-goto [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-avoid-non-const-global-variables-low-doc","text":"Disabled.","title":"(cppcoreguidelines-avoid-non-const-global-variables) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-init-variables-medium-doc","text":"","title":"cppcoreguidelines-init-variables [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-interfaces-global-init-low-doc","text":"","title":"cppcoreguidelines-interfaces-global-init [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-macro-usage-low-doc","text":"","title":"cppcoreguidelines-macro-usage [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-narrowing-conversions-medium-doc","text":"","title":"cppcoreguidelines-narrowing-conversions [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-no-malloc-low-doc","text":"","title":"cppcoreguidelines-no-malloc [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-owning-memory-style-doc","text":"Disabled.","title":"(cppcoreguidelines-owning-memory) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-prefer-member-initializer-style-doc","text":"Disabled.","title":"cppcoreguidelines-prefer-member-initializer [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-bounds-array-to-pointer-decay-low-doc","text":"Disabled.","title":"(cppcoreguidelines-pro-bounds-array-to-pointer-decay) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-bounds-constant-array-index-low-doc","text":"Disabled.","title":"(cppcoreguidelines-pro-bounds-constant-array-index) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-bounds-pointer-arithmetic-low-doc","text":"","title":"cppcoreguidelines-pro-bounds-pointer-arithmetic [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-const-cast-low-doc","text":"","title":"cppcoreguidelines-pro-type-const-cast [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-cstyle-cast-low-doc","text":"","title":"cppcoreguidelines-pro-type-cstyle-cast [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-member-init-low-doc","text":"","title":"cppcoreguidelines-pro-type-member-init [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-reinterpret-cast-low-doc","text":"","title":"cppcoreguidelines-pro-type-reinterpret-cast [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-static-cast-downcast-low-doc","text":"","title":"cppcoreguidelines-pro-type-static-cast-downcast [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-union-access-low-doc","text":"","title":"cppcoreguidelines-pro-type-union-access [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-pro-type-vararg-low-doc","text":"Disabled.","title":"(cppcoreguidelines-pro-type-vararg) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-slicing-low-doc","text":"","title":"cppcoreguidelines-slicing [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#cppcoreguidelines-special-member-functions-low-doc","text":"","title":"cppcoreguidelines-special-member-functions [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-","text":"We SHOULD fix the code problems detected by google-* rules if these are not false-positives.","title":"google-*"},{"location":"developer_guide/ClangTidyGuideline/#google-build-explicit-make-pair-medium-doc","text":"","title":"google-build-explicit-make-pair [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-build-namespaces-medium-doc","text":"","title":"google-build-namespaces [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-build-using-namespace-style-doc","text":"","title":"google-build-using-namespace [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-default-arguments-low-doc","text":"Disabled.","title":"(google-default-arguments) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-explicit-constructor-medium-doc","text":"","title":"google-explicit-constructor [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-global-names-in-headers-style-doc","text":"","title":"google-global-names-in-headers [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-objc-avoid-nsobject-new-undefined-doc","text":"Disabled.","title":"(google-objc-avoid-nsobject-new) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-objc-avoid-throwing-exception-undefined-doc","text":"Disabled.","title":"(google-objc-avoid-throwing-exception) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-objc-function-naming-undefined-doc","text":"Disabled.","title":"(google-objc-function-naming) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-objc-global-variable-declaration-undefined-doc","text":"Disabled.","title":"(google-objc-global-variable-declaration) [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-readability-avoid-underscore-in-googletest-name-style-doc","text":"","title":"(google-readability-avoid-underscore-in-googletest-name) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-readability-casting-low-doc","text":"Disabled.","title":"(google-readability-casting) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-readability-todo-style-doc","text":"Disabled.","title":"(google-readability-todo) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-runtime-int-low-doc","text":"Disabled.","title":"(google-runtime-int) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-runtime-operator-medium-doc","text":"Disabled.","title":"(google-runtime-operator) [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#google-upgrade-googletest-case-style-doc","text":"","title":"google-upgrade-googletest-case [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#hicpp-","text":"We SHOULD fix the code problems detected by hicpp-* rules if these are not false-positives.","title":"hicpp-*"},{"location":"developer_guide/ClangTidyGuideline/#hicpp-exception-baseclass-low-doc","text":"","title":"hicpp-exception-baseclass [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#hicpp-multiway-paths-covered-style-doc","text":"","title":"hicpp-multiway-paths-covered [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#hicpp-no-assembler-low-doc","text":"","title":"hicpp-no-assembler [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#hicpp-signed-bitwise-low-doc","text":"","title":"hicpp-signed-bitwise [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#llvm-","text":"We MUST fix the code problems detected by llvm-* rules if these are not false-positives.","title":"llvm-*"},{"location":"developer_guide/ClangTidyGuideline/#llvm-namespace-comment-low-doc","text":"","title":"llvm-namespace-comment [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-","text":"We SHOULD fix the code problems detected by misc-* rules if these are not false-positives.","title":"misc-*"},{"location":"developer_guide/ClangTidyGuideline/#misc-definitions-in-headers-medium-doc","text":"","title":"misc-definitions-in-headers [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-misplaced-const-low-doc","text":"","title":"misc-misplaced-const [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-new-delete-overloads-medium-doc","text":"","title":"misc-new-delete-overloads [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-no-recursion-low-doc","text":"","title":"misc-no-recursion [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-non-copyable-objects-high-doc","text":"","title":"misc-non-copyable-objects [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-non-private-member-variables-in-classes-low-doc","text":"Disabled because of annoyance.","title":"(misc-non-private-member-variables-in-classes) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-redundant-expression-medium-doc","text":"MUST.","title":"misc-redundant-expression [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-static-assert-low-doc","text":"","title":"misc-static-assert [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-throw-by-value-catch-by-reference-high-doc","text":"MUST.","title":"misc-throw-by-value-catch-by-reference [HIGH] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-unconventional-assign-operator-medium-doc","text":"","title":"misc-unconventional-assign-operator [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-uniqueptr-reset-release-medium-doc","text":"","title":"misc-uniqueptr-reset-release [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-unused-alias-decls-low-doc","text":"MUST.","title":"misc-unused-alias-decls [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-unused-parameters-low-doc","text":"MUST.","title":"misc-unused-parameters [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#misc-unused-using-decls-low-doc","text":"MUST.","title":"misc-unused-using-decls [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-","text":"We SHOULD fix the code problems detected by modernize-* rules if these are not false-positives.","title":"modernize-*"},{"location":"developer_guide/ClangTidyGuideline/#modernize-avoid-bind-style-doc","text":"Disabled because of incompatibility with ROS.","title":"(modernize-avoid-bind) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-avoid-c-arrays-low-doc","text":"","title":"modernize-avoid-c-arrays [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-concat-nested-namespaces-style-doc","text":"","title":"modernize-concat-nested-namespaces [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-deprecated-headers-low-doc","text":"MUST.","title":"modernize-deprecated-headers [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-deprecated-ios-base-aliases-low-doc","text":"","title":"modernize-deprecated-ios-base-aliases [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-loop-convert-style-doc","text":"","title":"modernize-loop-convert [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-make-shared-low-doc","text":"","title":"modernize-make-shared [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-make-unique-low-doc","text":"","title":"modernize-make-unique [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-pass-by-value-low-doc","text":"","title":"modernize-pass-by-value [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-raw-string-literal-style-doc","text":"","title":"modernize-raw-string-literal [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-redundant-void-arg-style-doc","text":"MUST.","title":"modernize-redundant-void-arg [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-replace-auto-ptr-low-doc","text":"","title":"modernize-replace-auto-ptr [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-replace-disallow-copy-and-assign-macro-low-doc","text":"","title":"modernize-replace-disallow-copy-and-assign-macro [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-replace-random-shuffle-low-doc","text":"","title":"modernize-replace-random-shuffle [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-return-braced-init-list-style-doc","text":"","title":"modernize-return-braced-init-list [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-shrink-to-fit-style-doc","text":"","title":"modernize-shrink-to-fit [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-unary-static-assert-style-doc","text":"","title":"modernize-unary-static-assert [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-auto-style-doc","text":"","title":"modernize-use-auto [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-bool-literals-style-doc","text":"MUST.","title":"modernize-use-bool-literals [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-default-member-init-style-doc","text":"","title":"modernize-use-default-member-init [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-emplace-style-doc","text":"MUST.","title":"modernize-use-emplace [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-equals-default-style-doc","text":"MUST.","title":"modernize-use-equals-default [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-equals-delete-style-doc","text":"MUST.","title":"modernize-use-equals-delete [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-nodiscard-low-doc","text":"We SHOULD follow the rule if we use C++17.","title":"modernize-use-nodiscard [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-noexcept-low-doc","text":"","title":"modernize-use-noexcept [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-nullptr-low-doc","text":"MUST.","title":"modernize-use-nullptr [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-override-low-doc","text":"MUST.","title":"modernize-use-override [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-trailing-return-type-low-doc","text":"Disabled.","title":"(modernize-use-trailing-return-type) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-transparent-functors-low-doc","text":"","title":"modernize-use-transparent-functors [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-uncaught-exceptions-style-doc","text":"","title":"modernize-use-uncaught-exceptions [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#modernize-use-using-style-doc","text":"MUST.","title":"modernize-use-using [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#openmp-","text":"","title":"openmp-*"},{"location":"developer_guide/ClangTidyGuideline/#openmp-use-default-none-undefined-doc","text":"","title":"openmp-use-default-none [undefined] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-","text":"We SHOULD fix the code problems detected by performance-* rules if these are not false-positives.","title":"performance-*"},{"location":"developer_guide/ClangTidyGuideline/#performance-faster-string-find-low-doc","text":"MUST.","title":"performance-faster-string-find [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-for-range-copy-low-doc","text":"","title":"performance-for-range-copy [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-implicit-conversion-in-loop-low-doc","text":"","title":"performance-implicit-conversion-in-loop [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-inefficient-algorithm-medium-doc","text":"MUST.","title":"performance-inefficient-algorithm [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-inefficient-string-concatenation-low-doc","text":"","title":"performance-inefficient-string-concatenation [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-inefficient-vector-operation-low-doc","text":"","title":"performance-inefficient-vector-operation [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-move-const-arg-medium-doc","text":"","title":"performance-move-const-arg [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-move-constructor-init-medium-doc","text":"","title":"performance-move-constructor-init [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-no-automatic-move-low-doc","text":"","title":"performance-no-automatic-move [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-no-int-to-ptr-low-doc","text":"","title":"performance-no-int-to-ptr [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-noexcept-move-constructor-medium-doc","text":"","title":"performance-noexcept-move-constructor [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-trivially-destructible-low-doc","text":"","title":"performance-trivially-destructible [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-type-promotion-in-math-fn-low-doc","text":"","title":"performance-type-promotion-in-math-fn [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-unnecessary-copy-initialization-low-doc","text":"","title":"performance-unnecessary-copy-initialization [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#performance-unnecessary-value-param-low-doc","text":"We MAY ignore the warning safely if Ptr, SharedPtr, and ConstSharedPtr are pointed out as follows (which is currently suppressed by the AllowedTypes option in .clang-tidy). src/autoware/AutowareArchitectureProposal.iv/planning/scenario_planning/lane_driving/motion_planning/obstacle_stop_planner/src/adaptive_cruise_control.cpp:176:58: warning: the const qualified parameter 'current_velocity_ptr' is copied for each invocation ; consider making it a reference [ performance-unnecessary-value-param ] const geometry_msgs::msg::TwistStamped::ConstSharedPtr current_velocity_ptr, bool _ need_to_stop, ^ &","title":"performance-unnecessary-value-param [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#portability-","text":"","title":"portability-*"},{"location":"developer_guide/ClangTidyGuideline/#portability-simd-intrinsics-style-doc","text":"","title":"portability-simd-intrinsics [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-","text":"We SHOULD fix the code problems detected by bugprone-* rules if these are not false-positives.","title":"readability-*"},{"location":"developer_guide/ClangTidyGuideline/#readability-avoid-const-params-in-decls-style-doc","text":"Disabled.","title":"(readability-avoid-const-params-in-decls) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-braces-around-statements-style-doc","text":"Disabled.","title":"(readability-braces-around-statements) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-const-return-type-low-doc","text":"","title":"readability-const-return-type [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-container-size-empty-style-doc","text":"","title":"readability-container-size-empty [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-convert-member-functions-to-static-style-doc","text":"Optional. It depends on the design of the class. We MAY ignore this warning safely.","title":"readability-convert-member-functions-to-static [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-delete-null-pointer-style-doc","text":"","title":"readability-delete-null-pointer [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-else-after-return-style-doc","text":"","title":"readability-else-after-return [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-function-cognitive-complexity-style-doc","text":"","title":"readability-function-cognitive-complexity [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-function-size-style-doc","text":"","title":"(readability-function-size) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-identifier-length-style-doc","text":"To be added.","title":"(readability-identifier-length) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-identifier-naming-style-doc","text":"To be added.","title":"(readability-identifier-naming) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-implicit-bool-conversion-style-doc","text":"","title":"(readability-implicit-bool-conversion) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-inconsistent-declaration-parameter-name-style-doc","text":"","title":"readability-inconsistent-declaration-parameter-name [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-isolate-declaration-style-doc","text":"","title":"readability-isolate-declaration [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-magic-numbers-style-doc","text":"Disabled because of annoyance.","title":"(readability-magic-numbers) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-make-member-function-const-style-doc","text":"MUST.","title":"readability-make-member-function-const [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-misleading-indentation-low-doc","text":"MUST.","title":"readability-misleading-indentation [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-misplaced-array-index-style-doc","text":"MUST.","title":"readability-misplaced-array-index [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-named-parameter-style-doc","text":"","title":"(readability-named-parameter) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-non-const-parameter-style-doc","text":"","title":"readability-non-const-parameter [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-qualified-auto-low-doc","text":"To be added.","title":"(readability-qualified-auto) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-access-specifiers-style-doc","text":"","title":"readability-redundant-access-specifiers [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-control-flow-style-doc","text":"","title":"readability-redundant-control-flow [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-declaration-style-doc","text":"","title":"readability-redundant-declaration [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-function-ptr-dereference-style-doc","text":"","title":"readability-redundant-function-ptr-dereference [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-member-init-style-doc","text":"","title":"readability-redundant-member-init [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-preprocessor-style-doc","text":"","title":"(readability-redundant-preprocessor) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-smartptr-get-style-doc","text":"","title":"readability-redundant-smartptr-get [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-string-cstr-style-doc","text":"","title":"readability-redundant-string-cstr [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-redundant-string-init-style-doc","text":"","title":"readability-redundant-string-init [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-simplify-boolean-expr-medium-doc","text":"","title":"readability-simplify-boolean-expr [MEDIUM] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-simplify-subscript-expr-style-doc","text":"","title":"readability-simplify-subscript-expr [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-static-accessed-through-instance-style-doc","text":"","title":"readability-static-accessed-through-instance [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-static-definition-in-anonymous-namespace-style-doc","text":"","title":"readability-static-definition-in-anonymous-namespace [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-string-compare-low-doc","text":"MUST.","title":"readability-string-compare [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-suspicious-call-argument-low-doc","text":"Disabled.","title":"(readability-suspicious-call-argument) [LOW] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-uniqueptr-delete-release-style-doc","text":"","title":"readability-uniqueptr-delete-release [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-uppercase-literal-suffix-style-doc","text":"Disabled temporarily. Will be enabled to merge the code to Autoware.Auto.","title":"(readability-uppercase-literal-suffix) [STYLE] (doc)"},{"location":"developer_guide/ClangTidyGuideline/#readability-use-anyofallof-style-doc","text":"Optional. We MAY ignore the warning safely.","title":"readability-use-anyofallof [STYLE] (doc)"},{"location":"developer_guide/CodingGuideline/","text":"TierIV Coding Guideline # See confluence page","title":"Coding guideline"},{"location":"developer_guide/CodingGuideline/#tieriv-coding-guideline","text":"See confluence page","title":"TierIV Coding Guideline"},{"location":"developer_guide/PullRequestGuideline/","text":"TierIV Pull Request Guideline # See confluence page","title":"Pull request guideline"},{"location":"developer_guide/PullRequestGuideline/#tieriv-pull-request-guideline","text":"See confluence page","title":"TierIV Pull Request Guideline"},{"location":"developer_guide/UnitTestGuideline/","text":"TierIV Unit Test Guideline # How to run unit tests # You may modify the code before running unit tests. This is how to rebuild the package you modified. colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release --packages-up-to <package name> package name must be the name of a package that contains package.xml Use the commands below to run unit tests. --packages-skip-test-passed can be used to ignore test cases that already passed. source install/setup.bash colcon test --event-handlers console_cohesion+ --packages-skip-test-passed If you would like to run unit tests in a specific directory, you can use the --base-paths option. For example, if you would like to run unit tests in autoware.iv, you can run colcon test --event-handlers console_cohesion+ --base-paths src/autoware/autoware.iv --packages-skip-test-passed How to write unit tests # See confluence page for the details.","title":"Unit test guideline"},{"location":"developer_guide/UnitTestGuideline/#tieriv-unit-test-guideline","text":"","title":"TierIV Unit Test Guideline"},{"location":"developer_guide/UnitTestGuideline/#how-to-run-unit-tests","text":"You may modify the code before running unit tests. This is how to rebuild the package you modified. colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release --packages-up-to <package name> package name must be the name of a package that contains package.xml Use the commands below to run unit tests. --packages-skip-test-passed can be used to ignore test cases that already passed. source install/setup.bash colcon test --event-handlers console_cohesion+ --packages-skip-test-passed If you would like to run unit tests in a specific directory, you can use the --base-paths option. For example, if you would like to run unit tests in autoware.iv, you can run colcon test --event-handlers console_cohesion+ --base-paths src/autoware/autoware.iv --packages-skip-test-passed","title":"How to run unit tests"},{"location":"developer_guide/UnitTestGuideline/#how-to-write-unit-tests","text":"See confluence page for the details.","title":"How to write unit tests"},{"location":"developer_guide/knowhow/Calibration/","text":"Calibration in Autoware # Sensor Calibration # Please see Sensor Calibration Tools .","title":"Calibration in Autoware"},{"location":"developer_guide/knowhow/Calibration/#calibration-in-autoware","text":"","title":"Calibration in Autoware"},{"location":"developer_guide/knowhow/Calibration/#sensor-calibration","text":"Please see Sensor Calibration Tools .","title":"Sensor Calibration"},{"location":"developer_guide/knowhow/PortingToROS2/","text":"Porting ROS1 code to ROS2 # Setting up the environment # Do the following to setup and start ADE environment Setup ade home cd ~ mkdir ade-home cd ade-home touch .adehome Setup AutowareArchitectureProposal git clone https://github.com/tier4/AutowareArchitectureProposal cd AutowareArchitectureProposal git checkout ros2 enter ADE cd ~/ade-home/AutowareArchitectureProposal ade start --update --enter cd AutowareArchitectureProposal All commands that follow are to be entered in ADE. Next step is to fetch the sub-repos: cd ~/AutowareArchitectureProposal mkdir src vcs import src < autoware.proj.repos rosdep update rosdep install -y --from-paths src --ignore-src --rosdistro foxy colcon build --event-handlers console_cohesion+ For instance, the shift_decider package is in the repository github.com:tier4/pilot.auto.git , which is now in the autoware/pilot.auto subdirectory. Now branch off ros2 inside that subdirectory and delete the COLCON_IGNORE file in the package you want to port. Important changes # The best source on migrating is the migration guide . It doesn't mention everything though, so this section lists some areas with important changes. A good general strategy is to try to implement those changes, then iteratively run colcon build --packages-up-to <your_package> and fix the first compiler error. Rewriting package.xml # The migration guide covers this well. See also here for a reference of the most recent version of this format. When to use which dependency tag # Any build tool needed only to set up the build needs buildtool_depend ; e.g., <buildtool_depend> ament_cmake </buildtool_depend> <buildtool_depend> rosidl_default_generators </buildtool_depend> Any external package included with #include in the files (source or headers) needs to have a corresponding <build_depend> ; e.g., <build_depend> logging </build_depend> Any external package included with #include in the header files also needs to have a corresponding <build_export_depend> ; e.g., <build_export_depend> eigen </build_export_depend> Any shared library that needs to be linked when the code is executed needs to have a corresponding <exec_depend> : this describes the runtime dependencies; e.g., <exec_depend> std_msgs </exec_depend> <exec_depend> rosidl_default_runtime </exec_depend> If a package falls under all three categories ( <build_depend> , <build_export_depend> , and <exec_depend> ), it is possible to just use <depend> <depend> shift_decider </depend> Rewriting CMakeLists.txt # This is not always straightforward. A starting point is to look at the pub-sub tutorial . This uses ament_cmake , which has a relatively good guide that still doesn't cover everything (like what to do when installing an executable). ament_cmake and ament_cmake_auto # One drawback of ament_cmake is that it requires typing out the dependencies at least twice, once in package.xml and once or more in CMakeLists.txt . Another possibility is to use ament_auto to get terse CMakeLists.txt . See this commit for an example. Unfortunately, there is no documentation for this tool, so you can only learn it from examples and reading the source code. It is also limited in what it does \u2013 it cannot currently generate message definitions, for instance, and always links all dependencies to all targets. There are more subtle issues too, like ament_auto_find_build_dependencies() . It just takes all the build dependencies verbatim from package.xml and calls find_package() without the REQUIRED option. This causes an issue when your library has a different name in package.xml / rosdep and in CMake . It also removes safety, as this won't complain when you mistype a package name or haven't installed the package yet. For these reasons, it's recommended to do the following: # Mark all packages as REQUIRED ament_auto_find_build_dependencies ( REQUIRED ${ ${PROJECT_NAME } _BUILD_DEPENDS} ${ ${PROJECT_NAME } _BUILDTOOL_DEPENDS} ) C++ standard # Add the following to ensure that a specific standard is required and extensions are not allowed if ( NOT CMAKE_CXX_STANDARD ) set ( CMAKE_CXX_STANDARD 14 ) set ( CMAKE_CXX_STANDARD_REQUIRED ON ) set ( CMAKE_CXX_EXTENSIONS OFF ) endif () Compiler flags # Make sure that flags are added only for specific compilers. Not everyone uses gcc or clang . if ( CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\" ) add_compile_options ( -Wall -Wextra -Wpedantic -Wno-unused-parameter ) endif () Linters # Add only ament_cmake_cppcheck to the list of linters in package.xml <test_depend> ament_lint_auto </test_depend> <test_depend> ament_cmake_cppcheck </test_depend> And the corresponding code in CMakeLists.txt if ( BUILD_TESTING ) find_package ( ament_lint_auto REQUIRED ) ament_lint_auto_find_test_dependencies () endif () Additionally, we use clang-tidy , in order for it to work, we need to build packages with the following: colcon build --packages-up-to <pkg_name> --cmake-args -DCMAKE_EXPORT_COMPILE_COMMANDS = 1 And then: clang-tidy -p build/compile_commands.json <path_to_pkg_source> To check the output of the linters we can just run the tests with: colcon test --packages-select <pkg_name> && colcon test-result --verbose Replacing std_msgs # In ROS2, you should define semantically meaningful wrappers around primitive (number) types. They are deprecated in Foxy. Changing the namespaces and header files for generated message types # If you follow the migration guide and change the included headers to have an extra /msg in the path and convert to snake_case , you might get a cryptic error. Turns out two files are being generated: One for C types ( .h headers) and one for CPP types ( .hpp headers). So don't forget to change .h to .hpp too. Also, don't forget to insert an additional ::msg between the package namespace and the class name. A tip: Sublime Text has a handy \"Case Conversion\" package for converting to snake case. Adapting message definitions # If your message included something like Header header , it needs to be changed to std_msgs/Header header . Otherwise you'll see messages like fatal error: autoware_api_msgs/msg/detail/header__struct.hpp: No such file or directory . Inheriting from Node instead of NodeHandle members # That's where differences start to show \u2013 I decided to make the VehicleCmdGate a Node even though the filename would suggest that vehicle_cmd_gate_node.cpp would be it. That's because it has publishers, subscribers, and logging. It previously had two NodeHandles, a public one and a private one ( \"~\" ). The public one was unused and could be removed. Private nodes are not supported in ROS2, so I simply made it public, but that is an area that needs to be brought up in review. Latched topics # For each latched publisher, you can use transient_local durability QoS on the publisher, e.g. when the history depth is 1: rclcpp :: QoS durable_qos { 1 }; durable_qos . transient_local (); or rclcpp :: QoS durable_qos = rclcpp :: QoS ( 1 ). transient_local (); However, all subscribers to that topic will also need transient_local durability. If this is omitted, the connection between the two will be negotiated to be volatile, i.e. old messages will not be delivered when the subscriber comes online. Timing issues # First, if the timer can be replaced with a data-driven pattern, it is the preferred alternative for the long term: The Problem with Timer-Driven Patterns # It is well understood that a polling or timer-driven pattern increases jitter (i.e. variance of latency). (Consider, for example: if every data processing node in a chain operates on a timer what is the best and worst case latency?) As a consequence for more timing-sensitive applications, it is generally not preferred to use a timer-driven pattern. On top of this, it is also reasonably well known that use of the clock is nondeterministic and internally this has been a large source of frustration with bad, or timing sensitive tests. Such tests typically require specific timing and/or implicitly require a certain execution order (loosely enforced by timing assumptions rather than explicitly via the code). As a whole, introducing the clock explicitly (or implicitly via timers) is problematic because it introduces additional state, and thus assumptions on the requirements for the operation of the component. Consider also leap seconds and how that might ruin the operation and/or assumptions needed for the proper operation of the component. Preferred Patterns # In general, a data-driven pattern should be preferred to a timer-driven pattern. One reasonable exception to this guideline is the state estimator/filter at the end of localization. A timer-driven pattern in this context is useful to provide smooth behavior and promote looser coupling between the planning stack and the remainder of the stack. The core idea behind a data-driven pattern is that as soon as data arrives, it should be appropriately processed. Furthermore, the system clock (or any other source of time) should not be used to manipulate data or the timestamps. This pattern is valuable since it implicitly cuts down on hidden state (being the clock), and thus simplifies assumptions needed for the node to work. For examples of this kind of pattern, see the lidar object detection stack in Autoware.Auto. By not using any mention of the clock save for in the drivers, the stack can run equivalently on bag data, simulation data, or live data. A similar pattern with multiple inputs can be seen in the MPC implementation both internally and externally. Replicating ros::Timer # Assuming you still want to replicate the existing ros::Timer functionality: There is rclcpp::WallTimer , which has a similar interface, but it's not equivalent. The wall timer uses a wall clock ( RCL_STEADY_TIME clock), i.e. it doesn't listen to the /clock topic populated by simulation time. That the timer doesn't stop when simulation time stops, and doesn't go faster/slower when simulation time goes faster or slower. By contrast, the GenericTimer provides an interface to supply a clock, but there is no convenient function for setting up such a timer, comparable to Node::create_wall_timer . For now, this works: auto timer_callback = std :: bind ( & VehicleCmdGate :: onTimer , this ); auto period = std :: chrono :: duration_cast < std :: chrono :: nanoseconds > ( std :: chrono :: duration < double > ( update_period_ )); timer_ = std :: make_shared < rclcpp :: GenericTimer < decltype ( timer_callback ) >> ( this -> get_clock (), period , std :: move ( timer_callback ), this -> get_node_base_interface () -> get_context ()); this -> get_node_timers_interface () -> add_timer ( timer_ , nullptr ); Also, this doesn't work, even with a subsequent add_timer() call: timer_ = rclcpp :: create_timer ( this , this -> get_clock (), period , timer_callback ); Rosbag recording # Unfortunately, one additional problem remains. ros2 bag does not record /clock (aka sim time) whereas rosbag does. This implies that in order to get the same behavior in ROS 2, either: rosbag along with the ros1_bridge must be used Some explicit time source must be used and explicitly recorded by ros2 bag Parameters # It's not strictly necessary, but you probably want to make sure the filename is xyz.param.yaml . Then come two steps: Adjust code # double vel_lim ; pnh_ . param < double > ( \"vel_lim\" , vel_lim , 25.0 ); becomes const double vel_lim = declare_parameter ( \"vel_lim\" , 25.0 ); which is equivalent to const double vel_lim = declare_parameter < double > ( \"vel_lim\" , 25.0 ); This allows to set the initial value e.g. via a parameter file. NOTE Calling ros2 param set <NODE> vel_lim 1.234 after starting the node works but will not alter the member vel_lim ! See the section below on dynamic reconfigure to achieve that. dynamic_reconfigure # Dynamic reconfigure as it existed in ROS1 does not exist anymore in ROS2 and can be achieved by simpler means using a parameter callback. cfg files # Remove the package's .cfg file and associated cfg/ subdirectory. Header file # In the header file, remove includes of dynamic_reconfigure and the node-specific config file. As a concrete example, take the MPC follower -#include <dynamic_reconfigure/server.h> -#include <mpc_follower/MPCFollowerConfig.h> you need to set a parameter handler and callback function: OnSetParametersCallbackHandle :: SharedPtr set_param_res_ ; rcl_interfaces :: msg :: SetParametersResult paramCallback ( const std :: vector < rclcpp :: Parameter > & parameters ); If there are many parameters (rule of thumb: more than 2), it is more practical to group them in a struct defined within the node's declaration: class MPCFollower : public rclcpp :: Node { struct MPCParam { int prediction_horizon ; ... } mpc_param ; }; Add a method to declare all the parameters void declareMPCparameters (); Implementation file # Write the following into the definition of the class that inherits from rclcpp::Node . A few macros and a utility function can help keep the following code clean and void of redundancy. These macros are optional but help when many parameters are to be updated dynamically. #define DECLARE_MPC_PARAM(PARAM_STRUCT, NAME, VALUE) \\ PARAM_STRUCT.NAME = declare_parameter(\"mpc_\" #NAME, VALUE) #define UPDATE_MPC_PARAM(PARAM_STRUCT, NAME) \\ update_param(parameters, \"mpc_\" #NAME, PARAM_STRUCT.NAME) namespace { template < typename T > void update_param ( const std :: vector < rclcpp :: Parameter > & parameters , const std :: string & name , T & value ) { auto it = std :: find_if ( parameters . cbegin (), parameters . cend (), [ & name ]( const rclcpp :: Parameter & parameter ) { return parameter . get_name () == name ; }); if ( it != parameters . cend ()) { value = it -> template get_value < T > (); } } } // namespace In the constructor, define the callback and declare parameters with default values // the type of the ROS parameter is defined by the C++ type of the default value, // so 50 is not equivalent to 50.0! DECLARE_MPC_PARAM ( mpc_param_ , prediction_horizon , 50 ); // set parameter callback set_param_res_ = add_on_set_parameters_callback ( std :: bind ( & MPCFollower :: paramCallback , this , _1 )); Inside the callback, you have to manually update each parameter for which you want to react to change from the outside. You can (inadvertently) declare more parameters than you react to. rcl_interfaces :: msg :: SetParametersResult result ; result . successful = true ; result . reason = \"success\" ; // strong exception safety wrt MPCParam MPCParam param = mpc_param_ ; try { UPDATE_MPC_PARAM ( param , prediction_horizon ); // update all other parameters, too // transaction succeeds, now assign values mpc_param_ = param ; } catch ( const rclcpp :: exceptions :: InvalidParameterTypeException & e ) { result . successful = false ; result . reason = e . what (); } return result ; When the node is running, you can set the parameter dynamically with the following command. The value is converted to a type as in C++, so setting an int to 51.0 leads to an InvalidParameterTypeException in the callback. ros2 param set /mpc_follower prediction_horizon 51 Make sure relevant parameters can be set from the command line or rqt and changes are reflected by the package. Parameter client [discouraged] # The parameter client is another way to dynamically set the parameter defined in the node. The client subscribes to the /parameter_event topic and call the callback function. This allows the client node to get all the information about parameter changes in every node. The callback argument contains the target node name, which can be used to determine which node the parameter change is for. In .hpp, rclcpp :: AsyncParametersClient :: SharedPtr param_client_ ; rclcpp :: Subscription < rcl_interfaces :: msg :: ParameterEvent >:: SharedPtr sub_param_event_ ; void paramCallback ( const rcl_interfaces :: msg :: ParameterEvent :: SharedPtr event ); In .cpp, // client setting param_client_ = std :: make_shared < rclcpp :: AsyncParametersClient > ( this , \"param_client\" ); sub_param_event_ = param_client_ -> on_parameter_event ( std :: bind ( & VelocityController :: paramCallback , this , std :: placeholders :: _1 )); // callback setting void paramCallback ( const rcl_interfaces :: msg :: ParameterEvent :: SharedPtr event ) { for ( auto & new_parameter : event -> new_parameters ) { std :: cout << \" \" << new_parameter . name << std :: endl ; } for ( auto & changed_parameter : event -> changed_parameters ) { std :: cout << \" \" << changed_parameter . name << std :: endl ; } for ( auto & deleted_parameter : event -> deleted_parameters ) { std :: cout << \" \" << deleted_parameter . name << std :: endl ; } }; However, this method calls the callback for all parameter changes of all nodes. So the add_on_set_parameters_callback is recommended for the porting of the dynamic reconfigure. reference: https://discourse.ros.org/t/composition-and-parameters-best-practice-suggestions/1001 https://github.com/ros2/rclcpp/issues/243 Adjust param file # Two levels of hierarchy need to be added around the parameters themselves and each level has to be indented relative to its parent (by two spaces in this example): <node name or /** > : ros__parameters: <params> Types # Also, ROS1 didn't have a problem when you specify an integer, e.g. 28 for a double parameter, but ROS2 does: [ vehicle_cmd_gate-1 ] terminate called after throwing an instance of 'rclcpp::exceptions::InvalidParameterTypeException' [ vehicle_cmd_gate-1 ] what () : parameter 'vel_lim' has invalid type: expected [ double ] got [ integer ] Best to just change 28 to 28.0 in the param file. See also this issue . Launch file # There is a migration guide . One thing it doesn't mention is that the .launch file also needs to be renamed to .launch.xml . Replacing tf2_ros::Buffer # A tf2_ros::Buffer member that is filled by a tf2_ros::TransformListener can become a tf2::BufferCore in most cases. This reduces porting effort, since the a tf2::BufferCore can be constructed like a ROS1 tf2_ros::Buffer . For an example, see this PR . However, in some cases the extra functionality of tf2_ros::Buffer is needed. For instance, waiting for a transform to arrive, usually in the form of lookupTransform() with a timeout argument. Avoiding a lookup with timeout # Often, code doesn't really need a transform lookup with timeout. For instance, this package has a \"main\" subscription callback, onTrigger() , that waits for the most recent transform ( tf2::TimePointZero ), then checks if auxiliary data from other subscriptions is there and returns early from the callback if it isn't. In that case, I think the callback can simply treat transforms the same way as this auxiliary data, i.e. just do a simple lookupTransform() with no timeout and return early from the callback if it fails. The node won't do any work anyway until it's ready (i.e. has all the auxiliary data). Note that this pattern works only when the node is waiting for the most recent transform \u2013 if your callback wants to use the transform at a specific time, e.g. the timestamp of the message that triggered the callback, this pattern doesn't make sense. In that case, avoiding waitForTransform() requires refactoring the architecture of your system, but that topic is currently out of scope. It's worth keeping in mind that waiting for transforms in general can be troublesome \u2013 it is only a probabilistic solution that fails in a bad way for latency spikes, makes it hard to reason about the behavior of the whole system and probably incurs more latency than necessary. When you can't avoid a lookup with timeout # You should be able to use tf2_ros::Buffer::lookupTransform() with a timeout out of the box. There is one caveat, namely there has been at least one report of such an error: Do not call canTransform or lookupTransform with a timeout unless you are using another thread for populating data. Without a dedicated thread it will always timeout. If you have a separate thread servicing tf messages, call setUsingDedicatedThread(true) on your Buffer instance. There is also the drawback of spamming the console with warnings like Warning: Invalid frame ID \"a\" passed to canTransform argument target_frame - frame does not exist at line 133 in /tmp/binarydeb/ros-foxy-tf2-0.13.6/src/buffer_core.cpp if the frame has never been sent before. What about waitForTransform() ? # There is also a waitForTransform API involving futures, but it has bugs and limitations. In particular: Its callback does not get called when the request can be answered immediately You can not call get() on the future and expect it to return the transform or throw an exception when the timeout ends. It continues waiting after the timeout expires if the future is not ready yet (i.e. the transform hasn't arrived). This limits you to the following style of calling the API, which doesn't use the callback and limits the waiting time before calling get() on the future: // In the node definition tf2_ros :: Buffer tf_buffer_ ; tf2_ros :: TransformListener tf_listener_ ; ... // In the constructor auto cti = std :: make_shared < tf2_ros :: CreateTimerROS > ( this -> get_node_base_interface (), this -> get_node_timers_interface ()); tf_buffer_ . setCreateTimerInterface ( cti ); ... // In the function processing data auto tf_future = tf_buffer_ . waitForTransform ( a , b , msg_time , std :: chrono :: milliseconds ( 0 ), [ this ]( auto ){}); auto status = tf_future . wait_for ( timeout_ ); if ( status == std :: future_status :: deferred ) { // This never happened in experiments } else if ( status == std :: future_status :: timeout ) { // The transform did not arrive within the timeout duration } else { // The transform is here, and can now be accessed without triggering the waiting-infinitely bug auto transform = tf_future . get (); } The waitForTransform() function will return immediately. Note that the timeout passed to waitForTransform() does not matter, only the timeout passed to wait_for() . There is a bug with this when tf2::TimeStampZero is requested instead of a nonzero time: the status will be ready , but accessing the result with get() throws an exception. There is another bug where this bug is not triggered under some conditions, for extra fun. So do not use waitForTransform() with tf2::TimeStampZero (or any other way of saying \"time 0\"). The silver lining is that this can be often avoided anyway since it is the scenario described in the section before. For more details, see https://github.com/nnmm/tf2_example and the resulting table . Shared pointers # Be careful in creating a std::shared_ptr to avoid a double-free situation when the constructor argument after porting is a dereferenced shared pointer. For example, if msg previously was a raw pointer and now is a shared pointer, the following would lead to both msg and a deleting the same resource. auto a = std :: make_shared < autoware_planning_msgs :: Trajectory > ( * msg ); To avoid this, just copy the shared pointer std :: shared_ptr < autoware_planning_msgs :: msg :: Trajectory > a = msg ; Service clients # There is no synchronous API for service calls, and the futures API can not be used from inside a node, only the callback API. The futures API is what is used in tutorials such as Writing a simple service and client , but note that the call to rclcpp::spin_until_future_complete() does not happen from inside any subscriber callback or similar. If you do call it from inside a node, you will get terminate called after throwing an instance of 'std::runtime_error' what () : Node has already been added to an executor. The node itself is already added to the executor in rclcpp::spin() function inside the main function, and rclcpp::spin_until_future_complete() tries to add the node to another executor. You might note that the function already returned a std::shared_future on which you could wait. But that just hangs forever. So you're left with using a callback . Unfortunately that leaves you with no option to handle failure: For instance, if the service dies, your callback will never get called. There's no easy way to say that you only want to wait for 2 seconds for a result. Another idea for a workaround is to do something similar to what is done in the rclcpp::spin_until_future_complete() function by ourselves. Another possible avenue is using multithreaded executors, see this post for some more detail. Logging # The node name is now automatically prepended to the log message, so that part can be removed. In methods, get the logger from the node with get_logger() . In a free function foo() , use rclcpp::get_logger(\"foo\") . To provide a further level of hierarchy, use get_logger(\"foo\").get_child(\"bar\") . For example, ROS_INFO_COND ( show_debug_info_ , \"[MPC] some message with a float value %g\" , some_member_ ); should become RCLCPP_INFO_EXPRESSION ( get_logger (), show_debug_info_ , \"some message with a float value %g\" , some_member_ ); The mapping of logger macros is basically just -ROS_INFO(...) +RCLCPP_INFO(get_logger(), ...) with the exception of -ROS_INFO_COND(cond, ...) +RCLCPP_INFO_EXPRESSION(logger, cond, ...) -ROS_WARN_DELAYED_THROTTLE(duration, ...) +RCLCPP_WARN_SKIPFIRST_THROTTLE(get_logger(), *get_clock(), duration, ...) where the duration is an integer interpreted as milliseconds as opposed to seconds in ROS1. A readable way to document that is RCLCPP_WARN_SKIPFIRST_THROTTLE ( get_logger (), * get_clock (), 5000 /* ms */ , ...) Shutting down a subscriber # The shutdown() method doesn't exist anymore, but you can just throw away the subscriber with this->subscription_ = nullptr; or similar, for instance inside the subscription callback. Curiously, this works even though the subscription_ member variable is not the sole owner \u2013 the use_count is 3 in the minimal_subscriber example. Durations # Beware of just replacing ros::Duration with rclcpp::Duration \u2013 it compiles, but now expects nanoseconds instead of seconds. Use rclcpp::Duration::from_seconds instead. Alternative: Semi-automated porting with ros2-migration-tools (not working) # The following instructions to use ros2-migration-tools are given for completeness, we gave up and decided to port packages manually. From https://github.com/awslabs/ros2-migration-tools : pip3 install parse_cmake git clone https://github.com/awslabs/ros2-migration-tools.git wget https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz tar xaf clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/lib/libclang.so ros2-migration-tools/clang/ cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/lib/libclang.so.10 ros2-migration-tools/clang/ cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/include/ ros2-migration-tools/clang/clang The package needs to be built with ROS1 . I followed http://wiki.ros.org/noetic/Installation/Ubuntu outside of ADE sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 sudo apt update sudo apt install ros-melodic-ros-base In https://github.com/awslabs/ros2-migration-tools#setup-the-ros1-packages , it instructs me to compile a ros1 package with colcon to get started. colcon build --cmake-args -DCMAKE_EXPORT_COMPILE_COMMANDS = ON And that fails. I thought colcon is only for ROS2 so it should only work after porting, not before. I retried with catkin_make but also ran into issues there frederik.beaujean@frederik-beaujean-01:~/ade-home/AutowareArchitectureProposal$ catkin_make --source src/autoware/autoware.iv/control/shift_decider/ -DCMAKE_EXPORT_COMPILE_COMMANDS = ON Base path: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal Source space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider Build space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/build Devel space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/devel Install space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/install #### #### Running command: \"cmake /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCATKIN_DEVEL_PREFIX=/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/devel -DCMAKE_INSTALL_PREFIX=/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/install -G Unix Makefiles\" in \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/build\" #### CMake Error: The source \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider/CMakeLists.txt\" does not match the source \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/CMakeLists.txt\" used to generate cache. Re-run cmake with a different source directory. Invoking \"cmake\" failed According to an expert: You should be able to compile it with colcon, cause it works for both ROS 1 and ROS 2 code. You are getting the same error with catkin so it's probably something related to ROS 1 and the build instructions. Strange errors and their causes # Some error messages are so unhelpful that it might help to collect them and their causes. package.xml errors # If you forget <build_type>ament_cmake</build_type> , or you use package format 2 in combination with a package format 3 tag like <member_of_group> , you'll get the unhelpful error CMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 ( message ) : Could NOT find FastRTPS ( missing: FastRTPS_INCLUDE_DIR FastRTPS_LIBRARIES ) YAML param file # Tabs instead of spaces # Used tabs instead of spaces in your param.yaml file? Clearly , the most user-friendly error message is $ ros2 launch mypackage mypackage.launch.xml [ INFO ] [ launch ] : All log files can be found below /home/user/.ros/log/2020-10-19-19-09-13-676799-t4-30425 [ INFO ] [ launch ] : Default logging verbosity is set to INFO [ INFO ] [ mypackage-1 ] : process started with pid [ 30427 ] [ mypackage-1 ] [ ERROR ] [ 1603127353 .755503075 ] [ rcl ] : Failed to parse global arguments [ mypackage-1 ] [ mypackage-1 ] >>> [ rcutils | error_handling.c:108 ] rcutils_set_error_state () [ mypackage-1 ] This error state is being overwritten: [ mypackage-1 ] [ mypackage-1 ] 'Couldn' t parse params file: '--params-file /home/user/workspace/install/mypackage/share/mypackage/param/myparameters.yaml' . Error: Error parsing a event near line 1 , at /tmp/binarydeb/ros-foxy-rcl-yaml-param-parser-1.1.8/src/parse.c:599, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/arguments.c:391 ' [mypackage-1] [mypackage-1] with this new error message: [mypackage-1] [mypackage-1] ' context is zero-initialized, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/context.c:51 ' [mypackage-1] [mypackage-1] rcutils_reset_error() should be called after error handling to avoid this. [mypackage-1] <<< [mypackage-1] [ERROR] [1603127353.755523149] [rclcpp]: failed to finalize context: context is zero-initialized, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/context.c:51 [mypackage-1] terminate called after throwing an instance of ' rclcpp::exceptions::RCLInvalidROSArgsError ' [mypackage-1] what(): failed to initialize rcl: error not set [ERROR] [mypackage-1]: process has died [pid 30427, exit code -6, cmd ' /home/user/workspace/install/mypackage/lib/mypackage/mypackage --ros-args -r __node: = mypackage --params-file /home/user/workspace/install/mypackage/share/mypackage/param/myparameters.yaml ' ] . and that is indeed what ROS2 will tell you. No indentation # Without proper indentation of levels, there is a segfault when the YAML is parsed during rclcpp::init(argc, argv) . The error is similar to the above but begins with [ mpc_follower-1 ] free () : double free detected in tcache 2 Note that this message may be hidden when just launching with ros2 launch . It is shown running the node under valgrind which requires a launch-prefix . For example, modify mpc_follower.launch.xml <node pkg= \"mpc_follower\" exec= \"mpc_follower\" name= \"mpc_follower\" output= \"screen\" launch-prefix= \"valgrind\" > Another helpful option for diagnosing segfaults is to run under gdb to get a backtrace. Change the prefix <node pkg= \"mpc_follower\" exec= \"mpc_follower\" name= \"mpc_follower\" output= \"screen\" launch-prefix= \"xterm -e gdb -ex run --args\" > and after the segfault occurred, you can enter bt in the xterm window.","title":"Porting ROS1 code to ROS2"},{"location":"developer_guide/knowhow/PortingToROS2/#porting-ros1-code-to-ros2","text":"","title":"Porting ROS1 code to ROS2"},{"location":"developer_guide/knowhow/PortingToROS2/#setting-up-the-environment","text":"Do the following to setup and start ADE environment Setup ade home cd ~ mkdir ade-home cd ade-home touch .adehome Setup AutowareArchitectureProposal git clone https://github.com/tier4/AutowareArchitectureProposal cd AutowareArchitectureProposal git checkout ros2 enter ADE cd ~/ade-home/AutowareArchitectureProposal ade start --update --enter cd AutowareArchitectureProposal All commands that follow are to be entered in ADE. Next step is to fetch the sub-repos: cd ~/AutowareArchitectureProposal mkdir src vcs import src < autoware.proj.repos rosdep update rosdep install -y --from-paths src --ignore-src --rosdistro foxy colcon build --event-handlers console_cohesion+ For instance, the shift_decider package is in the repository github.com:tier4/pilot.auto.git , which is now in the autoware/pilot.auto subdirectory. Now branch off ros2 inside that subdirectory and delete the COLCON_IGNORE file in the package you want to port.","title":"Setting up the environment"},{"location":"developer_guide/knowhow/PortingToROS2/#important-changes","text":"The best source on migrating is the migration guide . It doesn't mention everything though, so this section lists some areas with important changes. A good general strategy is to try to implement those changes, then iteratively run colcon build --packages-up-to <your_package> and fix the first compiler error.","title":"Important changes"},{"location":"developer_guide/knowhow/PortingToROS2/#rewriting-packagexml","text":"The migration guide covers this well. See also here for a reference of the most recent version of this format.","title":"Rewriting package.xml"},{"location":"developer_guide/knowhow/PortingToROS2/#when-to-use-which-dependency-tag","text":"Any build tool needed only to set up the build needs buildtool_depend ; e.g., <buildtool_depend> ament_cmake </buildtool_depend> <buildtool_depend> rosidl_default_generators </buildtool_depend> Any external package included with #include in the files (source or headers) needs to have a corresponding <build_depend> ; e.g., <build_depend> logging </build_depend> Any external package included with #include in the header files also needs to have a corresponding <build_export_depend> ; e.g., <build_export_depend> eigen </build_export_depend> Any shared library that needs to be linked when the code is executed needs to have a corresponding <exec_depend> : this describes the runtime dependencies; e.g., <exec_depend> std_msgs </exec_depend> <exec_depend> rosidl_default_runtime </exec_depend> If a package falls under all three categories ( <build_depend> , <build_export_depend> , and <exec_depend> ), it is possible to just use <depend> <depend> shift_decider </depend>","title":"When to use which dependency tag"},{"location":"developer_guide/knowhow/PortingToROS2/#rewriting-cmakeliststxt","text":"This is not always straightforward. A starting point is to look at the pub-sub tutorial . This uses ament_cmake , which has a relatively good guide that still doesn't cover everything (like what to do when installing an executable).","title":"Rewriting CMakeLists.txt"},{"location":"developer_guide/knowhow/PortingToROS2/#ament_cmake-and-ament_cmake_auto","text":"One drawback of ament_cmake is that it requires typing out the dependencies at least twice, once in package.xml and once or more in CMakeLists.txt . Another possibility is to use ament_auto to get terse CMakeLists.txt . See this commit for an example. Unfortunately, there is no documentation for this tool, so you can only learn it from examples and reading the source code. It is also limited in what it does \u2013 it cannot currently generate message definitions, for instance, and always links all dependencies to all targets. There are more subtle issues too, like ament_auto_find_build_dependencies() . It just takes all the build dependencies verbatim from package.xml and calls find_package() without the REQUIRED option. This causes an issue when your library has a different name in package.xml / rosdep and in CMake . It also removes safety, as this won't complain when you mistype a package name or haven't installed the package yet. For these reasons, it's recommended to do the following: # Mark all packages as REQUIRED ament_auto_find_build_dependencies ( REQUIRED ${ ${PROJECT_NAME } _BUILD_DEPENDS} ${ ${PROJECT_NAME } _BUILDTOOL_DEPENDS} )","title":"ament_cmake and ament_cmake_auto"},{"location":"developer_guide/knowhow/PortingToROS2/#c-standard","text":"Add the following to ensure that a specific standard is required and extensions are not allowed if ( NOT CMAKE_CXX_STANDARD ) set ( CMAKE_CXX_STANDARD 14 ) set ( CMAKE_CXX_STANDARD_REQUIRED ON ) set ( CMAKE_CXX_EXTENSIONS OFF ) endif ()","title":"C++ standard"},{"location":"developer_guide/knowhow/PortingToROS2/#compiler-flags","text":"Make sure that flags are added only for specific compilers. Not everyone uses gcc or clang . if ( CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\" ) add_compile_options ( -Wall -Wextra -Wpedantic -Wno-unused-parameter ) endif ()","title":"Compiler flags"},{"location":"developer_guide/knowhow/PortingToROS2/#linters","text":"Add only ament_cmake_cppcheck to the list of linters in package.xml <test_depend> ament_lint_auto </test_depend> <test_depend> ament_cmake_cppcheck </test_depend> And the corresponding code in CMakeLists.txt if ( BUILD_TESTING ) find_package ( ament_lint_auto REQUIRED ) ament_lint_auto_find_test_dependencies () endif () Additionally, we use clang-tidy , in order for it to work, we need to build packages with the following: colcon build --packages-up-to <pkg_name> --cmake-args -DCMAKE_EXPORT_COMPILE_COMMANDS = 1 And then: clang-tidy -p build/compile_commands.json <path_to_pkg_source> To check the output of the linters we can just run the tests with: colcon test --packages-select <pkg_name> && colcon test-result --verbose","title":"Linters"},{"location":"developer_guide/knowhow/PortingToROS2/#replacing-std_msgs","text":"In ROS2, you should define semantically meaningful wrappers around primitive (number) types. They are deprecated in Foxy.","title":"Replacing std_msgs"},{"location":"developer_guide/knowhow/PortingToROS2/#changing-the-namespaces-and-header-files-for-generated-message-types","text":"If you follow the migration guide and change the included headers to have an extra /msg in the path and convert to snake_case , you might get a cryptic error. Turns out two files are being generated: One for C types ( .h headers) and one for CPP types ( .hpp headers). So don't forget to change .h to .hpp too. Also, don't forget to insert an additional ::msg between the package namespace and the class name. A tip: Sublime Text has a handy \"Case Conversion\" package for converting to snake case.","title":"Changing the namespaces and header files for generated message types"},{"location":"developer_guide/knowhow/PortingToROS2/#adapting-message-definitions","text":"If your message included something like Header header , it needs to be changed to std_msgs/Header header . Otherwise you'll see messages like fatal error: autoware_api_msgs/msg/detail/header__struct.hpp: No such file or directory .","title":"Adapting message definitions"},{"location":"developer_guide/knowhow/PortingToROS2/#inheriting-from-node-instead-of-nodehandle-members","text":"That's where differences start to show \u2013 I decided to make the VehicleCmdGate a Node even though the filename would suggest that vehicle_cmd_gate_node.cpp would be it. That's because it has publishers, subscribers, and logging. It previously had two NodeHandles, a public one and a private one ( \"~\" ). The public one was unused and could be removed. Private nodes are not supported in ROS2, so I simply made it public, but that is an area that needs to be brought up in review.","title":"Inheriting from Node instead of NodeHandle members"},{"location":"developer_guide/knowhow/PortingToROS2/#latched-topics","text":"For each latched publisher, you can use transient_local durability QoS on the publisher, e.g. when the history depth is 1: rclcpp :: QoS durable_qos { 1 }; durable_qos . transient_local (); or rclcpp :: QoS durable_qos = rclcpp :: QoS ( 1 ). transient_local (); However, all subscribers to that topic will also need transient_local durability. If this is omitted, the connection between the two will be negotiated to be volatile, i.e. old messages will not be delivered when the subscriber comes online.","title":"Latched topics"},{"location":"developer_guide/knowhow/PortingToROS2/#timing-issues","text":"First, if the timer can be replaced with a data-driven pattern, it is the preferred alternative for the long term:","title":"Timing issues"},{"location":"developer_guide/knowhow/PortingToROS2/#the-problem-with-timer-driven-patterns","text":"It is well understood that a polling or timer-driven pattern increases jitter (i.e. variance of latency). (Consider, for example: if every data processing node in a chain operates on a timer what is the best and worst case latency?) As a consequence for more timing-sensitive applications, it is generally not preferred to use a timer-driven pattern. On top of this, it is also reasonably well known that use of the clock is nondeterministic and internally this has been a large source of frustration with bad, or timing sensitive tests. Such tests typically require specific timing and/or implicitly require a certain execution order (loosely enforced by timing assumptions rather than explicitly via the code). As a whole, introducing the clock explicitly (or implicitly via timers) is problematic because it introduces additional state, and thus assumptions on the requirements for the operation of the component. Consider also leap seconds and how that might ruin the operation and/or assumptions needed for the proper operation of the component.","title":"The Problem with Timer-Driven Patterns"},{"location":"developer_guide/knowhow/PortingToROS2/#preferred-patterns","text":"In general, a data-driven pattern should be preferred to a timer-driven pattern. One reasonable exception to this guideline is the state estimator/filter at the end of localization. A timer-driven pattern in this context is useful to provide smooth behavior and promote looser coupling between the planning stack and the remainder of the stack. The core idea behind a data-driven pattern is that as soon as data arrives, it should be appropriately processed. Furthermore, the system clock (or any other source of time) should not be used to manipulate data or the timestamps. This pattern is valuable since it implicitly cuts down on hidden state (being the clock), and thus simplifies assumptions needed for the node to work. For examples of this kind of pattern, see the lidar object detection stack in Autoware.Auto. By not using any mention of the clock save for in the drivers, the stack can run equivalently on bag data, simulation data, or live data. A similar pattern with multiple inputs can be seen in the MPC implementation both internally and externally.","title":"Preferred Patterns"},{"location":"developer_guide/knowhow/PortingToROS2/#replicating-rostimer","text":"Assuming you still want to replicate the existing ros::Timer functionality: There is rclcpp::WallTimer , which has a similar interface, but it's not equivalent. The wall timer uses a wall clock ( RCL_STEADY_TIME clock), i.e. it doesn't listen to the /clock topic populated by simulation time. That the timer doesn't stop when simulation time stops, and doesn't go faster/slower when simulation time goes faster or slower. By contrast, the GenericTimer provides an interface to supply a clock, but there is no convenient function for setting up such a timer, comparable to Node::create_wall_timer . For now, this works: auto timer_callback = std :: bind ( & VehicleCmdGate :: onTimer , this ); auto period = std :: chrono :: duration_cast < std :: chrono :: nanoseconds > ( std :: chrono :: duration < double > ( update_period_ )); timer_ = std :: make_shared < rclcpp :: GenericTimer < decltype ( timer_callback ) >> ( this -> get_clock (), period , std :: move ( timer_callback ), this -> get_node_base_interface () -> get_context ()); this -> get_node_timers_interface () -> add_timer ( timer_ , nullptr ); Also, this doesn't work, even with a subsequent add_timer() call: timer_ = rclcpp :: create_timer ( this , this -> get_clock (), period , timer_callback );","title":"Replicating ros::Timer"},{"location":"developer_guide/knowhow/PortingToROS2/#rosbag-recording","text":"Unfortunately, one additional problem remains. ros2 bag does not record /clock (aka sim time) whereas rosbag does. This implies that in order to get the same behavior in ROS 2, either: rosbag along with the ros1_bridge must be used Some explicit time source must be used and explicitly recorded by ros2 bag","title":"Rosbag recording"},{"location":"developer_guide/knowhow/PortingToROS2/#parameters","text":"It's not strictly necessary, but you probably want to make sure the filename is xyz.param.yaml . Then come two steps:","title":"Parameters"},{"location":"developer_guide/knowhow/PortingToROS2/#adjust-code","text":"double vel_lim ; pnh_ . param < double > ( \"vel_lim\" , vel_lim , 25.0 ); becomes const double vel_lim = declare_parameter ( \"vel_lim\" , 25.0 ); which is equivalent to const double vel_lim = declare_parameter < double > ( \"vel_lim\" , 25.0 ); This allows to set the initial value e.g. via a parameter file. NOTE Calling ros2 param set <NODE> vel_lim 1.234 after starting the node works but will not alter the member vel_lim ! See the section below on dynamic reconfigure to achieve that.","title":"Adjust code"},{"location":"developer_guide/knowhow/PortingToROS2/#dynamic_reconfigure","text":"Dynamic reconfigure as it existed in ROS1 does not exist anymore in ROS2 and can be achieved by simpler means using a parameter callback.","title":"dynamic_reconfigure"},{"location":"developer_guide/knowhow/PortingToROS2/#cfg-files","text":"Remove the package's .cfg file and associated cfg/ subdirectory.","title":"cfg files"},{"location":"developer_guide/knowhow/PortingToROS2/#header-file","text":"In the header file, remove includes of dynamic_reconfigure and the node-specific config file. As a concrete example, take the MPC follower -#include <dynamic_reconfigure/server.h> -#include <mpc_follower/MPCFollowerConfig.h> you need to set a parameter handler and callback function: OnSetParametersCallbackHandle :: SharedPtr set_param_res_ ; rcl_interfaces :: msg :: SetParametersResult paramCallback ( const std :: vector < rclcpp :: Parameter > & parameters ); If there are many parameters (rule of thumb: more than 2), it is more practical to group them in a struct defined within the node's declaration: class MPCFollower : public rclcpp :: Node { struct MPCParam { int prediction_horizon ; ... } mpc_param ; }; Add a method to declare all the parameters void declareMPCparameters ();","title":"Header file"},{"location":"developer_guide/knowhow/PortingToROS2/#implementation-file","text":"Write the following into the definition of the class that inherits from rclcpp::Node . A few macros and a utility function can help keep the following code clean and void of redundancy. These macros are optional but help when many parameters are to be updated dynamically. #define DECLARE_MPC_PARAM(PARAM_STRUCT, NAME, VALUE) \\ PARAM_STRUCT.NAME = declare_parameter(\"mpc_\" #NAME, VALUE) #define UPDATE_MPC_PARAM(PARAM_STRUCT, NAME) \\ update_param(parameters, \"mpc_\" #NAME, PARAM_STRUCT.NAME) namespace { template < typename T > void update_param ( const std :: vector < rclcpp :: Parameter > & parameters , const std :: string & name , T & value ) { auto it = std :: find_if ( parameters . cbegin (), parameters . cend (), [ & name ]( const rclcpp :: Parameter & parameter ) { return parameter . get_name () == name ; }); if ( it != parameters . cend ()) { value = it -> template get_value < T > (); } } } // namespace In the constructor, define the callback and declare parameters with default values // the type of the ROS parameter is defined by the C++ type of the default value, // so 50 is not equivalent to 50.0! DECLARE_MPC_PARAM ( mpc_param_ , prediction_horizon , 50 ); // set parameter callback set_param_res_ = add_on_set_parameters_callback ( std :: bind ( & MPCFollower :: paramCallback , this , _1 )); Inside the callback, you have to manually update each parameter for which you want to react to change from the outside. You can (inadvertently) declare more parameters than you react to. rcl_interfaces :: msg :: SetParametersResult result ; result . successful = true ; result . reason = \"success\" ; // strong exception safety wrt MPCParam MPCParam param = mpc_param_ ; try { UPDATE_MPC_PARAM ( param , prediction_horizon ); // update all other parameters, too // transaction succeeds, now assign values mpc_param_ = param ; } catch ( const rclcpp :: exceptions :: InvalidParameterTypeException & e ) { result . successful = false ; result . reason = e . what (); } return result ; When the node is running, you can set the parameter dynamically with the following command. The value is converted to a type as in C++, so setting an int to 51.0 leads to an InvalidParameterTypeException in the callback. ros2 param set /mpc_follower prediction_horizon 51 Make sure relevant parameters can be set from the command line or rqt and changes are reflected by the package.","title":"Implementation file"},{"location":"developer_guide/knowhow/PortingToROS2/#parameter-client-discouraged","text":"The parameter client is another way to dynamically set the parameter defined in the node. The client subscribes to the /parameter_event topic and call the callback function. This allows the client node to get all the information about parameter changes in every node. The callback argument contains the target node name, which can be used to determine which node the parameter change is for. In .hpp, rclcpp :: AsyncParametersClient :: SharedPtr param_client_ ; rclcpp :: Subscription < rcl_interfaces :: msg :: ParameterEvent >:: SharedPtr sub_param_event_ ; void paramCallback ( const rcl_interfaces :: msg :: ParameterEvent :: SharedPtr event ); In .cpp, // client setting param_client_ = std :: make_shared < rclcpp :: AsyncParametersClient > ( this , \"param_client\" ); sub_param_event_ = param_client_ -> on_parameter_event ( std :: bind ( & VelocityController :: paramCallback , this , std :: placeholders :: _1 )); // callback setting void paramCallback ( const rcl_interfaces :: msg :: ParameterEvent :: SharedPtr event ) { for ( auto & new_parameter : event -> new_parameters ) { std :: cout << \" \" << new_parameter . name << std :: endl ; } for ( auto & changed_parameter : event -> changed_parameters ) { std :: cout << \" \" << changed_parameter . name << std :: endl ; } for ( auto & deleted_parameter : event -> deleted_parameters ) { std :: cout << \" \" << deleted_parameter . name << std :: endl ; } }; However, this method calls the callback for all parameter changes of all nodes. So the add_on_set_parameters_callback is recommended for the porting of the dynamic reconfigure. reference: https://discourse.ros.org/t/composition-and-parameters-best-practice-suggestions/1001 https://github.com/ros2/rclcpp/issues/243","title":"Parameter client [discouraged]"},{"location":"developer_guide/knowhow/PortingToROS2/#adjust-param-file","text":"Two levels of hierarchy need to be added around the parameters themselves and each level has to be indented relative to its parent (by two spaces in this example): <node name or /** > : ros__parameters: <params>","title":"Adjust param file"},{"location":"developer_guide/knowhow/PortingToROS2/#types","text":"Also, ROS1 didn't have a problem when you specify an integer, e.g. 28 for a double parameter, but ROS2 does: [ vehicle_cmd_gate-1 ] terminate called after throwing an instance of 'rclcpp::exceptions::InvalidParameterTypeException' [ vehicle_cmd_gate-1 ] what () : parameter 'vel_lim' has invalid type: expected [ double ] got [ integer ] Best to just change 28 to 28.0 in the param file. See also this issue .","title":"Types"},{"location":"developer_guide/knowhow/PortingToROS2/#launch-file","text":"There is a migration guide . One thing it doesn't mention is that the .launch file also needs to be renamed to .launch.xml .","title":"Launch file"},{"location":"developer_guide/knowhow/PortingToROS2/#replacing-tf2_rosbuffer","text":"A tf2_ros::Buffer member that is filled by a tf2_ros::TransformListener can become a tf2::BufferCore in most cases. This reduces porting effort, since the a tf2::BufferCore can be constructed like a ROS1 tf2_ros::Buffer . For an example, see this PR . However, in some cases the extra functionality of tf2_ros::Buffer is needed. For instance, waiting for a transform to arrive, usually in the form of lookupTransform() with a timeout argument.","title":"Replacing tf2_ros::Buffer"},{"location":"developer_guide/knowhow/PortingToROS2/#avoiding-a-lookup-with-timeout","text":"Often, code doesn't really need a transform lookup with timeout. For instance, this package has a \"main\" subscription callback, onTrigger() , that waits for the most recent transform ( tf2::TimePointZero ), then checks if auxiliary data from other subscriptions is there and returns early from the callback if it isn't. In that case, I think the callback can simply treat transforms the same way as this auxiliary data, i.e. just do a simple lookupTransform() with no timeout and return early from the callback if it fails. The node won't do any work anyway until it's ready (i.e. has all the auxiliary data). Note that this pattern works only when the node is waiting for the most recent transform \u2013 if your callback wants to use the transform at a specific time, e.g. the timestamp of the message that triggered the callback, this pattern doesn't make sense. In that case, avoiding waitForTransform() requires refactoring the architecture of your system, but that topic is currently out of scope. It's worth keeping in mind that waiting for transforms in general can be troublesome \u2013 it is only a probabilistic solution that fails in a bad way for latency spikes, makes it hard to reason about the behavior of the whole system and probably incurs more latency than necessary.","title":"Avoiding a lookup with timeout"},{"location":"developer_guide/knowhow/PortingToROS2/#when-you-cant-avoid-a-lookup-with-timeout","text":"You should be able to use tf2_ros::Buffer::lookupTransform() with a timeout out of the box. There is one caveat, namely there has been at least one report of such an error: Do not call canTransform or lookupTransform with a timeout unless you are using another thread for populating data. Without a dedicated thread it will always timeout. If you have a separate thread servicing tf messages, call setUsingDedicatedThread(true) on your Buffer instance. There is also the drawback of spamming the console with warnings like Warning: Invalid frame ID \"a\" passed to canTransform argument target_frame - frame does not exist at line 133 in /tmp/binarydeb/ros-foxy-tf2-0.13.6/src/buffer_core.cpp if the frame has never been sent before.","title":"When you can't avoid a lookup with timeout"},{"location":"developer_guide/knowhow/PortingToROS2/#what-about-waitfortransform","text":"There is also a waitForTransform API involving futures, but it has bugs and limitations. In particular: Its callback does not get called when the request can be answered immediately You can not call get() on the future and expect it to return the transform or throw an exception when the timeout ends. It continues waiting after the timeout expires if the future is not ready yet (i.e. the transform hasn't arrived). This limits you to the following style of calling the API, which doesn't use the callback and limits the waiting time before calling get() on the future: // In the node definition tf2_ros :: Buffer tf_buffer_ ; tf2_ros :: TransformListener tf_listener_ ; ... // In the constructor auto cti = std :: make_shared < tf2_ros :: CreateTimerROS > ( this -> get_node_base_interface (), this -> get_node_timers_interface ()); tf_buffer_ . setCreateTimerInterface ( cti ); ... // In the function processing data auto tf_future = tf_buffer_ . waitForTransform ( a , b , msg_time , std :: chrono :: milliseconds ( 0 ), [ this ]( auto ){}); auto status = tf_future . wait_for ( timeout_ ); if ( status == std :: future_status :: deferred ) { // This never happened in experiments } else if ( status == std :: future_status :: timeout ) { // The transform did not arrive within the timeout duration } else { // The transform is here, and can now be accessed without triggering the waiting-infinitely bug auto transform = tf_future . get (); } The waitForTransform() function will return immediately. Note that the timeout passed to waitForTransform() does not matter, only the timeout passed to wait_for() . There is a bug with this when tf2::TimeStampZero is requested instead of a nonzero time: the status will be ready , but accessing the result with get() throws an exception. There is another bug where this bug is not triggered under some conditions, for extra fun. So do not use waitForTransform() with tf2::TimeStampZero (or any other way of saying \"time 0\"). The silver lining is that this can be often avoided anyway since it is the scenario described in the section before. For more details, see https://github.com/nnmm/tf2_example and the resulting table .","title":"What about waitForTransform()?"},{"location":"developer_guide/knowhow/PortingToROS2/#shared-pointers","text":"Be careful in creating a std::shared_ptr to avoid a double-free situation when the constructor argument after porting is a dereferenced shared pointer. For example, if msg previously was a raw pointer and now is a shared pointer, the following would lead to both msg and a deleting the same resource. auto a = std :: make_shared < autoware_planning_msgs :: Trajectory > ( * msg ); To avoid this, just copy the shared pointer std :: shared_ptr < autoware_planning_msgs :: msg :: Trajectory > a = msg ;","title":"Shared pointers"},{"location":"developer_guide/knowhow/PortingToROS2/#service-clients","text":"There is no synchronous API for service calls, and the futures API can not be used from inside a node, only the callback API. The futures API is what is used in tutorials such as Writing a simple service and client , but note that the call to rclcpp::spin_until_future_complete() does not happen from inside any subscriber callback or similar. If you do call it from inside a node, you will get terminate called after throwing an instance of 'std::runtime_error' what () : Node has already been added to an executor. The node itself is already added to the executor in rclcpp::spin() function inside the main function, and rclcpp::spin_until_future_complete() tries to add the node to another executor. You might note that the function already returned a std::shared_future on which you could wait. But that just hangs forever. So you're left with using a callback . Unfortunately that leaves you with no option to handle failure: For instance, if the service dies, your callback will never get called. There's no easy way to say that you only want to wait for 2 seconds for a result. Another idea for a workaround is to do something similar to what is done in the rclcpp::spin_until_future_complete() function by ourselves. Another possible avenue is using multithreaded executors, see this post for some more detail.","title":"Service clients"},{"location":"developer_guide/knowhow/PortingToROS2/#logging","text":"The node name is now automatically prepended to the log message, so that part can be removed. In methods, get the logger from the node with get_logger() . In a free function foo() , use rclcpp::get_logger(\"foo\") . To provide a further level of hierarchy, use get_logger(\"foo\").get_child(\"bar\") . For example, ROS_INFO_COND ( show_debug_info_ , \"[MPC] some message with a float value %g\" , some_member_ ); should become RCLCPP_INFO_EXPRESSION ( get_logger (), show_debug_info_ , \"some message with a float value %g\" , some_member_ ); The mapping of logger macros is basically just -ROS_INFO(...) +RCLCPP_INFO(get_logger(), ...) with the exception of -ROS_INFO_COND(cond, ...) +RCLCPP_INFO_EXPRESSION(logger, cond, ...) -ROS_WARN_DELAYED_THROTTLE(duration, ...) +RCLCPP_WARN_SKIPFIRST_THROTTLE(get_logger(), *get_clock(), duration, ...) where the duration is an integer interpreted as milliseconds as opposed to seconds in ROS1. A readable way to document that is RCLCPP_WARN_SKIPFIRST_THROTTLE ( get_logger (), * get_clock (), 5000 /* ms */ , ...)","title":"Logging"},{"location":"developer_guide/knowhow/PortingToROS2/#shutting-down-a-subscriber","text":"The shutdown() method doesn't exist anymore, but you can just throw away the subscriber with this->subscription_ = nullptr; or similar, for instance inside the subscription callback. Curiously, this works even though the subscription_ member variable is not the sole owner \u2013 the use_count is 3 in the minimal_subscriber example.","title":"Shutting down a subscriber"},{"location":"developer_guide/knowhow/PortingToROS2/#durations","text":"Beware of just replacing ros::Duration with rclcpp::Duration \u2013 it compiles, but now expects nanoseconds instead of seconds. Use rclcpp::Duration::from_seconds instead.","title":"Durations"},{"location":"developer_guide/knowhow/PortingToROS2/#alternative-semi-automated-porting-with-ros2-migration-tools-not-working","text":"The following instructions to use ros2-migration-tools are given for completeness, we gave up and decided to port packages manually. From https://github.com/awslabs/ros2-migration-tools : pip3 install parse_cmake git clone https://github.com/awslabs/ros2-migration-tools.git wget https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz tar xaf clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/lib/libclang.so ros2-migration-tools/clang/ cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/lib/libclang.so.10 ros2-migration-tools/clang/ cp -r clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/include/ ros2-migration-tools/clang/clang The package needs to be built with ROS1 . I followed http://wiki.ros.org/noetic/Installation/Ubuntu outside of ADE sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 sudo apt update sudo apt install ros-melodic-ros-base In https://github.com/awslabs/ros2-migration-tools#setup-the-ros1-packages , it instructs me to compile a ros1 package with colcon to get started. colcon build --cmake-args -DCMAKE_EXPORT_COMPILE_COMMANDS = ON And that fails. I thought colcon is only for ROS2 so it should only work after porting, not before. I retried with catkin_make but also ran into issues there frederik.beaujean@frederik-beaujean-01:~/ade-home/AutowareArchitectureProposal$ catkin_make --source src/autoware/autoware.iv/control/shift_decider/ -DCMAKE_EXPORT_COMPILE_COMMANDS = ON Base path: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal Source space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider Build space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/build Devel space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/devel Install space: /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/install #### #### Running command: \"cmake /home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCATKIN_DEVEL_PREFIX=/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/devel -DCMAKE_INSTALL_PREFIX=/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/install -G Unix Makefiles\" in \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/build\" #### CMake Error: The source \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/autoware/autoware.iv/control/shift_decider/CMakeLists.txt\" does not match the source \"/home/frederik.beaujean/ade-home/AutowareArchitectureProposal/src/CMakeLists.txt\" used to generate cache. Re-run cmake with a different source directory. Invoking \"cmake\" failed According to an expert: You should be able to compile it with colcon, cause it works for both ROS 1 and ROS 2 code. You are getting the same error with catkin so it's probably something related to ROS 1 and the build instructions.","title":"Alternative: Semi-automated porting with ros2-migration-tools (not working)"},{"location":"developer_guide/knowhow/PortingToROS2/#strange-errors-and-their-causes","text":"Some error messages are so unhelpful that it might help to collect them and their causes.","title":"Strange errors and their causes"},{"location":"developer_guide/knowhow/PortingToROS2/#packagexml-errors","text":"If you forget <build_type>ament_cmake</build_type> , or you use package format 2 in combination with a package format 3 tag like <member_of_group> , you'll get the unhelpful error CMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 ( message ) : Could NOT find FastRTPS ( missing: FastRTPS_INCLUDE_DIR FastRTPS_LIBRARIES )","title":"package.xml errors"},{"location":"developer_guide/knowhow/PortingToROS2/#yaml-param-file","text":"","title":"YAML param file"},{"location":"developer_guide/knowhow/PortingToROS2/#tabs-instead-of-spaces","text":"Used tabs instead of spaces in your param.yaml file? Clearly , the most user-friendly error message is $ ros2 launch mypackage mypackage.launch.xml [ INFO ] [ launch ] : All log files can be found below /home/user/.ros/log/2020-10-19-19-09-13-676799-t4-30425 [ INFO ] [ launch ] : Default logging verbosity is set to INFO [ INFO ] [ mypackage-1 ] : process started with pid [ 30427 ] [ mypackage-1 ] [ ERROR ] [ 1603127353 .755503075 ] [ rcl ] : Failed to parse global arguments [ mypackage-1 ] [ mypackage-1 ] >>> [ rcutils | error_handling.c:108 ] rcutils_set_error_state () [ mypackage-1 ] This error state is being overwritten: [ mypackage-1 ] [ mypackage-1 ] 'Couldn' t parse params file: '--params-file /home/user/workspace/install/mypackage/share/mypackage/param/myparameters.yaml' . Error: Error parsing a event near line 1 , at /tmp/binarydeb/ros-foxy-rcl-yaml-param-parser-1.1.8/src/parse.c:599, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/arguments.c:391 ' [mypackage-1] [mypackage-1] with this new error message: [mypackage-1] [mypackage-1] ' context is zero-initialized, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/context.c:51 ' [mypackage-1] [mypackage-1] rcutils_reset_error() should be called after error handling to avoid this. [mypackage-1] <<< [mypackage-1] [ERROR] [1603127353.755523149] [rclcpp]: failed to finalize context: context is zero-initialized, at /tmp/binarydeb/ros-foxy-rcl-1.1.8/src/rcl/context.c:51 [mypackage-1] terminate called after throwing an instance of ' rclcpp::exceptions::RCLInvalidROSArgsError ' [mypackage-1] what(): failed to initialize rcl: error not set [ERROR] [mypackage-1]: process has died [pid 30427, exit code -6, cmd ' /home/user/workspace/install/mypackage/lib/mypackage/mypackage --ros-args -r __node: = mypackage --params-file /home/user/workspace/install/mypackage/share/mypackage/param/myparameters.yaml ' ] . and that is indeed what ROS2 will tell you.","title":"Tabs instead of spaces"},{"location":"developer_guide/knowhow/PortingToROS2/#no-indentation","text":"Without proper indentation of levels, there is a segfault when the YAML is parsed during rclcpp::init(argc, argv) . The error is similar to the above but begins with [ mpc_follower-1 ] free () : double free detected in tcache 2 Note that this message may be hidden when just launching with ros2 launch . It is shown running the node under valgrind which requires a launch-prefix . For example, modify mpc_follower.launch.xml <node pkg= \"mpc_follower\" exec= \"mpc_follower\" name= \"mpc_follower\" output= \"screen\" launch-prefix= \"valgrind\" > Another helpful option for diagnosing segfaults is to run under gdb to get a backtrace. Change the prefix <node pkg= \"mpc_follower\" exec= \"mpc_follower\" name= \"mpc_follower\" output= \"screen\" launch-prefix= \"xterm -e gdb -ex run --args\" > and after the segfault occurred, you can enter bt in the xterm window.","title":"No indentation"},{"location":"tutorial/HowToInstall/","text":"Installation steps # Note: If the CUDA or TensorRT frameworks have already been installed, we strongly recommend uninstalling them first. We use vcstool to setup the workspace. Please install it first if it's not installed yet. sudo apt update && sudo apt install curl gnupg lsb-release sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null sudo apt-get update && sudo apt-get install python3-vcstool Set up the Autoware repository mkdir -p ~/workspace cd ~/workspace git clone git@github.com:tier4/autoware.proj.git cd autoware.proj mkdir src vcs import src < autoware.proj.repos Run the setup script to install CUDA, cuDNN 8, OSQP, ROS 2 and TensorRT 7, entering 'y' when prompted (this step will take around 45 minutes) ./setup_ubuntu20.04.sh Build the source code (this will take around 15 minutes) source ~/.bashrc colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release Several modules will report stderr output, but these are just warnings and can be safely ignored. Updating workspace # When you update your workspace after installation, please follow the steps below. cd autoware.proj git pull # Usually you can ignore this step, but note that it sometimes causes errors. rm -rf src mkdir src rm -rf build/ install/ log/ vcs import src < autoware.proj.repos vcs pull src rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release","title":"How to install"},{"location":"tutorial/HowToInstall/#installation-steps","text":"Note: If the CUDA or TensorRT frameworks have already been installed, we strongly recommend uninstalling them first. We use vcstool to setup the workspace. Please install it first if it's not installed yet. sudo apt update && sudo apt install curl gnupg lsb-release sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null sudo apt-get update && sudo apt-get install python3-vcstool Set up the Autoware repository mkdir -p ~/workspace cd ~/workspace git clone git@github.com:tier4/autoware.proj.git cd autoware.proj mkdir src vcs import src < autoware.proj.repos Run the setup script to install CUDA, cuDNN 8, OSQP, ROS 2 and TensorRT 7, entering 'y' when prompted (this step will take around 45 minutes) ./setup_ubuntu20.04.sh Build the source code (this will take around 15 minutes) source ~/.bashrc colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release Several modules will report stderr output, but these are just warnings and can be safely ignored.","title":"Installation steps"},{"location":"tutorial/HowToInstall/#updating-workspace","text":"When you update your workspace after installation, please follow the steps below. cd autoware.proj git pull # Usually you can ignore this step, but note that it sometimes causes errors. rm -rf src mkdir src rm -rf build/ install/ log/ vcs import src < autoware.proj.repos vcs pull src rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE = Release","title":"Updating workspace"},{"location":"tutorial/QuickStart/","text":"Quick Start # Rosbag simulation # Download the sample pointcloud and vector maps , unpack the zip archive and copy the two map files to the same folder. Download the sample rosbag files and put them into the same folder, e.g., ~/rosbag2/sample/ . db3 yaml Open a terminal and launch Autoware cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch logging_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Open a second terminal and play the sample rosbag file cd ~/workspace/autoware.proj source install/setup.bash ros2 bag play /path/to/sample.625-2.bag2_0.db3 -r 0 .2 Focus the view on the ego vehicle by changing the Target Frame in the RViz Views panel from viewer to base_link . Note # Sample map and rosbag: \u00a9 2020 Tier IV, Inc. Due to privacy concerns, the rosbag does not contain image data, and so traffic light recognition functionality cannot be tested with this sample rosbag. As a further consequence, object detection accuracy is decreased. Planning Simulator # Download the sample pointcloud and vector maps , unpack the zip archive and copy the two map files to the same folder. Open a terminal and launch Autoware cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch planning_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Set an initial pose for the ego vehicle a) Click the 2D Pose estimate button in the toolbar, or hit the P key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the initial pose. Set a goal pose for the ego vehicle a) Click the 2D Nav Goal button in the toolbar, or hit the G key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the goal pose. Engage the ego vehicle. a) Open the autoware_web_controller in a browser. b) Click the Engage button. Note # Sample map: \u00a9 2020 Tier IV, Inc.","title":"Quick start"},{"location":"tutorial/QuickStart/#quick-start","text":"","title":"Quick Start"},{"location":"tutorial/QuickStart/#rosbag-simulation","text":"Download the sample pointcloud and vector maps , unpack the zip archive and copy the two map files to the same folder. Download the sample rosbag files and put them into the same folder, e.g., ~/rosbag2/sample/ . db3 yaml Open a terminal and launch Autoware cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch logging_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Open a second terminal and play the sample rosbag file cd ~/workspace/autoware.proj source install/setup.bash ros2 bag play /path/to/sample.625-2.bag2_0.db3 -r 0 .2 Focus the view on the ego vehicle by changing the Target Frame in the RViz Views panel from viewer to base_link .","title":"Rosbag simulation"},{"location":"tutorial/QuickStart/#note","text":"Sample map and rosbag: \u00a9 2020 Tier IV, Inc. Due to privacy concerns, the rosbag does not contain image data, and so traffic light recognition functionality cannot be tested with this sample rosbag. As a further consequence, object detection accuracy is decreased.","title":"Note"},{"location":"tutorial/QuickStart/#planning-simulator","text":"Download the sample pointcloud and vector maps , unpack the zip archive and copy the two map files to the same folder. Open a terminal and launch Autoware cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch planning_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Set an initial pose for the ego vehicle a) Click the 2D Pose estimate button in the toolbar, or hit the P key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the initial pose. Set a goal pose for the ego vehicle a) Click the 2D Nav Goal button in the toolbar, or hit the G key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the goal pose. Engage the ego vehicle. a) Open the autoware_web_controller in a browser. b) Click the Engage button.","title":"Planning Simulator"},{"location":"tutorial/QuickStart/#note_1","text":"Sample map: \u00a9 2020 Tier IV, Inc.","title":"Note"},{"location":"tutorial/Requirements/","text":"Minimum Requirements # Hardware # x86 CPU (8 cores) 16GB RAM [Optional] NVIDIA GPU (4GB RAM) Performance will be improved with more cores, RAM and a higher-spec graphics card. - Although not required to run basic functionality, a GPU is mandatory in order to run the following components: - Perception/ObjectRecognition - Perception/TrafficLightRecognitionRecognition Software # Ubuntu 20.04 NVIDIA driver","title":"Requirements"},{"location":"tutorial/Requirements/#minimum-requirements","text":"","title":"Minimum Requirements"},{"location":"tutorial/Requirements/#hardware","text":"x86 CPU (8 cores) 16GB RAM [Optional] NVIDIA GPU (4GB RAM) Performance will be improved with more cores, RAM and a higher-spec graphics card. - Although not required to run basic functionality, a GPU is mandatory in order to run the following components: - Perception/ObjectRecognition - Perception/TrafficLightRecognitionRecognition","title":"Hardware"},{"location":"tutorial/Requirements/#software","text":"Ubuntu 20.04 NVIDIA driver","title":"Software"},{"location":"tutorial/SimulationTutorial/","text":"Simulation in Autoware # Autoware provides two types of simulation: rosbag-based simulation that can be used for testing/validation of the Sensing , Localization and Perception stacks. The Planning Simulator tool which is mainly used for testing/validation of Planning stack by simulating traffic rules, interactions with dynamic objects and control commands to the ego vehicle. How to use a pre-recorded rosbag file for simulation # Assumes that installation and setup of Autoware.IV has already been completed. Download the sample pointcloud and vector maps from here , unpack the zip archive and copy the two map files to the same folder. Download the sample rosbag files and put them into the same folder, e.g., ~/rosbag2/sample/ . db3 https://drive.google.com/file/d/1wLWyOlfH_-k4VYBgae1KAFlKdwJnH_si/view?usp=sharing yaml https://drive.google.com/file/d/1Arb-QVnNHM-BFdB_icm7J7fWkyuZt7mZ/view?usp=sharing Sensor Topic name Velodyne 128 (Top) /sensing/velodyne/top/velodyne_packets Velodyne 16 (Right) /sensing/velodyne/right/velodyne_packets Velodyne 16 (Left) /sensing/velodyne/left/velodyne_packets IMU (Tamagawa TAG300) /sensing/imu/tamagawa/imu_raw GNSS (Ublox F9P) /sensing/gnss/ublox/fix_velocity /sensing/gnss/ublox/nav_sat_fix /sensing/gnss/ublox/navpvt CAN data /vehicle/status/control_mode /vehicle/status/shift /vehicle/status/steering /vehicle/status/twist ~~Camera x 7~~ ~~/sensing/camera/camera[]/image_raw~~ Note: Due to privacy concerns, image data has been removed from the rosbag file. Open a terminal and launch Autoware in \"rosbag mode\". cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch logging_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Open a second terminal and play the sample rosbag file cd ~/workspace/autoware.proj source install/setup.bash ros2 bag play /path/to/sample.625-2.bag2_0.db3 -r 0 .2 Note # Sample map and rosbag: \u00a9 2020 Tier IV, Inc. How to use the Planning Simulator # Assumes that installation and setup of Autoware.IV has already been completed. Download the sample pointcloud and vector maps from here , unpack the zip archive and copy the two map files to the same folder. Launch Autoware with Planning Simulator cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch planning_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Set an initial pose for the ego vehicle a) Click the 2D Pose estimate button in the toolbar, or hit the P key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the initial pose. Set a goal pose for the ego vehicle a) Click the \"2D Nav Goal\" button in the toolbar, or hit the G key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the goal pose. Engage the ego vehicle. a) Open the autoware_web_controller in a browser. b) Click the Engage button. Simulate dummy obstacles # Set the position of dummy obstacle by clicking the 2D Dummy Pedestrian or 2D Dummy Car buttons in Rviz. These two buttons correspond to the shortcut keys L and K respectively. The properties of an object (including velocity, position/orientation error etc) can be adjusted via the Tool Properties panel in Rviz. Objects placed in the 3D View can be deleted by clicking the Delete All Objects button in Rviz and then clicking inside the 3D View pane. Simulate parking maneuver # Set an initial pose for the ego vehicle first, and then set the goal pose in a parking area. Note # sample map : \u00a9 2020 TierIV inc.","title":"Simulation"},{"location":"tutorial/SimulationTutorial/#simulation-in-autoware","text":"Autoware provides two types of simulation: rosbag-based simulation that can be used for testing/validation of the Sensing , Localization and Perception stacks. The Planning Simulator tool which is mainly used for testing/validation of Planning stack by simulating traffic rules, interactions with dynamic objects and control commands to the ego vehicle.","title":"Simulation in Autoware"},{"location":"tutorial/SimulationTutorial/#how-to-use-a-pre-recorded-rosbag-file-for-simulation","text":"Assumes that installation and setup of Autoware.IV has already been completed. Download the sample pointcloud and vector maps from here , unpack the zip archive and copy the two map files to the same folder. Download the sample rosbag files and put them into the same folder, e.g., ~/rosbag2/sample/ . db3 https://drive.google.com/file/d/1wLWyOlfH_-k4VYBgae1KAFlKdwJnH_si/view?usp=sharing yaml https://drive.google.com/file/d/1Arb-QVnNHM-BFdB_icm7J7fWkyuZt7mZ/view?usp=sharing Sensor Topic name Velodyne 128 (Top) /sensing/velodyne/top/velodyne_packets Velodyne 16 (Right) /sensing/velodyne/right/velodyne_packets Velodyne 16 (Left) /sensing/velodyne/left/velodyne_packets IMU (Tamagawa TAG300) /sensing/imu/tamagawa/imu_raw GNSS (Ublox F9P) /sensing/gnss/ublox/fix_velocity /sensing/gnss/ublox/nav_sat_fix /sensing/gnss/ublox/navpvt CAN data /vehicle/status/control_mode /vehicle/status/shift /vehicle/status/steering /vehicle/status/twist ~~Camera x 7~~ ~~/sensing/camera/camera[]/image_raw~~ Note: Due to privacy concerns, image data has been removed from the rosbag file. Open a terminal and launch Autoware in \"rosbag mode\". cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch logging_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Open a second terminal and play the sample rosbag file cd ~/workspace/autoware.proj source install/setup.bash ros2 bag play /path/to/sample.625-2.bag2_0.db3 -r 0 .2","title":"How to use a pre-recorded rosbag file for simulation"},{"location":"tutorial/SimulationTutorial/#note","text":"Sample map and rosbag: \u00a9 2020 Tier IV, Inc.","title":"Note"},{"location":"tutorial/SimulationTutorial/#how-to-use-the-planning-simulator","text":"Assumes that installation and setup of Autoware.IV has already been completed. Download the sample pointcloud and vector maps from here , unpack the zip archive and copy the two map files to the same folder. Launch Autoware with Planning Simulator cd ~/workspace/autoware.proj source install/setup.bash ros2 launch autoware_launch planning_simulator.launch.xml map_path: = /path/to/map_folder vehicle_model: = lexus sensor_model: = aip_xx1 Set an initial pose for the ego vehicle a) Click the 2D Pose estimate button in the toolbar, or hit the P key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the initial pose. Set a goal pose for the ego vehicle a) Click the \"2D Nav Goal\" button in the toolbar, or hit the G key b) In the 3D View pane, click and hold the left-mouse button, and then drag to set the direction for the goal pose. Engage the ego vehicle. a) Open the autoware_web_controller in a browser. b) Click the Engage button.","title":"How to use the Planning Simulator"},{"location":"tutorial/SimulationTutorial/#simulate-dummy-obstacles","text":"Set the position of dummy obstacle by clicking the 2D Dummy Pedestrian or 2D Dummy Car buttons in Rviz. These two buttons correspond to the shortcut keys L and K respectively. The properties of an object (including velocity, position/orientation error etc) can be adjusted via the Tool Properties panel in Rviz. Objects placed in the 3D View can be deleted by clicking the Delete All Objects button in Rviz and then clicking inside the 3D View pane.","title":"Simulate dummy obstacles"},{"location":"tutorial/SimulationTutorial/#simulate-parking-maneuver","text":"Set an initial pose for the ego vehicle first, and then set the goal pose in a parking area.","title":"Simulate parking maneuver"},{"location":"tutorial/SimulationTutorial/#note_1","text":"sample map : \u00a9 2020 TierIV inc.","title":"Note"}]}